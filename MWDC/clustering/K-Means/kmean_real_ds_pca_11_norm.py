# -*- coding: utf-8 -*-
"""KMean_Real_DS_PCA_11_Norm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aDVN56m72xziIgxRAJ5_HdDoHAHEXF4z
"""

from google.colab import drive
drive.mount('/content/drive')

"""###Install and Import needed libraries###"""

!pip install "dask[dataframe]"

!pip install cartopy
import cartopy

!pip install proj

!pip install basemap



import os
import sys
import pandas as pd
import numpy as np
import xarray as xr
import netCDF4 as nc
import matplotlib.pyplot as plt
#sys.path.insert(0, '/global/homes/x/xzheng/python_lib/')
from netCDF4 import Dataset
import matplotlib as mpl 
import matplotlib.colors as colors
#os.environ['PROJ_LIB'] = r'/global/cfs/cdirs/e3sm/xzheng/conda/pkgs/proj-7.2.0-h277dcde_2/share/'
from mpl_toolkits.basemap import Basemap
import cartopy.crs as ccrs
from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter
from sklearn import preprocessing
from statistics import mean
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from yellowbrick.cluster import KElbowVisualizer
from sklearn.metrics import silhouette_samples, silhouette_score

"""###Read our data as an xarray##"""

#path2 = ('/content/drive/MyDrive/Data/mock.nc')
#path2 = ('/content/drive/MyDrive/Private/Image_Similarity/mock1.nc')
#path2 = ('/content/drive/MyDrive/Data/mock_v2.nc')
#path2 = ('/content/drive/MyDrive/Data/mock_v2.1.nc')
#path2 = ('/content/drive/MyDrive/Data/mock_v3.nc')
#path2 = ('/content/drive/MyDrive/Data/mock_v3.1.nc')
#path2 = ('/content/drive/MyDrive/Data/mock_v4.nc')
path2 = ('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc')
#path2 = ('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily_smalldomain.nc')
#path2 = ('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_hourly.nc')
#path2 = ('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_hourly_smalldomain.nc')
data = xr.open_dataset(path2, decode_times=False) #To view the date as integers of 0, 1, 2,....
#data = xr.open_dataset(path2)# decode_times=False) #To view the date as integers of 0, 1, 2,....
#data5 = xr.open_dataset(path2) # To view time in datetime format
data

"""###Preprocessing###


1.   Apply Physics-based NaN imputation
2.   Transform and Normalize data
3.   PCA reduce data




"""

#Function to input NaN values across variables
def null_fill(input):

  dask_df = input.to_dask_dataframe(dim_order=None, set_index=False)
  pd_df = dask_df.compute()
  pd_df1 = pd_df.iloc[:, 3:]
  df2 = pd_df1[pd_df1.isnull().any(axis=1)]
  lst = list(df2.index.values)
  df2.loc[:] = np.nan
  dt = pd.concat([pd_df1, df2], axis=0)
  dt3 = dt[~dt.index.duplicated(keep='last')]
  dt4 = dt3[['sst', 'sp', 'u10', 'v10', 'sshf', 'slhf', 't2m']]
  pd_df4 = pd_df.iloc[:, 0:5]
  dff = pd_df4[['time', 'longitude', 'latitude', 'sst']]
  df = pd.merge(dff, dt4, left_index=True, right_index=True).drop('sst_y', axis=1)
  df.rename(columns={'sst_x':'sst'}, inplace=True)
  df_rows = pd.DataFrame(df).set_index(["time", "longitude", "latitude"])
  data = xr.Dataset.from_dataframe(df_rows)
  df_rows = pd.DataFrame(df).set_index(["time", "longitude", "latitude"])
  data = xr.Dataset.from_dataframe(df_rows)

  return data

data = null_fill(data)



'''Since the difference between sst values for the first 10 days are nominal, K-means finds it difficult to pick 2 clusters.
    Hence, we're dropping sst and running the algorithm on t2m to check if the algorithm is capable to identifying clusters within the 
    first 10 days'''
# data = data.drop_vars('sst')

"""### Elbow Curve Method
To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion.
"""

def datatransformation(data):
        dask_df = data.to_dask_dataframe(dim_order=None, set_index=False)
        pd_df = dask_df.compute()

        for i in pd_df.columns:
          if pd_df[i].isna().sum() > 0:
            pd_df[i].fillna(value=pd_df[i].mean(), inplace=True)
        
        #col = 'time','lat','lon'
        col = 'time','latitude','longitude'
        fin_df = pd_df.loc[:, ~pd_df.columns.isin(col)]

        trans_data = pd.DataFrame()
        for j in fin_df.columns:
          for i in range(0,pd_df.shape[0]):
              #c=(j + '(' + str(pd_df.lat[i])+','+str(pd_df.lon[i]) + ')')
              c=(j + '(' + str(pd_df.latitude[i])+','+str(pd_df.longitude[i]) + ')')
              trans_data.loc[pd_df.time[i], c] = pd_df[j][i]

        return trans_data

def datanormalization(input):
  x = input.values #returns a numpy array
  min_max_scaler = preprocessing.MinMaxScaler()
  x_scaled = min_max_scaler.fit_transform(x)
  trans_data = pd.DataFrame(x_scaled, columns=input.columns, index=input.index)
        
  return trans_data

from sklearn import metrics
from scipy.spatial.distance import cdist

# Data Transformation for Elbow Curve Method
#trans_data = datatransformation(data)
trans_data = pd.read_csv("/content/drive/MyDrive/Private/Image_Similarity/daily_trans_norm.csv")
trans_data = trans_data.iloc[:,1:]

distortions = []
inertias = []
mapping1 = {}
mapping2 = {}
K = range(1, 10)
  
for k in K:
    # Building and fitting the model
    kmeanModel = KMeans(n_clusters=k).fit(trans_data)
    kmeanModel.fit(trans_data)
  
    distortions.append(sum(np.min(cdist(trans_data, kmeanModel.cluster_centers_,
                                        'euclidean'), axis=1)) / trans_data.shape[0])
    inertias.append(kmeanModel.inertia_)
  
    mapping1[k] = sum(np.min(cdist(trans_data, kmeanModel.cluster_centers_,
                                   'euclidean'), axis=1)) / trans_data.shape[0]
    mapping2[k] = kmeanModel.inertia_

print('Distortion Values:')
for key, val in mapping1.items():
    print(f'{key} : {val}')    
print()
print('Inertia Values:')
for key, val in mapping2.items():
    print(f'{key} : {val}')

plt.plot(K, distortions, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Distortion')
plt.title('The Elbow Method using Distortion')
plt.show()


plt.plot(K, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

#trans_data.to_csv("/content/drive/MyDrive/Private/Image_Similarity/trans_data_daily1.csv")

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer

# trans_data = datatransformation(data)

# Instantiate the clustering model and visualizer
# Inertia scores
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,10))

visualizer.fit(trans_data)        # Fit the data to the visualizer
visualizer.show()

trans_data

pca = PCA(n_components=11)
principalComponents = pca.fit_transform(trans_data)
trans_data = pd.DataFrame(data = principalComponents
             , columns = ['PC_1', 'PC_2', 'PC_3','PC_4', 'PC_5', 'PC_6','PC_7', 'PC_8', 'PC_9','PC_10', 'PC_11'])

trans_data

"""### Implementing the algorithm by using the actual data"""

from numpy.random import uniform
import seaborn as sns
import random
import dask.dataframe as dd
from sklearn.preprocessing import MinMaxScaler

def euclidean(point, data):
    """
    Euclidean distance between point & data.
    Point has dimensions (m,), data has dimensions (n,m), and output will be of size (n,).
    """
    return np.sqrt(np.sum((point - data)**2, axis=1))
    

class KMeans:
    def __init__(self, n_clusters, max_iter=500):
        self.n_clusters = n_clusters
        self.max_iter = max_iter

    def fit(self, data):
        X_train = data

        # Initialize the centroids, using the "k-means++" method, where a random datapoint is selected as the first,
        # then the rest are initialized w/ probabilities proportional to their distances to the first
        # Pick a random point from train data for first centroid
        self.centroids = [random.choice(X_train)]
        for _ in range(self.n_clusters-1):
            # Calculate distances from points to the centroids
            dists = np.sum([euclidean(centroid, X_train) for centroid in self.centroids], axis=0)
            # Normalize the distances
            dists /= np.sum(dists)
            # Choose remaining points based on their distances
            new_centroid_idx, = np.random.choice(range(len(X_train)), size=1, p=dists)
            #print(new_centroid_idx)
            self.centroids += [X_train[new_centroid_idx]]
        # This initial method of randomly selecting centroid starts is less effective
        # min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)
        # self.centroids = [uniform(min_, max_) for _ in range(self.n_clusters)]
        # Iterate, adjusting centroids until converged or until passed max_iter
        iteration = 0
        prev_centroids = None

        while np.not_equal(self.centroids, prev_centroids).any() and iteration < self.max_iter:
            # Sort each datapoint, assigning to nearest centroid
            sorted_points = [[] for _ in range(self.n_clusters)]
            for x in X_train:
                dists = euclidean(x, self.centroids)
                centroid_idx = np.argmin(dists)
                sorted_points[centroid_idx].append(x)
            # Push current centroids to previous, reassign centroids as mean of the points belonging to them
            prev_centroids = self.centroids
            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]
            for i, centroid in enumerate(self.centroids):
                if np.isnan(centroid).any():  # Catch any np.nans, resulting from a centroid having no points
                    self.centroids[i] = prev_centroids[i]
            iteration += 1
            #print(self.centroids)
               

    def evaluate(self,z):
        X = z
        #Y = np.array(datanormalization(X))
        
        centroid = []
        centroid_idx = []
        #print(X.shape)
        i=0
        for x in X:
            # print('x:',x)
            dists = euclidean(x, self.centroids)
            # print('Centroid:', self.centroids)
            # print('Distance:',dists)
            centroid_id = np.argmin(dists)
           # print(centroid)
            centroid.append(self.centroids[centroid_id])
            centroid_idx.append(centroid_id)

        return centroid, centroid_idx

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=7, init='k-means++')
norm_data = np.array(datanormalization(trans_data))
kmeans.fit(norm_data)
labels = kmeans.fit(norm_data).labels_
pred = kmeans.predict(norm_data)
frame = pd.DataFrame(norm_data)
frame['Cluster'] = labels
frame['Cluster'].value_counts()

classification = labels
classification

df = pd.read_csv("/content/drive/MyDrive/ECRP_Data_Science/Implementation and Results/Jianwu_test/PCA_Result_Combination/PCA_Combined_var1.csv")

df2 = df.time_step
df2

# # calling the agglomerative algorithm and choosing n_clusters = 4 based on elbow value
# model = AgglomerativeClustering(n_clusters = 7, affinity = 'euclidean', linkage ='average')

# # training the model on transformed data
# y_model = model.fit(trans_data)
# labels = y_model.labels_

# creating pandas dataframe on transformed data
df1 = pd.DataFrame(df2, columns=['time_step'])
df1['clusterid'] = labels
#df1["cluster"] = cluster.labels_
df1['clusterid'].value_counts()
df1.to_csv("/content/drive/MyDrive/Private/Image_Similarity/kmeans_Daily_PCA11_Norm.csv")
df1

df1.groupby('clusterid').count()

# # creating pandas dataframe on transformed data
# norm_data = pd.DataFrame(norm_data, columns=['time_step'])
# norm_data['Cluster'] = labels
# #df1["cluster"] = cluster.labels_
# norm_data['Cluster'].value_counts()

classification = labels
classification

trans_data['Cluster'] = classification
trans_data

# Converting the normalized data array into a pandas dataframe
# trans_data = pd.DataFrame(norm_data, columns=trans_data.columns, index=trans_data.index)
# Adding class centers and cluster numbers as columns to the dataframe
# trans_data['Class_Center'] = class_centers
trans_data['Cluster'] = classification

# Rearranging the columns in the dataframe
# trans_data = trans_data[['Cluster'] + [c for c in trans_data if c not in ['Cluster']]]
# trans_data1 = trans_data
# trans_data = trans_data.reset_index()

#trans_data = trans_data.iloc[:,1:]
trans_data



"""#**Evaluation - RMSE**

### RMSE - Non-normalized functions
"""



# Non-normalized
# Function that creates a dictionary that holds the values of dates in each cluster
def get_datewise_clusters(formed_clusters): # classification
  Dates_Cluster = {}
  for i in set(formed_clusters): # classification
    Dates_Cluster['Dates_Cluster'+str(i)] = trans_data.index[trans_data.Cluster == i].to_list()
  return Dates_Cluster

# Non-normalized
# Function that creats a dictionary that holds all the clusters
def n_nor_get_clusters(input,formed_clusters): # classification
  com_arr = []
  Clusters = {}
  Dates_Cluster = get_datewise_clusters(formed_clusters)
  for i in set(formed_clusters):
    for j in Dates_Cluster['Dates_Cluster'+str(i)]:
      arr = np.array(input.isel(time=j).to_array()) # input is data
      com_arr.append(arr)
    Clusters['Cluster' + str(i)] = np.array(com_arr)
    com_arr = []
  return Clusters

# Non-normalized
# Function that creates a dictionary that holds all the cluster centers
def n_nor_get_cluster_centers(input,formed_clusters): #classification
  Cluster_Centers = {}
  Clusters = n_nor_get_clusters(input,formed_clusters)
  for i in set(formed_clusters):
    Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
  return Cluster_Centers

# Non-normalized 
#Intra RMSE Calculation Function
def n_nor_intra_rmse(input,formed_clusters):
  sq_diff = []
  intra_rmse = []
  Clusters = n_nor_get_clusters(input,formed_clusters)
  Cluster_Centers = n_nor_get_cluster_centers(input,formed_clusters)

  for i in range(len(Clusters)):
    for j in range(len(Clusters['Cluster' + str(i)])):
      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]
      Sq_diff = (diff**2)
      sq_diff.append(Sq_diff)
    Sq_diff_sum = sum(sq_diff)
    sq_diff = []
    n = len(Clusters['Cluster' + str(i)])
    Sqrt_diff_sum = np.sqrt(sum(sum(sum(Sq_diff_sum/n))))
    intra_rmse.append(Sqrt_diff_sum)
  return intra_rmse

# Non-normalized
def handle_missing_values(input):
  var_mean = {}
  for i in input.data_vars:
    if input[i].isnull().sum().item() > 0:
      print(i,'has null values')
      var_mean[str(i) + '_mean'] = input[i].mean().item()
      input[i] = input[i].fillna(var_mean[str(i) + '_mean'])
  return input

"""### RMSE - Normalized functions"""

# Normalized
# Function that creates two dictionaries that hold all the clusters and cluster centers
def nor_get_clusters_and_centers(input,formed_clusters):
  Clusters = {}
  Cluster_Centers = {}
  for i in set(formed_clusters):
    Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))
    Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
  return Clusters,Cluster_Centers

# Normalized
# Intra RMSE Calculation Function
def nor_intra_rmse(input,formed_clusters):
  intra_rmse = []
  sq_diff = []
  Clusters,Cluster_Centers = nor_get_clusters_and_centers(input,formed_clusters)
  for i in range(len(Clusters)):
    for j in range(len(Clusters['Cluster' + str(i)])):
      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]
      Sq_diff = (diff**2)
      sq_diff.append(Sq_diff)
    Sq_diff_sum = sum(sum(sq_diff))
    sq_diff = []
    n = len(Clusters['Cluster' + str(i)])
    Sqrt_diff_sum = np.sqrt(Sq_diff_sum/n)
    intra_rmse.append(Sqrt_diff_sum)
  return intra_rmse

# RMSE Calculation
def RMSE(input,formed_clusters,normalize=False):
  inter_rmse = []
  avg_cluster = {}

  if normalize == False:
    input = handle_missing_values(input)
    Clusters = n_nor_get_clusters(input,formed_clusters)
    mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
    for i in range(len(Clusters)):
      avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    for i in range(len(Clusters)):
      for j in range(len(Clusters)):
        if i == j:
          a = n_nor_intra_rmse(input,formed_clusters)
          mat[i].iloc[j] = round(a[i],2)
        else:
          diff = avg_cluster['avg_cluster' + str(i)] - avg_cluster['avg_cluster' + str(j)]
          Sq_diff = (diff**2)
          #Sq_diff_sum = sum(Sq_diff)
          Sq_diff_sum = sum(sum(sum(Sq_diff)))
          #inter_rmse.append(np.sqrt(Sq_diff_sum))
          n = len(avg_cluster['avg_cluster'+str(i)][0])
          Sqrt_diff_sum = np.sqrt(Sq_diff_sum/n)
          mat[i].iloc[j] = round(Sqrt_diff_sum,2)
          #print('Inter RMSE between cluster',i,'and cluster',j,'is:',inter_rmse.pop())
        

  else:
    # trans_data = datatransformation(input)

    # # Data Normalization
    # trans_data = datanormalization(trans_data)

    # # Adding class centers and cluster numbers as columns to the dataframe
    # trans_data['Cluster'] = formed_clusters

    # # Rearranging the columns in the dataframe
    # trans_data = trans_data[['Cluster'] + [c for c in trans_data if c not in ['Cluster']]]
  
    Clusters, Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)

    # Doing the below step after finding the cluster centers. Otherwise, we'll be calculating mean on date (index) too.
    #trans_data = trans_data.reset_index()

  
    mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
    for i in range(len(Clusters)):
      avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    for i in range(len(Clusters)):
      for j in range(len(Clusters)):
        if i == j:
          a = nor_intra_rmse(trans_data,formed_clusters)
          mat[i].iloc[j] = round(a[i],2)
        else:
          diff = avg_cluster['avg_cluster' + str(i)] - avg_cluster['avg_cluster' + str(j)]
          Sq_diff = (diff**2)
          #Sq_diff_sum = sum(Sq_diff)
          Sq_diff_sum = sum(Sq_diff)
          #inter_rmse.append(np.sqrt(Sq_diff_sum))
          Sqrt_diff_sum = np.sqrt(Sq_diff_sum)
          mat[i].iloc[j] = round(Sqrt_diff_sum,2)
          #print('Inter RMSE between cluster',i,'and cluster',j,'is:',inter_rmse.pop())

  return mat

final1 = RMSE(data,classification,True)
final1

final2 = RMSE(data,classification,False)
final2

# This is to showcase the scenario where a user does not pass the third parameter
# The default value has been set to False
final3 = RMSE(data,classification)
final3

"""#**Evaluation - Spatial Correlation Coefficient**"""

#For Spatial Correlation coefficient computation
'''
   Input parameters: 
   X : An array, A 2-D array containing multiple variables and observations. 
   Y : An array, A 2-D array containing multiple variables and observations. 
   Each row of x represents a variable, and each column a single observation of all those variables.

   Returns: Pearson product-moment correlation coefficients.:A scaler quantity [-1, 1] 
   where -1 means the inputs are inversly correlated(opposite movements) 
   and 1 means directly correlated (unidirectional parallel movement) 
   while 0 means no correlation.
   
'''


def pearson_PM(x, y):

  #convert format from netcdf to np array
    #x_form = x.to_numpy()
    #y_form = y.to_numpy()

    #Flatten/transform from 2d to 1d
    X_flat = x.flatten()
    Y_flat = y.flatten()

    #Compute correlation matrix
    corr_mat = np.corrcoef(X_flat, Y_flat)

    #Return entry [0,1]
    return corr_mat[0,1]

def corr_space_2d(var1_2d,var2_2d):
    var1= np.reshape(var1_2d,var1_2d.shape[0]*var1_2d.shape[1])
    var2= np.reshape(var2_2d,var2_2d.shape[0]*var2_2d.shape[1])
    R0=np.corrcoef(var1,var2)
    if(R0.shape !=(2,2)):
        #print("corr_space_2d error: corrcoef maxtrix R0 :",R0)
        stop
    R = R0[0,1]
    return R



# # Converting the normalized data array into a pandas dataframe
# trans_data = datatransformation(data)
# nor_data = np.array(datanormalization(trans_data)) # This is just a trial
# trans_data = pd.DataFrame(nor_data, columns=trans_data.columns, index=trans_data.index)
# # Adding class centers and cluster numbers as columns to the dataframe
# #trans_data['Class_Center'] = class_centers
# trans_data['Cluster'] = classification
# # Rearranging the columns in the dataframe
# #trans_data = trans_data[['Class_Center', 'Cluster'] + [c for c in trans_data if c not in ['Class_Center', 'Cluster']]]
# #trans_data1 = trans_data
# #trans_data = trans_data.reset_index()
# trans_data

trans_data.to_csv("/content/drive/MyDrive/Private/Image_Similarity/Kmean7cl_trans_data_daily_PCA_11_Norm.csv")

#trans_data = pd.read_csv("/content/drive/MyDrive/Private/Image_Similarity/Kmean_trans_data_daily_PCA_Transformed.csv")

#trans_data = trans_data.iloc[:,1:]

#trans_data

#Intra-spatial correlation coefficient Calculation Function
import functools 

def n_nor_intra_sp_corr(input,formed_clusters):
  mylist = []
  intra_sp_corr = []
  Clusters = n_nor_get_clusters(input,formed_clusters)
  Cluster_Centers = n_nor_get_cluster_centers(input,formed_clusters)
  
  for i in range(len(Clusters)):
    mylist = []
    for j in range(len(Clusters['Cluster' + str(i)])):
      corr_coeff = pearson_PM(Clusters['Cluster' + str(i)][j], Cluster_Centers['Cluster_Center' + str(i)])
       #print('i: {}, j: {}, corr_coeff:{}'.format(i, j, corr_coeff))
      mylist.append(corr_coeff)
      average_corr_coeff = sum(mylist) / len(mylist)
    intra_sp_corr.append(average_corr_coeff)
  return intra_sp_corr

"""##Normalized - Spatial Correlation##"""

# Normalized
# Function that creates two dictionaries that hold all the clusters and cluster centers
def nor_get_clusters_and_centers(input,formed_clusters):
  Clusters = {}
  Cluster_Centers = {}
  for i in set(formed_clusters):
    Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))
    Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
  return Clusters,Cluster_Centers

#Intra-spatial correlation coefficient Calculation Function

def nor_intra_sp_corr(input,formed_clusters):
  mylist = []
  intra_sp_corr = []
  Clusters,Cluster_Centers = nor_get_clusters_and_centers(input,formed_clusters)
  
  for i in range(len(Clusters)):
    mylist = []
    for j in range(len(Clusters['Cluster' + str(i)])):
      corr_coeff = pearson_PM(Clusters['Cluster' + str(i)][j], Cluster_Centers['Cluster_Center' + str(i)])
       #print('i: {}, j: {}, corr_coeff:{}'.format(i, j, corr_coeff))
      mylist.append(corr_coeff)
      average_corr_coeff = sum(mylist) / len(mylist)
    intra_sp_corr.append(average_corr_coeff)
  return intra_sp_corr

# Non-normalized Spatial Correlation Calculation
def sp_corr(input,formed_clusters,normalize=False):
  inter_sp_corr = []
  avg_cluster = {}

  if normalize == False:
    input = handle_missing_values(input)
    Clusters = n_nor_get_clusters(input,formed_clusters)
    mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
    for i in range(len(Clusters)):
      avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    for i in range(len(Clusters)):
      for j in range(len(Clusters)):
        if i == j:
          a = n_nor_intra_sp_corr(input,formed_clusters)
          mat[i].iloc[j] = round(a[i],2)
        else:
          corr_coeff = pearson_PM(avg_cluster['avg_cluster' + str(i)], avg_cluster['avg_cluster' + str(j)])
          mat[i].iloc[j] = corr_coeff
          #print('Inter RMSE between cluster',i,'and cluster',j,'is:',inter_rmse.pop())
        

  else:
    # trans_data = datatransformation(input)

    # # Data Normalization
    # trans_data = datanormalization(trans_data)

    # # Adding class centers and cluster numbers as columns to the dataframe
    # trans_data['Cluster'] = formed_clusters

    # # Rearranging the columns in the dataframe
    # trans_data = trans_data[['Cluster'] + [c for c in trans_data if c not in ['Cluster']]]
  
    Clusters, Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)

    # Doing the below step after finding the cluster centers. Otherwise, we'll be calculating mean on date (index) too.
    #trans_data = trans_data.reset_index()

  
    mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
    for i in range(len(Clusters)):
      avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    for i in range(len(Clusters)):
      for j in range(len(Clusters)):
        if i == j:
          a = n_nor_intra_sp_corr(input,formed_clusters)
          mat[i].iloc[j] = round(a[i],2)
        else:
          corr_coeff = pearson_PM(avg_cluster['avg_cluster' + str(i)], avg_cluster['avg_cluster' + str(j)])
          mat[i].iloc[j] = corr_coeff
          #print('Inter RMSE between cluster',i,'and cluster',j,'is:',inter_rmse.pop())
        

  return mat

sp_final1 = sp_corr(data, classification, True)
sp_final1

sp_final2 = sp_corr(data, classification, False)
sp_final2

"""#**Evaluation - Silhouette Coefficient**"""

def silhouette_score1(X, labels, pass_trans_data = True, *, metric="euclidean", sample_size=None, random_state=None, **kwds):
   if pass_trans_data == True:
          k = list(locals().values()) # k will be the list which holds the values of the parameters that are being passed to the function
          #path = str(k[0]) # path will hold the first parameter's value
          #fullpath = os.path.join("/content/drive/MyDrive/Private/Image_Similarity/Transformed_data/" + path + ".csv")
          

          path = ("/content/drive/MyDrive/Private/Image_Similarity/Kmean7cl_trans_data_daily_PCA_11_Norm.csv")
          #X1 = pd.read_csv(fullpath, index_col=[0]) # saved transformed dataframe will be read
          X1 = pd.read_csv(path, index_col=[0])
          X1 = datanormalization(X1) # the data will be normalized

          if sample_size is not None:
            X1, labels = check_X_y(X1, labels, accept_sparse=["csc", "csr"])
            random_state = check_random_state(random_state)
            indices = random_state.permutation(X1.shape[0])[:sample_size]
            if metric == "precomputed":
              X1, labels = X1[indices].T[indices].T, labels[indices]
            else:
              X1, labels = X1[indices], labels[indices]
          return np.mean(silhouette_samples(X1, labels, metric=metric, **kwds))
   
   else:
     X1 = data[X]
     X1 = datatransformation(X)
     X1 = datanormalization(X1)   
     if sample_size is not None:
        X1, labels = check_X_y(X1, labels, accept_sparse=["csc", "csr"])
        random_state = check_random_state(random_state)
        indices = random_state.permutation(X1.shape[0])[:sample_size]
        if metric == "precomputed":
            X1, labels = X1[indices].T[indices].T, labels[indices]
        else:
            X1, labels = X1[indices], labels[indices]
     return np.mean(silhouette_samples(X1, labels, metric=metric, **kwds))

silhouette_Coefficient = silhouette_score1('data', classification)
print("The average silhouette_score is :", silhouette_Coefficient)

"""#**Calinski-Harabasz Calculation Function**#"""

n_nor_get_cluster_centers(data,classification)

# Non-normalized
# Function that creates a dictionary that holds all the cluster centers
def data_centroid(input,formed_clusters): #classification
  Cluster_Centers = {}
  centers = []
  Center = []
  Clusters = n_nor_get_clusters(input,formed_clusters)
  for i in set(formed_clusters):
    Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    centers.append(Cluster_Centers['Cluster_Center' + str(i)])

  centers_sum = sum(centers)
  centers = []
  n = len(Cluster_Centers['Cluster_Center' + str(i)])
  #Centers_sum = np.sum(sum(sum(centers_sum/n)))
  Centers_sum = centers_sum/n
  Center.append(Centers_sum)

  return Center

data_center = data_centroid(data, classification)
data_center

"""#**Intra and Inter Calinski-Harabasz Calculation Function**#
[link text](https://python-bloggers.com/2022/03/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python/)
"""

# Non-normalized 
#Intra Calculation Function
def n_nor_intra_CH(input,formed_clusters):
  sq_diff = []
  intra_CH = []
  Clusters = n_nor_get_clusters(input,formed_clusters)
  Cluster_Centers = n_nor_get_cluster_centers(input,formed_clusters)

  for i in range(len(Clusters)):
    for j in range(len(Clusters['Cluster' + str(i)])):
      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]
      Sq_diff = (diff**2)
      sq_diff.append(Sq_diff)
    Sq_diff_sum = sum(sq_diff)
    sq_diff = []
    n = len(Clusters['Cluster' + str(i)])
    Sum_diff = np.sum(sum(sum(Sq_diff_sum/n)))
    intra_CH.append(Sum_diff)
  return intra_CH

n_nor_intra_CH(data,classification)

"""### Normalized functions ###"""

# Normalized
# Function that creates two dictionaries that hold all the clusters and cluster centers
def nor_get_clusters_and_centers(input,formed_clusters):
  Clusters = {}
  Cluster_Centers = {}
  for i in set(formed_clusters):
    Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))
    Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
  return Clusters,Cluster_Centers

# Normalized

def nor_intra_CH(input,formed_clusters):
  intra_CH = []
  sq_diff = []
  Clusters = n_nor_get_clusters(input,formed_clusters)
  Cluster_Centers = n_nor_get_cluster_centers(input,formed_clusters)
  #Clusters,Cluster_Centers = nor_get_clusters_and_centers(input,formed_clusters)
  for i in range(len(Clusters)):
    for j in range(len(Clusters['Cluster' + str(i)])):
      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]
      Sq_diff = (diff**2)
      sq_diff.append(Sq_diff)
    Sq_diff_sum = sum(sum(sq_diff))
    sq_diff = []
    n = len(Clusters['Cluster' + str(i)])
    Sum_diff = np.sum(sum(sum(Sq_diff_sum/n)))
    intra_CH.append(Sum_diff)
  return intra_CH

nor_intra_CH(data,classification)

def nor_inter_CH(input,formed_clusters):
  inter_CH = []
  sq_diffs = []
  Cluster_Centers = n_nor_get_cluster_centers(input,formed_clusters)
  #Clusters,Cluster_Centers = nor_get_clusters_and_centers(input,formed_clusters)
  for i in range(len(Cluster_Centers )):
    diff = Cluster_Centers['Cluster_Center' + str(i)] - data_center
                           
    Sq_diff = (diff**2)
    sq_diffs.append(Sq_diff)
    Sq_diff_sum = sum(sq_diffs)
    sq_diffs = []
    n = len(Cluster_Centers['Cluster_Center' + str(i)] )
    Sum_diff = np.sum(sum(sum(Sq_diff_sum/n)))
    inter_CH.append(Sum_diff)
  return inter_CH

nor_inter_CH(data,classification)

# Calinski_Harabasz Calculation
def Calinski_Harabasz(input,formed_clusters,normalize=False):
  inter_CH = []
  avg_cluster = {}

  if normalize == False:
    input = handle_missing_values(input)
    Clusters = n_nor_get_clusters(input,formed_clusters)
    mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
    for i in range(len(Clusters)):
      avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    for i in range(len(Clusters)):
      for j in range(len(Clusters)):
        if i == j:
          a = n_nor_intra_CH(input,formed_clusters)
          mat[i].iloc[j] = round(a[i],2)
        else:
            b = nor_inter_CH(input,formed_clusters)
            mat[i].iloc[j] = round(b[i],2)
        

  else:
    trans_data = datatransformation(input)

    # Data Normalization
    trans_data = datanormalization(trans_data)

    # Adding class centers and cluster numbers as columns to the dataframe
    trans_data['Cluster'] = formed_clusters

    # Rearranging the columns in the dataframe
    trans_data = trans_data[['Cluster'] + [c for c in trans_data if c not in ['Cluster']]]
  
    Clusters, Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)

    # Doing the below step after finding the cluster centers. Otherwise, we'll be calculating mean on date (index) too.
    #trans_data = trans_data.reset_index()

  
    mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
    for i in range(len(Clusters)):
      avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    for i in range(len(Clusters)):
      for j in range(len(Clusters)):
        if i == j:
          a = n_nor_intra_CH(input,formed_clusters)
          mat[i].iloc[j] = round(a[i],2)
        else:
            b = nor_inter_CH(input,formed_clusters)
            mat[i].iloc[j] = round(b[i],2)

  return mat

def Calinski_Harabasz(input,formed_clusters):
  inter_CH = []
  avg_cluster = {}
  
  input = handle_missing_values(input)
  Clusters = n_nor_get_clusters(input,formed_clusters)
  mat = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
  for i in range(len(Clusters)):
    avg_cluster['avg_cluster'+str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
  for i in range(len(Clusters)):
    for j in range(len(Clusters)):
      if i == j:
        a = n_nor_intra_CH(input,formed_clusters)
        mat[i].iloc[j] = round(a[i],2)
      else:
          b = nor_inter_CH(input,formed_clusters)
          mat[i].iloc[j] = round(b[i],2)
  return mat

final = Calinski_Harabasz(data,classification)
final



"""#Visualizations

**Map Visualization with original 2-dimensional dataset**
"""

## read the timestep and it's cluster ID from your clustering results
def read_combined_cluster(csvlink,varid):
    with open(csvlink, mode ='r')as file:
       # reading the CSV file
       csvFile = pd.read_csv(file)
       if(len(varid)>0):    
          id = csvFile['clusterid']
          time_step = csvFile['time_step']
          days = np.zeros(len(id))-999 
 
          for i in range(len(id)):   
              days[i] = int(time_step[i][5:])
                            
                            
    return days,id

##called by plot_map to plot the coastline on the map
def plotcoastline(color='k'):
    lon_c = []
    lat_c = []
    # with open('coast.txt') as f:
    with open('/content/drive/MyDrive/ECRP_Data_Science/Implementation and Results/Jianwu_test/visualizations/coast.txt') as f:
        for line in f:
            data = line.split()
            lon_c.append(float(data[0])-360)
            lat_c.append(float(data[1]))
    plt.plot(lon_c,lat_c,color=color)
    return [lon_c,lat_c]

# plot the map of orignal data (mean value and standard deviation) for each cluster on the specified subpanel
def plot_map(var, var_range,lon0,lat0,fig,panel,cmap0,colorbar,title,ifcontourf):
  ax=plt.subplot(panel)
  if(ifcontourf):  
     p1=plt.contourf(lon0,lat0,var,cmap=cmap0,levels=np.arange(var_range[0],var_range[1],(var_range[1]-var_range[0])/31),extend = 'both') 
     p1.ax.tick_params(labelsize=12)
     plotcoastline(color='k',)
     plt.xlim([min(lon0),max(lon0)])  
     plt.ylim([min(lat0),max(lat0)])    
     plt.title(title,loc='left')   
     plt.xlabel('Longitude')
     plt.ylabel('Latitude')
     if(colorbar):
        ticks = np.linspace(var_range[0], var_range[1], 8, endpoint=True)
        cax = ax.inset_axes([1.04, 0, 0.02, 1], transform=ax.transAxes)
        cb2 = fig.colorbar(p1,orientation='vertical',ax=ax,cax=cax,ticks=ticks)
        cb2.ax.tick_params(labelsize=9)
        
  else:
     p1=ax.contour(lon0,lat0,var,cmap=cmap0,levels=np.arange(var_range[0],var_range[1],(var_range[1]-var_range[0])/11),extend = 'both',linewidth=0.6) 
     p1.ax.tick_params(labelsize=12)
     plt.title(title,loc='right')
     if(colorbar):
        ticks = np.linspace(var_range[0], var_range[1], 12, endpoint=True)
        cax = ax.inset_axes([1.23, 0, 0.02, 1], transform=ax.transAxes)
        cb2 = fig.colorbar(p1,orientation='vertical',ax=ax,cax=cax,ticks=ticks)
        cb2.ax.tick_params(labelsize=9)
  ax.set_aspect(0.65)      

        #cbar = fig.colorbar(p1)
  return [p1]

"""**Specify the location and file name of the raw dataset**"""

# input_dir = './'
# fig_dir = './'
# data_file = '/content/drive/MyDrive/Multivariate Data Independent Study/ERA5_meteo_sfc_2021_daily.nc'

input_dir = '/content/drive/MyDrive/Private/Image_Similarity'
fig_dir = '/content/drive/MyDrive/Data/new_ERA5_meteo_sfc_2021_daily.nc'
year_str='2021'
data_file = '/new_ERA5_meteo_sfc_'+year_str+'_daily.nc'

fcase = input_dir+data_file
##Read time, lat and lon for visualization
fin = Dataset(fcase, "r")
time = np.squeeze(fin['time'][:])
lat0 = np.squeeze(fin['latitude'][:])
lon0 = np.squeeze(fin['longitude'][:])

"""**Specify the variable name and it's unit coefficient here**"""

varids=['sst','slhf','u10','v10','sshf','sp']
ccoefs=[1,-1./3600,1,1,-1./3600,1e-2]

"""**Read the cluster ID assiged to each time step from the clustering results. You might need to modify this cell to fit your csv format**"""

import pandas as pd
# cluster_filename='Kmeans'
# #cluster_filename='PCA_Combined_Norm_Clusters_5'
# cluster_link = cluster_filename+'.csv'

cluster_filename="Agglo_Daily_PCA11_Norm"
cluster_link = input_dir+'/cluster/'+cluster_filename+'.csv'

[days,id]=read_combined_cluster(cluster_link,'OK')


[days,id]=read_combined_cluster(cluster_link,'OK') 
n_cluster = max(id)-min(id)+1
width = 0.3
height = 0.5
panels=[(0.06, 0.08,width, height), (0.39, 0.08,width, height),(0.72, 0.08,width, height),
        (0.06, 0.38, width, height), (0.39, 0.38, width, height),(0.72, 0.38, width, height),
        (0.06, 0.68, width, height),(0.39, 0.68, width, height), (0.72, 0.68, width, height),
]
panels=[(0.06, 0.08,width, height), (0.5, 0.08,width, height),(0.05, 0.5,0.9, 0.3),]


n_cluster = max(id)-min(id)+1
print('total clusters: ',n_cluster)

days

"""**Specify the total subpanels for the visualized clusters. 4 rows x 2 columns for now:**"""

panels=np.arange(421,428,dtype=int) #4 rows 2 columns depending on clusters (how much panels you want?)
#You can choose different colormaps
cmap0='coolwarm'

"""**Start plotting each individual variables: **"""

for ivar in range(len(varids)):
  fig=plt.figure(ivar+1,figsize=[19,22])  
  varid = varids[ivar]  
  var_range=[0,1]
  ccoef = ccoefs[ivar] 
  colorbar = True
  var0 = ccoef*np.squeeze(fin[varid][:])
  ## Fix the range of the values for all of the clusters
  var_range[0]= np.nanmin(var0)+(np.nanmax(var0)-np.nanmin(var0))*0.05
  var_range[1]=np.nanmax(var0)-(np.nanmax(var0)-np.nanmin(var0))*0.05
  print('varid:',varid)
  print('var_range:',var_range)
  ipanel = 0
  for icluster in range(min(id),max(id)+1):
      days_icluster = days[np.where(id==icluster)[0]]
      ndays_icluster = len(days_icluster)
      ## calculate the mean value and standard deviation from the time series of the variable at each (lat,lon)  
      time_icluster = np.zeros(ndays_icluster)
      var_icluster = np.zeros([ndays_icluster,len(lat0),len(lon0)])  
      for iday in range(ndays_icluster):
          istep = np.where(time==days_icluster[iday])
          time_icluster[iday] = time[istep]
          var_icluster[iday]=  np.squeeze(var0[istep])       
      var_mean_icluster = np.nanmean(var_icluster,axis=0)
      var_std_icluster = 2*np.nanstd(var_icluster,axis=0)
           
      title1= 'cluster'+str(icluster)+' n='+ str(len(time_icluster))+' '+varid+' '+ f'{np.nanmean(var_mean_icluster):9.3f}' 
      p = plot_map(var_mean_icluster, var_range,lon0,lat0,fig,panels[ipanel],cmap0,colorbar,title1,ifcontourf=True)
      if(np.nanstd(var_std_icluster)>1e-6*np.nanmean(var_std_icluster)):
          p = plot_map(var_std_icluster, [np.nanmin(var_std_icluster),np.nanmax(var_std_icluster)] ,lon0,lat0,fig,panels[ipanel],'BrBG',colorbar,'2std='+f'{np.nanmean(var_std_icluster):9.3f}',ifcontourf=False)
      else:
          print(np.nanstd(var_std_icluster),np.nanmean(var_std_icluster))
          plt.title('2std='+f'{np.nanmean(var_std_icluster):9.3f}',loc='right')
      ipanel += 1      
  #fig.savefig(varid+'_'+cluster_filename+'.jpg')


#plt.show()

"""**Plot the time series of the cluster ID:**"""

fig=plt.figure(ivar+2,figsize=[18,6])
plt.bar(days,id+0.1,width=0.3)
plt.tick_params(labelsize=12)
plt.xlim([min(days),max(days)])
plt.ylabel('Cluster ID',fontsize=14)
plt.xlabel('Time steps',fontsize=14)