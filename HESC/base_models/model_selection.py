# -*- coding: utf-8 -*-
"""Model_selection.ipynb

Automatically generated by Colaboratory.

    

# **Model Selection**

This notebook contains all the models that were pre-selected based on the category they belonged. These include KMeans,
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Install needed libraries"""

# Install dask.dataframe
!pip install "dask[dataframe]"
!pip install netCDF4

import pandas as pd
import numpy as np
import netCDF4 as nc
import time
from matplotlib import pyplot as plt
from matplotlib.pyplot import cm

from sklearn.cluster import MiniBatchKMeans, KMeans, SpectralClustering
from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score
from sklearn.metrics import calinski_harabasz_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn import preprocessing

from sklearn.mixture import GaussianMixture
import xarray as xr
from scipy.cluster.hierarchy import dendrogram, linkage
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from yellowbrick.cluster import KElbowVisualizer

import warnings
warnings.filterwarnings("ignore")



## This function will will pre-process our daily data for DEC model as numpy array

def data_preprocessing(data_path):
  rdata_daily = xr.open_dataset(data_path)    # data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'
  rdata_daily_np_array = np.array(rdata_daily.to_array())   # the shape of the dailt data is (7, 365, 41, 41)
  rdata_daily_np_array_latitude = np.concatenate((rdata_daily_np_array, np.zeros((7, 365, 41,7), dtype=int)), axis=3)
  rdata_daily_np_array_longitude = np.concatenate((rdata_daily_np_array_latitude, np.zeros((7, 365, 7, 48), dtype=int)), axis=2)
  rdata_daily_np_array = rdata_daily_np_array_longitude
  rdata_daily_np_array_T = rdata_daily_np_array.transpose(1,0,2,3)   # transform the dailt data from (7, 365, 41, 41) to (365, 7, 41, 41)
  overall_mean = np.nanmean(rdata_daily_np_array_T[:, :, :, :])
  for i in range(rdata_daily_np_array_T.shape[0]):
    for j in range(rdata_daily_np_array_T.shape[1]):
      for k in range(rdata_daily_np_array_T.shape[2]):
        for l in range(rdata_daily_np_array_T.shape[3]):
          if np.isnan(rdata_daily_np_array_T[i, j, k, l]):
            #print("NAN data in ", i, j, k, l)
            rdata_daily_np_array_T[i, j, k, l] = overall_mean  # np.nanmean(rdata_daily_np_array_T[i, j, k, :])
  rdata_daily_np_array_T = rdata_daily_np_array_T.transpose(0,2,3,1)
  rdata_daily_np_array_T_R = rdata_daily_np_array_T.reshape((rdata_daily_np_array_T.shape[0], -1))  # transform the dailt data from (365, 7, 41, 41) to (365, 11767)
  min_max_scaler = preprocessing.MinMaxScaler() # calling the function
  rdata_daily_np_array_T_R_nor = min_max_scaler.fit_transform(rdata_daily_np_array_T_R)   # now normalize the data, otherwise the loss will be very big
  #rdata_daily_np_array_T_R_nor = np.float32(rdata_daily_np_array_T_R_nor)    # convert the data type to float32, otherwise the loass will be out-of-limit
  rdata_daily_np_array_T_R_nor_R = rdata_daily_np_array_T_R_nor.reshape((rdata_daily_np_array_T_R_nor.shape[0], rdata_daily_np_array.shape[2], rdata_daily_np_array.shape[3], rdata_daily_np_array.shape[0]))
  return rdata_daily_np_array_T_R_nor, rdata_daily_np_array_T_R_nor_R

data_nor, data_clustering = data_preprocessing('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc')

data_nor.shape, data_clustering.shape

X = data_nor

"""# **Evaluation metrics used**

**Silhouette**
"""

def silhouette_score1(X, labels, *, metric="cosine", sample_size=None, random_state=None, **kwds):
 return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))

"""**Davies bouldin**"""

def davies_bouldin_score(X, labels):
 return print("Davies-Bouldin score is ", davies_bouldin_score(X, labels))

"""**Calinski Harabasz**"""

def calinski_harabasz_score(X, labels):
 return print("Calinski Harabasz score is ", calinski_harabasz_score(X, labels))

"""**RMSE**"""

def total_rmse(data_path,formed_clusters):
  #processed_data = data_preprocessing(data_path)
  processed_data = data_nor
  trans_data = pd.DataFrame(processed_data)
  trans_data['Cluster'] = formed_clusters

  # Normalized
  # Function that creates two dictionaries that hold all the clusters and cluster centers
  def nor_get_clusters_and_centers(input,formed_clusters):
    Clusters = {}
    Cluster_Centers = {}
    for i in set(formed_clusters):
      Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))
      Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)
    return Clusters,Cluster_Centers

  intra_rmse = []
  sq_diff = []
  Clusters,Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)
  for i in range(len(Clusters)):
    for j in range(len(Clusters['Cluster' + str(i)])):
      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]
      Sq_diff = (diff)**2
      sq_diff.append(Sq_diff)

  Sq_diff_sum = np.sum(np.sum(sq_diff))
  rmse = np.sqrt(Sq_diff_sum/data_nor.shape[0])
  return rmse

"""**Avg Cluster Variance**"""

def avg_var(norm_data, result):
  trans_data = pd.DataFrame(data_nor)
  trans_data['Cluster'] = result
  Clusters = {}
  Cluster_Centers = {}
  for i in set(result):
    Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))

  variances = pd.DataFrame(columns=range(len(Clusters)),index=range(2))
  for i in range(len(Clusters)):
      variances[i].iloc[0] = np.var(Clusters['Cluster' + str(i)])
      variances[i].iloc[1] = Clusters['Cluster' + str(i)].shape[0]

  var_sum = 0
  for i in range(7):
      var_sum = var_sum + (variances[i].iloc[0] * variances[i].iloc[1])

  var_avg = var_sum/data_nor.shape[0]


  return (print("The Average variance is:", var_avg))

"""**Avg Inter-cluster distance**"""

def avg_inter_dist(norm_data, clustering_results):

  from scipy.spatial.distance import cdist,pdist
  n_clusters = 7
  trans_data = pd.DataFrame(norm_data)
  trans_data['Cluster'] = clustering_results
  Clusters = {}
  Cluster_Centers = {}
  for i in set(clustering_results):
    Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))

  distance_matrix = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))
  for i in range(len(Clusters)):
    for j in range(len(Clusters)):
      if i == j:
        #distance_matrix[i].iloc[j] = 0
        distance_intra = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')
        distance_matrix[i].iloc[j] = np.max(distance_intra)
      elif i > j:
        continue
      else:
        distance = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')
        distance_matrix[i].iloc[j] = np.min(distance)
        distance_matrix[j].iloc[i] = np.min(distance)

  sum_min = 0
  for i in range(n_clusters):
      sum_min = sum_min + np.min(distance_matrix[i])

  avg_inter = sum_min/n_clusters

  return (print("The Average Inter-cluster dist is:", avg_inter))

"""# **Number of clusters using KMeans**"""

# Elbow Method for Agglomerative Clustering
model = KMeans()
# k is range of number of clusters.
visualizer = KElbowVisualizer(model, k=(2,20), timings= True)
visualizer.fit(data_nor)        # Fit data to visualizer
visualizer.show()        # Finalize and render figure

"""# **KMEANS**"""

results = KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
       n_clusters=7, n_init=10,
       random_state=0, tol=0.0001, verbose=0)

results.fit_predict(data_nor)

def kmean(data):
  results = KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
       n_clusters=7, n_init=10,
       random_state=0, tol=0.0001, verbose=0)

  labels = results.fit_predict(data)

  #to print the cluster centroids
  centroids = results.cluster_centers_
  #print(centroids)

  #To get the inertia
  inertia = results.inertia_
  #print(inertia)

  return labels

result_km = kmean(data_nor)

results.inertia_

silh = silhouette_score1(data_nor,  result_km)
u,indices = np.unique(result_km,return_counts = True)

print("Silhouette score is ", silh)
print("Cluster index ", u, "and Cluster Sizes: ", indices)



silh = silhouette_score1(data_nor,  result_km)
u,indices = np.unique(result_km,return_counts = True)

print("Silhouette score is ", silh)
print("Cluster index ", u, "and Cluster Sizes: ", indices)

from sklearn.metrics import davies_bouldin_score

db = davies_bouldin_score(data_nor, result_km)
print("Davies-Bouldin score is ", db)

from sklearn.metrics import calinski_harabasz_score
ch = calinski_harabasz_score(data_nor, result_km)
print("Davies-Bouldin score is ", ch)

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_km))

print("Variance is ", avg_var(data_nor, result_km))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_km))

def best_clustering(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    results = KMeans(n_clusters = 7).fit_predict(X)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, results)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, results])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  return sil[max_index]

silhouette, result_km = best_clustering(20)

silh = silhouette_score1(data_nor,  result_km)
u,indices = np.unique(result_km,return_counts = True)

print("Silhouette score is ", silh)
print("Cluster index ", u, "and Cluster Sizes: ", indices)

from sklearn.metrics import davies_bouldin_score

db = davies_bouldin_score(data_nor, result_km)
print("Davies-Bouldin score is ", db)

from sklearn.metrics import calinski_harabasz_score
ch = calinski_harabasz_score(data_nor, result_km)
print("Davies-Bouldin score is ", ch)

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_km))

print("Variance is ", avg_var(data_nor, result_km))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_km))

"""# **Hierachical Clustering**"""

result_a = AgglomerativeClustering(n_clusters = 7, affinity = 'euclidean', linkage ='ward').fit_predict(data_nor)



silh = silhouette_score1(data_nor,  result_a)
u,indices = np.unique(result_a,return_counts = True)

print("Silhouette score is ", silh)
print("Cluster index ", u, "and Cluster Sizes: ", indices)

from sklearn.metrics import davies_bouldin_score

db = davies_bouldin_score(data_nor, result_a)
print("Davies-Bouldin score is ", db)

from sklearn.metrics import calinski_harabasz_score
ch = calinski_harabasz_score(data_nor, result_a)
print("Davies-Bouldin score is ", ch)

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_a))

print("Variance is ", avg_var(data_nor, result_a))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_a))

def best_clustering5(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    results = AgglomerativeClustering(n_clusters = 7, affinity = 'euclidean', linkage ='ward').fit_predict(X)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, results)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, results])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  #return sil[max_index]
  return sil[max_index]

silhouette, result_a = best_clustering5(50)

silh = silhouette_score1(data_nor,  result_a)
u,indices = np.unique(result_a,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)

print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_a))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_a))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_a))

print("Variance is ", avg_var(data_nor, result_a))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_a))

"""# **spectral clusterin**g


"""

# spectral clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import SpectralClustering
from matplotlib import pyplot

# define the model
# model = SpectralClustering(n_clusters=2)
model = SpectralClustering(n_clusters=7, affinity='nearest_neighbors', random_state=0)
# fit model and predict clusters
result_ssc = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(result_ssc)
# create scatter plot for samples from each cluster
for cluster in clusters:
 # get row indexes for samples with this cluster
 row_ix = where(result_ssc == cluster)
 # create scatter of these samples
 pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()





silh = silhouette_score1(data_nor,  result_ssc)
u,indices = np.unique(result_ssc,return_counts = True)

print("Silhouette score is ", silh)
print("Cluster index ", u, "and Cluster Sizes: ", indices)

from sklearn.metrics import davies_bouldin_score

db = davies_bouldin_score(data_nor, result_ssc)
print("Davies-Bouldin score is ", db)

from sklearn.metrics import calinski_harabasz_score
ch = calinski_harabasz_score(data_nor, result_ssc)
print("Davies-Bouldin score is ", ch)

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_ssc))

print("Variance is ", avg_var(data_nor, result_ssc))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_ssc))

def best_clustering5(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    model = SpectralClustering(n_clusters=7, affinity='nearest_neighbors', random_state=500)
    # fit model and predict clusters
    yhat_sc = model.fit_predict(X)

    silhouette_avg_rdata_daily = silhouette_score1(X, yhat_sc)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, yhat_sc])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  #return sil[max_index]
  return sil[max_index]

silhouette, result_sc = best_clustering5(40)

silh = silhouette_score1(data_nor,  result_sc)
u,indices = np.unique(result_sc,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)



print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_sc))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_sc))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_sc))

print("Variance is ", avg_var(data_nor, result_sc))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_sc))

"""# ** HDBSCAN**"""

# dbscan clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import DBSCAN
from matplotlib import pyplot

model = DBSCAN(eps=0.80, min_samples=10)
# fit model and predict clusters
result_sdb = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(result_sdb)
# create scatter plot for samples from each cluster
for cluster in clusters:
 # get row indexes for samples with this cluster
 row_ix = where(result_sdb == cluster)
 # create scatter of these samples
 pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()



silh = silhouette_score1(data_nor,  result_sdb)
u,indices = np.unique(result_sdb,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)



print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_sdb))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_sdb))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_sdb))

print("Variance is ", avg_var(data_nor, result_sdb))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_sdb))



def best_clustering(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    model = DBSCAN(eps=0.30, min_samples=9)
    # fit model and predict clusters
    yhat_db = model.fit_predict(X)
    # retrieve unique clusters
    clusters = unique(yhat_db)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, yhat_db)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, yhat_ap])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  return sil[max_index]

#silhouette, result_db = best_clustering(10)

# silh = silhouette_score1(data_nor,  result_db)
# u,indices = np.unique(result_db,return_counts = True) # sc=0.3412 st 64
# # u,indices
# print(silh)
# # print(u,indices)



# print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_db))

# print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_db))

# print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_db))

# print("Variance is ", avg_var(data_nor, result_db))

# print("Inter-cluster distance ", avg_inter_dist(data_nor, result_db))

"""# **Affinity Propagation**"""

from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import AffinityPropagation
from matplotlib import pyplot

def pca1(data,n): # data is data to be input , n is the number of components
  pca = PCA(n_components=n)
  pca.fit(data)

  # Get pca scores
  pca_scores = pca.transform(data)

  # Convert pca_scores to a dataframe
  scores_df = pd.DataFrame(pca_scores)

  # Round to two decimals
  scores_df = scores_df.round(2)

  # Return scores
  return scores_df

#X = pca1(data_nor_eval, 3)



# define the model
model = AffinityPropagation(damping=0.9, preference=-1800, random_state=0)
# fit the model
model.fit(X)
# assign a cluster to each example
result_sap = model.predict(X)
# retrieve unique clusters
clusters = unique(result_sap)
# create scatter plot for samples from each cluster
for cluster in clusters:
 # get row indexes for samples with this cluster
 row_ix = where(result_sap == cluster)
 # create scatter of these samples
 pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()



silh = silhouette_score1(data_nor,  result_sap)
u,indices = np.unique(result_sap,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)



print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_sap))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_sap))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_sap))

print("Variance is ", avg_var(data_nor, result_sap))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_sap))

def best_clustering(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    model = AffinityPropagation(damping=0.9)
    # fit the model
    model.fit(X)
    # assign a cluster to each example
    result_ap = model.predict(X)
    # retrieve unique clusters
    clusters = unique(result_ap)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, result_ap)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, result_ap])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  return sil[max_index]

silhouette, result_ap = best_clustering(20)

silh = silhouette_score1(data_nor,  result_ap)
u,indices = np.unique(result_ap,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)



print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_ap))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_ap))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_ap))

print("Variance is ", avg_var(data_nor, result_ap))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_ap))

"""# **BIRCH**"""

# birch clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import Birch
from matplotlib import pyplot

model = Birch(threshold=0.01, n_clusters=7)
# fit the model
model.fit(X)
# assign a cluster to each example
result_sb = model.predict(X)
# retrieve unique clusters
clusters = unique(result_sb)
# create scatter plot for samples from each cluster
for cluster in clusters:
 # get row indexes for samples with this cluster
 row_ix = where(result_sb == cluster)
 # create scatter of these samples
 pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()



silh = silhouette_score1(data_nor,  result_sb)
u,indices = np.unique(result_sb,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)



print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_sb))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_sb))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_sb))

print("Variance is ", avg_var(data_nor, result_sb))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_sb))

def best_clustering(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    model = Birch(threshold=0.01, n_clusters=7)
    # fit the model
    model.fit(X)
    # assign a cluster to each example
    yhat_b = model.predict(X)
    # retrieve unique clusters
    clusters = unique(yhat_b)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, yhat_b)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, yhat_b])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  return sil[max_index]

silhouette, result_b = best_clustering(20)

silh = silhouette_score1(data_nor,  result_b)
u,indices = np.unique(result_b,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)

print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_b))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_b))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_b))

print("Variance is ", avg_var(data_nor, result_b))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_b))

"""# **OPTICS**"""

# optics clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import OPTICS
from matplotlib import pyplot

# define the model
model = OPTICS(eps=0.8, min_samples=10)
# fit model and predict clusters
result_sOP = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(result_sOP)
# create scatter plot for samples from each cluster
for cluster in clusters:
 # get row indexes for samples with this cluster
 row_ix = where(result_sOP == cluster)
 # create scatter of these samples
 pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

silh = silhouette_score1(data_nor,  result_sOP)
u,indices = np.unique(result_sOP,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)

print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_sOP))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_sOP))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_sOP))

print("Variance is ", avg_var(data_nor, result_sOP))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_sOP))

def best_clustering(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    model = OPTICS(eps=0.4, min_samples=10)
    # fit model and predict clusters
    yhat_OP = model.fit_predict(X)
    # retrieve unique clusters
    clusters = unique(yhat_OP)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, yhat_OP)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, yhat_OP])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  return sil[max_index]

# silhouette, result_op = best_clustering(20)

silh = silhouette_score1(data_nor,  yhat_OP)
u,indices = np.unique(yhat_OP,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)

print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, yhat_OP))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, yhat_OP))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', yhat_OP))

print("Variance is ", avg_var(data_nor, yhat_OP))

print("Inter-cluster distance ", avg_inter_dist(data_nor, yhat_OP))

"""# **Gaussian Mixture Model (GMM)**"""

# gaussian mixture clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.mixture import GaussianMixture
from matplotlib import pyplot

# define the model
model = GaussianMixture(n_components=7)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat_gmm = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat_gmm)
# create scatter plot for samples from each cluster
for cluster in clusters:
 # get row indexes for samples with this cluster
 row_ix = where(yhat_gmm == cluster)
 # create scatter of these samples
 pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

silh = silhouette_score1(data_nor,  yhat_gmm)
u,indices = np.unique(yhat_gmm,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)

print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, yhat_gmm))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, yhat_gmm))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', yhat_gmm))

print("Variance is ", avg_var(data_nor, yhat_gmm))

print("Inter-cluster distance ", avg_inter_dist(data_nor, yhat_gmm))

def best_clustering(n):

  n # Number of single models used
    #MIN_PROBABILITY = 0.6 # The minimum threshold of occurances of datapoints in a cluster

  sil = []
  sil_score = []
  # Generating a "Cluster Forest"
  #clustering_models = NUM_alg * [main()]#
  for i in range(n):
    model = GaussianMixture(n_components=7)
    # fit the model
    model.fit(X)
    # assign a cluster to each example
    yhat_gmm1 = model.predict(X)
    # retrieve unique clusters
    clusters = unique(yhat_gmm1)
    #results = main()
    #sil.append(results)
    #print(results)
    silhouette_avg_rdata_daily = silhouette_score1(X, yhat_gmm1)
    sil_score.append(silhouette_avg_rdata_daily)

    sil.append([silhouette_avg_rdata_daily, yhat_gmm1])

  print("Our silhouettes range is: ", sil_score)

  max_index = 0

  for j in range(len(sil)):
    if(sil[j][0]>=sil[max_index][0]):
      max_index = j
  print(sil[max_index])

  return sil[max_index]

silhouette, result_gmm = best_clustering(10)

silh = silhouette_score1(data_nor,  result_gmm)
u,indices = np.unique(yhat_gmm,return_counts = True) # sc=0.3412 st 64
# u,indices
print(silh)
print(u,indices)



print("Davies-Bouldin score is ", davies_bouldin_score(data_nor, result_gmm))

print("Calinski Harabas score is ", calinski_harabasz_score(data_nor, result_gmm))

print("RMSE score is ", total_rmse('/content/drive/MyDrive/Data/ERA5_meteo_sfc_2021_daily.nc', result_gmm))

print("Variance is ", avg_var(data_nor, result_gmm))

print("Inter-cluster distance ", avg_inter_dist(data_nor, result_gmm))
