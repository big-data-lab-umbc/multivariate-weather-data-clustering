{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Multivariate Climate Data Clustering using CNN-LSTM Encoder Model**\n","Here we are dealing with climate data that comprises spatial information, time information, and scientific values. The dataset contains the value of 7 parameters for a region of 41 longitudes and 41 latitudes for 365 days in a year.\n","\n","Our goal is to create meaningful clusters of 365 days based on the values of these 7 parameters. As the data dimension is high we planned to use deep learning-based models to generate the latent representation of each day and then generate clusters for 365 days.\n","\n","As our dataset has spatial and temporal features, therefore we have developed a new CNN-LSTM model that learns the spatial features first from the dataset by considering the data of each day as an image of 41x41x7 dimension. The output of the CNN model is then passed through the LSTM model to learn the temporal features and generate the final latent variables. Using the final latent variables we perform the clustering to 365 days into different groups. During the training process, we try to reduce the clustering loss by learning better latent representations.  "],"metadata":{"id":"Zka-keupt31o"}},{"cell_type":"markdown","source":["# **1. Model Creation:**\n","This CNN-LSTM model considers our daily data as an image of size 41x41x7. The model takes a 365x1x41x41x7 NumPy array as input and applies convolution layers, and max pooling layers to learn the spatial features. Then applies the LSTM layers with dropout and dense layers to learn the temporal features. The output of this model is 256 latent features for each day's data."],"metadata":{"id":"6_rA_Q3iks0J"}},{"cell_type":"code","source":["import keras,os\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, AveragePooling2D, LSTM, Activation, ConvLSTM2D, TimeDistributed, Input\n","import numpy as np\n","\n","\n","def get_conv_cnn(input_batch):\n","\n","  conv_model = TimeDistributed(Conv2D(64, (3,3), padding='same', activation='tanh') )(input_batch)\n","  conv_model = TimeDistributed(Conv2D(64, (3,3), padding='same', activation='tanh') )(conv_model)\n","  conv_model = TimeDistributed(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))(conv_model)\n","  conv_model = TimeDistributed(Conv2D(256, (3,3), padding='same', activation='tanh') )(conv_model)\n","  conv_model = TimeDistributed(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))(conv_model)\n","  conv_model = TimeDistributed(Flatten())(conv_model)\n","\n","  return conv_model\n","\n","\n","def myCNNModel(input_dims):\n","  input_batch = Input(shape = input_dims)\n","  image_features = get_conv_cnn(input_batch)\n","  lstm_network = LSTM(512, return_sequences=True,dropout=0.1,recurrent_dropout=0.1)(image_features)\n","  lstm_network = keras.layers.LSTM(512, return_sequences=False,dropout=0.1,recurrent_dropout=0.1)(lstm_network)\n","  lstm_network = keras.layers.Dense(256,activation='sigmoid')(lstm_network)\n","\n","  full_network = keras.Model([input_batch],lstm_network)\n","  return full_network\n"],"metadata":{"id":"UTo1A7DSjUcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from time import time\n","import numpy as np\n","import keras.backend as K\n","from tensorflow.keras.layers import Layer, InputSpec\n","from keras.layers import Dense, Input, Dropout\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras import callbacks\n","from keras.initializers import VarianceScaling\n","from sklearn.cluster import KMeans\n","\n","\n","class ClusteringLayer(Layer):\n","    \"\"\"\n","    # Arguments\n","        n_clusters: number of clusters.\n","        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n","        alpha: parameter in Student's t-distribution. Default to 1.0.\n","    # Input shape\n","        2D tensor with shape: `(n_samples, n_features)`.\n","    # Output shape\n","        2D tensor with shape: `(n_samples, n_clusters)`.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(ClusteringLayer, self).__init__(**kwargs)\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.initial_weights = weights\n","        self.input_spec = InputSpec(ndim=2)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 2\n","        input_dim = input_shape[1]\n","        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n","        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='normal', name='clusters') #glorot_uniform\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","        self.built = True\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n","                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n","        Arguments:\n","            inputs: the variable containing data, shape=(n_samples, n_features)\n","        Return:\n","            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n","        \"\"\"\n","        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n","        q **= (self.alpha + 1.0) / 2.0\n","        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n","        return q\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) == 2\n","        return input_shape[0], self.n_clusters\n","\n","    def get_config(self):\n","        config = {'n_clusters': self.n_clusters}\n","        base_config = super(ClusteringLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class CNNModel(object):\n","    def __init__(self,\n","                 dims,\n","                 n_clusters=10,\n","                 alpha=1.0,\n","                 init='glorot_uniform'):\n","\n","        super(CNNModel, self).__init__()\n","\n","        self.dims = dims\n","        #self.input_dim = dims[0]\n","        self.n_stacks = len(self.dims) - 1\n","\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.model_cnn = myCNNModel(self.dims);\n","        print(\"====Model created=====\")\n","\n","        # prepare the CNN model with cnn_layers+clustering _layer\n","        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.model_cnn.output)\n","        print(\"====== clustering layer created ========\")\n","        self.model = Model(inputs=self.model_cnn.input, outputs=clustering_layer)\n","\n","    def load_weights(self, weights):  # load weights\n","        self.model.load_weights(weights)\n","\n","    def extract_features(self, x):\n","        return self.model_cnn.predict(x)\n","\n","    def predict(self, x):  # predict cluster labels using the output of clustering layer\n","        q = self.model.predict(x, verbose=0)\n","        return q.argmax(1)\n","\n","    @staticmethod\n","    def target_distribution(q):\n","        weight = q ** 2 / q.sum(0)\n","        return (weight.T / weight.sum(1)).T\n","\n","    def compile(self, optimizer='sgd', loss='kld'):\n","        self.model.compile(optimizer=optimizer, loss=loss)\n","\n","    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n","            update_interval=140, save_dir='./results/temp'):\n","\n","        print('Update interval', update_interval)\n","        #save_interval = int(x.shape[0] / batch_size) * 5  # 5 epochs\n","        save_interval = 500\n","        print('Save interval', save_interval)\n","\n","        # Step 1: initialize cluster centers using k-means\n","        t1 = time()\n","        print('Initializing cluster centers with k-means.')\n","        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n","        y_pred = kmeans.fit_predict(self.model_cnn.predict(x))\n","        y_pred_last = np.copy(y_pred)\n","        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n","\n","        # Step 2: deep clustering\n","        # logging file\n","        import csv\n","        logfile = open(save_dir + '/cnn_log.csv', 'w')\n","        logwriter = csv.DictWriter(logfile, fieldnames=['iter','loss'])\n","        logwriter.writeheader()\n","\n","        loss = 0\n","        index = 0\n","        index_array = np.arange(x.shape[0])\n","        for ite in range(int(maxiter)):\n","            if ite % update_interval == 0:\n","                q = self.model.predict(x, verbose=0)\n","                p = self.target_distribution(q)  # update the auxiliary target distribution p\n","\n","                # evaluate the clustering performance\n","                y_pred = q.argmax(1)\n","                print(\"#### inside iteration ### \", ite)\n","\n","                # check stop criterion\n","                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n","                print(\"##### Prediction in side the iter and the delta_label is \", delta_label, \", cluster change= \", np.sum(y_pred != y_pred_last))\n","                y_pred_last = np.copy(y_pred)\n","                if ite > 0 and delta_label < tol:\n","                    print('delta_label ', delta_label, '< tol ', tol)\n","                    print('Reached tolerance threshold. Stopping training.')\n","                    logfile.close()\n","                    break\n","\n","            # train on batch\n","            # if index == 0:\n","            #     np.random.shuffle(index_array)\n","            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n","            loss = self.model.train_on_batch(x=x[idx], y=p[idx])\n","            print(\"#### the loss is \", loss)\n","            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n","\n","            # save intermediate model\n","            if ite % save_interval == 0:\n","                print('saving model to:', save_dir + '/CNN_model_' + str(ite) + '.h5')\n","                self.model.save_weights(save_dir + '/CNN_model_' + str(ite) + '.h5')\n","\n","            ite += 1\n","\n","        # save the trained model\n","        logfile.close()\n","        file_name  = \"/CNN_model_final_\" + str(round(time()))+ \".h5\"\n","        print('saving model to:', save_dir + file_name)\n","        self.model.save_weights(save_dir + file_name)\n","\n","        return y_pred"],"metadata":{"id":"swC8lZ-c38sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install netCDF4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0g8xNdNJQjZ","executionInfo":{"status":"ok","timestamp":1680475510506,"user_tz":240,"elapsed":12286,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"b1d15019-de3f-4c30-dcc4-1dd0c23ac173"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting netCDF4\n","  Downloading netCDF4-1.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cftime\n","  Downloading cftime-1.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from netCDF4) (1.22.4)\n","Installing collected packages: cftime, netCDF4\n","Successfully installed cftime-1.6.2 netCDF4-1.6.3\n"]}]},{"cell_type":"code","source":["import netCDF4 as nc\n","import pandas as pd\n","import numpy as np\n","import xarray as xr\n","import datetime\n","import datetime as dt\n","from netCDF4 import date2num,num2date\n","from math import sqrt"],"metadata":{"id":"iUjV0nswxgqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W97e8KvKxhZu","outputId":"8fd091c9-6724-4f9e-f293-8457998ce8db","executionInfo":{"status":"ok","timestamp":1680475532847,"user_tz":240,"elapsed":19813,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **2.Data preparation**\n","The dataset has some NaN values in the SST variable. To replace these NaN values we have used the mean value of the longitude and latitude of that day. The function returns 2 NumPy arrays one with size (365, 11767) and another with size (365, 1, 41, 41, 7). The array with size (365, 11767) calculates the silhouette score, and the array with size (365, 1, 41, 41, 7) trains the model."],"metadata":{"id":"NOq53SC30Sod"}},{"cell_type":"code","source":["from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def data_preprocessing(data_path):\n","  rdata_daily = xr.open_dataset(data_path)    # data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'\n","  rdata_daily_np_array = np.array(rdata_daily.to_array())   # the shape of the dailt data is (7, 365, 41, 41)\n","  rdata_daily_np_array_T = rdata_daily_np_array.transpose(1,0,2,3)   # transform the dailt data from (7, 365, 41, 41) to (365, 7, 41, 41)\n","  overall_mean = np.nanmean(rdata_daily_np_array_T[:, :, :, :])\n","  for i in range(rdata_daily_np_array_T.shape[0]):\n","    for j in range(rdata_daily_np_array_T.shape[1]):\n","      for k in range(rdata_daily_np_array_T.shape[2]):\n","        for l in range(rdata_daily_np_array_T.shape[3]):\n","          if np.isnan(rdata_daily_np_array_T[i, j, k, l]):\n","            #print(\"NAN data in \", i, j, k, l)\n","            rdata_daily_np_array_T[i, j, k, l] = overall_mean #np.nanmean(rdata_daily_np_array_T[i, j, k, :])\n","  rdata_daily_np_array_T = rdata_daily_np_array_T.transpose(0,2,3,1)\n","  rdata_daily_np_array_T_R = rdata_daily_np_array_T.reshape((rdata_daily_np_array_T.shape[0], -1))  # transform the dailt data from (365, 7, 41, 41) to (365, 11767)\n","  min_max_scaler = preprocessing.MinMaxScaler() # calling the function\n","  rdata_daily_np_array_T_R_nor = min_max_scaler.fit_transform(rdata_daily_np_array_T_R)   # now normalize the data, otherwise the loss will be very big\n","  #rdata_daily_np_array_T_R_nor = np.float32(rdata_daily_np_array_T_R_nor)    # convert the data type to float32, otherwise the loass will be out-of-limit\n","  rdata_daily_np_array_T_R_nor_R = rdata_daily_np_array_T_R_nor.reshape((rdata_daily_np_array_T_R_nor.shape[0], 41,41,7))\n","  rdata_daily_np_array_T_R_nor_new = rdata_daily_np_array_T_R_nor_R.reshape(rdata_daily_np_array_T_R_nor_R.shape[0], 1, rdata_daily_np_array.shape[2], rdata_daily_np_array.shape[3], rdata_daily_np_array.shape[0])\n","  return rdata_daily_np_array_T_R_nor, rdata_daily_np_array_T_R_nor_new\n"],"metadata":{"id":"54jIAcCZt-Fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_nor_eval, data_clustering = data_preprocessing('/content/drive/MyDrive/ERA5_Dataset.nc')"],"metadata":{"id":"Mffc3qawfr0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_nor_eval.shape, data_clustering.shape"],"metadata":{"id":"cKgcQzFsf_tF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"30d22eb2-d6c5-4694-ef3e-e4a954163396","executionInfo":{"status":"ok","timestamp":1680475584133,"user_tz":240,"elapsed":4,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((365, 11767), (365, 1, 41, 41, 7))"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# **4. Model Training**\n","This function defines related parameters to train the model. Then instantiate the model and train on the pre-processed data. The model tries to optimize the clustering loss. After training the model returns the cluster assignment for each day of the input dataset."],"metadata":{"id":"VK65Y5rV2FbG"}},{"cell_type":"code","source":["def main():\n","\n","    batch_size = 366     # Number of input will be considerer for each training iteration\n","    maxiter = 2e4        # Maximum number of times the model traning will iterate\n","    update_interval = 50 # After each interval the clustering weights will be modified\n","    tol = 0.0000001      # If there is a cluster change more than this tollerance the model training will run\n","    save_dir = '/content/drive/MyDrive/my_CNN_LSTM_result'     # The trained model will be stored here\n","\n","\n","    x = data_clustering  # Input dataset of the transformed daily data\n","    y = None             # The cluster level of input data. Not available for our dataset.\n","    n_clusters = 7       # Number of clusters we want to generate.\n","\n","    # prepare the model\n","    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","    input_dims=data_clustering.shape[1:5] # CNN Model input layer dimension (1, 41, 41, 7)\n","    cnnmodel = CNNModel(dims=input_dims, n_clusters=n_clusters, init=init)\n","\n","\n","    cnnmodel.model.summary()\n","    t0 = time()\n","    cnnmodel.compile(optimizer=SGD(0.0000001, 0.9), loss='kld')#mse    kld\n","    y_pred = cnnmodel.fit(x, y=y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n","                     update_interval=update_interval, save_dir=save_dir)\n","    #print('acc:', metrics.acc(y, y_pred))\n","    print('clustering time: ', (time() - t0))\n","    return y_pred"],"metadata":{"id":"GSPnRn0t3pMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import silhouette_samples, silhouette_score\n","def silhouette_score(X, labels, *, metric=\"cosine\", sample_size=None, random_state=None, **kwds):\n"," return np.mean(silhouette_samples(X, labels, metric=\"cosine\", **kwds))"],"metadata":{"id":"BXJ9_3OY-iIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = main()\n","silhouette_avg_rdata_daily = silhouette_score(data_nor_eval, result)\n","print(\"The average silhouette_score is :\", silhouette_avg_rdata_daily)"],"metadata":{"id":"6Zy2e0LUqVFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The average silhouette_score is :\", silhouette_avg_rdata_daily)\n","print(\"The result :\", result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVKUbc664rlU","outputId":"88916262-c5d9-463d-f143-c44d277e5dbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The average silhouette_score is : 0.36386310160012764\n","The result : [0 3 3 3 3 3 3 4 4 4 0 0 0 0 0 0 0 6 6 6 6 0 0 0 0 0 6 6 6 0 0 6 2 2 2 4 4\n"," 4 6 6 6 6 6 6 6 2 4 4 6 2 2 6 6 2 4 4 4 0 0 4 6 6 4 4 4 4 4 4 4 4 4 0 0 0\n"," 0 0 0 4 4 4 4 4 4 4 4 4 4 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 6 2 2 4 0 0 4 4 4 0 0 0 0 0 0 0 6 4 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 0 0 5 5\n"," 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 1 3 3 3 3 3 3 3 5 5 5\n"," 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 3 6 3 3 2 6 2 2 6 6 6 6 6 6 6 6]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import davies_bouldin_score\n","\n","print(\"Davies-Bouldin score is \", davies_bouldin_score(data_nor_eval, result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wDHHJFi28QvE","outputId":"c49faf02-3c67-47dd-980a-7825d8393a3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Davies-Bouldin score is  1.4382477188374951\n"]}]},{"cell_type":"markdown","source":["# **4. Model testing with pre-trained weights:**\n","To test the model on other datasets we have to create the model and initialize the model with pre-trained weights stored in the drive. Then we have to call the main function to get the clustering results."],"metadata":{"id":"4shIDrxnJqKV"}},{"cell_type":"code","source":["def main_test():\n","\n","    batch_size = 366     # Number of input will be considerer for each training iteration\n","    maxiter = 2e4        # Maximum number of times the model traning will iterate\n","    update_interval = 50 # After each interval the clustering weights will be modified\n","    tol = 0.0000001      # If there is a cluster change more than this tollerance the model training will run\n","    save_dir = '/content/drive/MyDrive/my_CNN_LSTM_result'     # The trained model will be stored here\n","\n","\n","    x = data_clustering  # Input dataset of the transformed daily data\n","    y = None             # The cluster level of input data. Not available for our dataset.\n","    n_clusters = 7       # Number of clusters we want to generate.\n","\n","    # prepare the model\n","    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","    input_dims=data_clustering.shape[1:5] # CNN Model input layer dimension (1, 41, 41, 7)\n","    cnnmodel = CNNModel(dims=input_dims, n_clusters=n_clusters, init=init)\n","\n","\n","    cnnmodel.model.summary()\n","    cnnmodel.load_weights('/content/drive/MyDrive/my_CNN_LSTM_result/model_final.h5')\n","    t0 = time()\n","\n","    y_pred = cnnmodel.predict(x)\n","    print('clustering time: ', (time() - t0))\n","    return y_pred"],"metadata":{"id":"uHCZkk0KJpXu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = main_test()\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpzOj45qKSYn","executionInfo":{"status":"ok","timestamp":1680475783911,"user_tz":240,"elapsed":14401,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"8081d298-1a71-41ef-d808-d868501037ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====Model created=====\n","====== clustering layer created ========\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1, 41, 41, 7)]    0         \n","                                                                 \n"," time_distributed (TimeDistr  (None, 1, 41, 41, 64)    4096      \n"," ibuted)                                                         \n","                                                                 \n"," time_distributed_1 (TimeDis  (None, 1, 41, 41, 64)    36928     \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_2 (TimeDis  (None, 1, 21, 21, 64)    0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_3 (TimeDis  (None, 1, 21, 21, 256)   147712    \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_4 (TimeDis  (None, 1, 11, 11, 256)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_5 (TimeDis  (None, 1, 30976)         0         \n"," tributed)                                                       \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 512)            64489472  \n","                                                                 \n"," lstm_1 (LSTM)               (None, 512)               2099200   \n","                                                                 \n"," dense (Dense)               (None, 256)               131328    \n","                                                                 \n"," clustering (ClusteringLayer  (None, 7)                1792      \n"," )                                                               \n","                                                                 \n","=================================================================\n","Total params: 66,910,528\n","Trainable params: 66,910,528\n","Non-trainable params: 0\n","_________________________________________________________________\n","clustering time:  7.754872560501099\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0, 3, 3, 3, 3, 3, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 0,\n","       0, 0, 0, 0, 6, 6, 6, 0, 0, 6, 2, 2, 2, 4, 4, 4, 6, 6, 6, 6, 6, 6,\n","       6, 2, 4, 4, 6, 2, 2, 6, 6, 2, 4, 4, 4, 0, 0, 4, 6, 6, 4, 4, 4, 4,\n","       4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0,\n","       4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 6, 2, 2, 4, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 6, 4, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5,\n","       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3,\n","       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 3, 6, 3,\n","       3, 2, 6, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# **5. Plotting the clustering results:**\n","To plot the clustering results first we have to create the model and initialize the model with pre-trained weights. Then we will create a new model by taking the Dense layer output and clustering output from the original model. The Dense layer output will be used to plot the clusters using the dimension reduction algorithm."],"metadata":{"id":"9Bcl7Y6_ZCX4"}},{"cell_type":"code","source":["n_clusters = 7\n","init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","cnnmodel = CNNModel(dims=data_clustering.shape[1:5], n_clusters=n_clusters, init=init)\n","\n","cnnmodel.model.summary()\n","cnnmodel.load_weights('/content/drive/MyDrive/my_CNN_LSTM_result/model_final.h5')"],"metadata":{"id":"lWtfucfwZJgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gradModel = Model(\n","\t\t\tinputs=[cnnmodel.model.input],\n","\t\t\toutputs=[cnnmodel.model.get_layer('dense').output,\n","\t\t\t\tcnnmodel.model.output])"],"metadata":{"id":"4aAeGYRTHKpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(convOutputs, predictions) = gradModel(data_clustering)"],"metadata":{"id":"W7VNebp_IoQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.lines import Line2D\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","tsne = TSNE(n_components=2, learning_rate='auto', perplexity=10)\n","tsne_data = tsne.fit_transform(convOutputs)\n","\n","tsne_df = pd.DataFrame(tsne_data, columns=['TSNE1','TSNE2'])\n","tsne_df['cluster'] = pd.Categorical(result)\n","\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","    Clusters['Cluster' + str(i)] = np.array(tsne_df[tsne_df.cluster == i].drop(columns=['cluster']))\n","for i in range(len(Clusters)):\n","    Cluster_Centers[i] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","\n","cen_x = [Cluster_Centers[i][0] for i in range(7)]\n","cen_y = [Cluster_Centers[i][1] for i in range(7)]\n","\n","\n","tsne_df['cen_x'] = tsne_df.cluster.map({0:Cluster_Centers[0][0], 1:Cluster_Centers[1][0], 2:Cluster_Centers[2][0],\n","                                        3:Cluster_Centers[3][0], 4:Cluster_Centers[4][0], 5:Cluster_Centers[5][0],\n","                                        6:Cluster_Centers[6][0]})\n","tsne_df['cen_y'] = tsne_df.cluster.map({0:Cluster_Centers[0][1], 1:Cluster_Centers[1][1], 2:Cluster_Centers[2][1],\n","                                        3:Cluster_Centers[3][1], 4:Cluster_Centers[4][1], 5:Cluster_Centers[5][1],\n","                                        6:Cluster_Centers[6][1]})\n","\n","colors = ['blue', 'orange', 'green', 'red', 'purple', 'cyan', 'olive']\n","tsne_df['c'] = tsne_df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3], 4:colors[4], 5:colors[5], 6:colors[6]})\n","\n","fig, ax = plt.subplots(1, figsize=(8,8))\n","# plot data\n","plt.scatter(tsne_df.TSNE1, tsne_df.TSNE2, c=tsne_df.c, alpha = 0.6, s=10,)\n","# plot centroids\n","plt.scatter(cen_x, cen_y, marker='^', c=colors, s=70)\n","# plot lines\n","for idx, val in tsne_df.iterrows():\n","    x = [val.TSNE1, val.cen_x,]\n","    y = [val.TSNE2, val.cen_y]\n","    plt.plot(x, y, c=val.c, alpha=0.2)\n","# legend\n","legend_elements = [Line2D([0], [0], marker='o', color='w', label='Cluster {}'.format(i+1),\n","                   markerfacecolor=mcolor, markersize=5) for i, mcolor in enumerate(colors)]\n","legend_elements.extend([Line2D([0], [0], marker='^', color='w', label='Centroid - C{}'.format(i+1),\n","            markerfacecolor=mcolor, markersize=10) for i, mcolor in enumerate(colors)])\n","\n","plt.legend(handles=legend_elements, title='Clusters', bbox_to_anchor=(1.02, 1), loc='upper left', ncol=2, borderaxespad=0)\n","\n","plt.title('CNN-LSTM Encoder Clustering Result\\n', loc='left', fontsize=22)\n","plt.xlabel('Feature-1')\n","plt.ylabel('Feature-2')"],"metadata":{"id":"aUl9TmzFGiDL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **6. Evaluation:**\n","To compute the RMSE, variance, and average inter cluster distance we have to use the xarray format of our real data and the clustering result."],"metadata":{"id":"Fom8O-jx2ljT"}},{"cell_type":"code","source":["def total_rmse(data_path,formed_clusters):\n","  processed_data = data_preprocessing(data_path)\n","  trans_data = pd.DataFrame(processed_data)\n","  trans_data['Cluster'] = formed_clusters\n","\n","  # Normalized\n","  # Function that creates two dictionaries that hold all the clusters and cluster centers\n","  def nor_get_clusters_and_centers(input,formed_clusters):\n","    Clusters = {}\n","    Cluster_Centers = {}\n","    for i in set(formed_clusters):\n","      Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))\n","      Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","    return Clusters,Cluster_Centers\n","\n","  intra_rmse = []\n","  sq_diff = []\n","  Clusters,Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)\n","  for i in range(len(Clusters)):\n","    for j in range(len(Clusters['Cluster' + str(i)])):\n","      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]\n","      Sq_diff = (diff)**2\n","      sq_diff.append(Sq_diff)\n","\n","  Sq_diff_sum = np.sum(np.sum(sq_diff))\n","  rmse = np.sqrt(Sq_diff_sum/data_nor_eval.shape[0])\n","  return rmse"],"metadata":{"id":"mv59mZc_bTlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_rmse_omar('/content/multivariate-weather-data-clustering/data/ERA5_meteo_sfc_2021_daily.nc', result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679782493669,"user_tz":240,"elapsed":8144,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"61744a0e-6f92-4810-9798-a3a3824d9496","id":"EjFAIlIn1KrI"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13.822368449639312"]},"metadata":{},"execution_count":130}]},{"cell_type":"markdown","source":["### This cell measure the variances of the generated clusters.  "],"metadata":{"id":"gb9wb99kbhIM"}},{"cell_type":"code","source":["trans_data = pd.DataFrame(data_nor_eval)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","variances = pd.DataFrame(columns=range(len(Clusters)),index=range(2))\n","for i in range(len(Clusters)):\n","    #distance = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'cosine')\n","    variances[i].iloc[0] = np.var(Clusters['Cluster' + str(i)])\n","    variances[i].iloc[1] = Clusters['Cluster' + str(i)].shape[0]\n","\n","var_sum = 0\n","for i in range(7):\n","    var_sum = var_sum + (variances[i].iloc[0] * variances[i].iloc[1])\n","\n","var_avg = var_sum/data_nor_eval.shape[0]\n","var_avg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679782395160,"user_tz":240,"elapsed":357,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"fd49f3e3-5078-49a6-d6b1-f0963b2ec451","id":"5Dg3Rt7v1KrH"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.045096215224214004"]},"metadata":{},"execution_count":126}]},{"cell_type":"markdown","source":["### The following cell measure the average inter cluster distance.  "],"metadata":{"id":"d21VYp7bbkzr"}},{"cell_type":"code","source":["from scipy.spatial.distance import cdist,pdist\n","\n","trans_data = pd.DataFrame(data_nor_eval)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","distance_matrix = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))\n","for i in range(len(Clusters)):\n","  for j in range(len(Clusters)):\n","    if i == j:\n","      #distance_matrix[i].iloc[j] = 0\n","      distance_intra = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.max(distance_intra)\n","    elif i > j:\n","       continue\n","    else:\n","      distance = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.min(distance)\n","      distance_matrix[j].iloc[i] = np.min(distance)\n","\n","sum_min = 0\n","for i in range(n_clusters):\n","    sum_min = sum_min + np.min(distance_matrix[i])\n","\n","avg_inter = sum_min/n_clusters\n","avg_inter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXD_SNJZKwBb","executionInfo":{"status":"ok","timestamp":1680476179777,"user_tz":240,"elapsed":2,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"042ace5c-f237-410c-be2b-16cb7e770041"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8.106520289078537"]},"metadata":{},"execution_count":15}]}]}