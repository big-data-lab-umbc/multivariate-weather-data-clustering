{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G06VgB3ii9UmTjQenvgXZvocmfFOVFtC","timestamp":1675215687741}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Multivariate Climate Data Clustering using CNN Autoencoder Model**\n","Here we are dealing with climate data that comprises spatial information, time information, and scientific values. The dataset contains the value of 7 parameters for a region of 41 longitudes and 41 latitudes for 365 days in a year.\n","\n","Our goal is to create meaningful clusters of 365 days based on the values of these 7 parameters. As the data dimension is high we planned to use deep learning-based models to generate the latent representation of each day and then generate clusters for 365 days.\n","\n","We have developed a new CNN Autoencoder using different convolution neural network layers and activation functions from the Keras library. This new model structure yielded a better result than the previous CNN model.\n"],"metadata":{"id":"G7_ICF2rpwue"}},{"cell_type":"markdown","source":["# **1. Model Creation:**\n","This Autoencoder model considers our daily data as an image of size 41x41x7. The model takes a 365x41x41x7 NumPy array as input and applies convolution layers, and max pooling layers to learn the latent features. The output of this model is 512 latent features for each data point. The decoder part takes the 512 latent features of the encoder as input and applies stacks of upsampling and convolution layers to reconstruct the input data.\n","\n"],"metadata":{"id":"_n_e9hlor19G"}},{"cell_type":"code","source":["import keras,os\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, AveragePooling2D, LSTM, Activation, ConvLSTM2D, TimeDistributed, Input, Reshape\n","from keras.layers import UpSampling1D, Conv2DTranspose, UpSampling2D\n","import numpy as np\n","\n","def get_model(input_dims):\n","    input_batch = Input(shape = input_dims)\n","\n","    conv_model = Sequential()\n","    conv_model = Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"tanh\", name='ConvL1')(input_batch)\n","    conv_model = Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"tanh\", name='ConvL2')(conv_model)\n","    conv_model = MaxPool2D(pool_size=(2,2),strides=(2,2))(conv_model)\n","    conv_model = Dropout(0.1)(conv_model)\n","    conv_model = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"tanh\")(conv_model)\n","    conv_model = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"tanh\")(conv_model)\n","    conv_model = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"tanh\")(conv_model)\n","    conv_model = MaxPool2D(pool_size=(2,2),strides=(2,2))(conv_model)\n","    conv_model = Dropout(0.1)(conv_model)\n","    conv_model = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"tanh\")(conv_model)\n","    conv_model = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"tanh\")(conv_model)\n","    conv_model = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"tanh\")(conv_model)\n","    conv_model = MaxPool2D(pool_size=(2,2),strides=(2,2))(conv_model)\n","    conv_model = Flatten()(conv_model)\n","    conv_model = Dense(2048, activation=\"sigmoid\", name='fc1')(conv_model)\n","    conv_model = Dense(512,  activation=\"sigmoid\", name='fc2')(conv_model)\n","\n","\n","    decoder_network = Dense(6400, name='dense1')(conv_model)\n","    decoder_network = Reshape((5, 5, 256), name='reshape_1') (decoder_network)\n","    decoder_network = UpSampling2D((2, 2))(decoder_network)\n","    decoder_network = Conv2D(256, (3, 3), activation=\"tanh\", padding=\"same\")(decoder_network)\n","    decoder_network = UpSampling2D((2, 2))(decoder_network)\n","    decoder_network = Conv2D(128, (3, 3), activation=\"tanh\", padding=\"same\")(decoder_network)\n","    decoder_network = UpSampling2D((2, 2))(decoder_network)\n","    decoder_network = Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\")(decoder_network)\n","    decoder_network = Conv2D(7, (3, 3), activation=\"tanh\", padding=\"same\")(decoder_network)\n","    decoder_network = Flatten()(decoder_network)\n","    decoder_network = Dense(11767,activation='sigmoid', name='dense3')(decoder_network)\n","    decoder_network = Reshape(input_dims, name='reshape_3') (decoder_network)\n","\n","    autoencoder = Model(inputs=input_batch, outputs=decoder_network, name='AE')\n","\n","    encoder = Model(inputs=input_batch, outputs=conv_model, name='encoder')\n","\n","    return autoencoder, encoder\n"],"metadata":{"id":"UTo1A7DSjUcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from time import time\n","import numpy as np\n","import keras.backend as K\n","from tensorflow.keras.layers import Layer, InputSpec\n","from keras.layers import Dense, Input, Dropout\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras import callbacks\n","from keras.initializers import VarianceScaling\n","from sklearn.cluster import KMeans\n","\n","\n","class ClusteringLayer(Layer):\n","    \"\"\"\n","    # Arguments\n","        n_clusters: number of clusters.\n","        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n","        alpha: parameter in Student's t-distribution. Default to 1.0.\n","    # Input shape\n","        2D tensor with shape: `(n_samples, n_features)`.\n","    # Output shape\n","        2D tensor with shape: `(n_samples, n_clusters)`.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(ClusteringLayer, self).__init__(**kwargs)\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.initial_weights = weights\n","        self.input_spec = InputSpec(ndim=2)\n","\n","    def build(self, input_shape):\n","        print(input_shape)\n","        assert len(input_shape) == 2\n","        input_dim = input_shape[1]\n","        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n","        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='normal', name='clusters') #glorot_uniform\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","        self.built = True\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n","                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n","        Arguments:\n","            inputs: the variable containing data, shape=(n_samples, n_features)\n","        Return:\n","            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n","        \"\"\"\n","        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n","        q **= (self.alpha + 1.0) / 2.0\n","        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n","        return q\n","\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) == 2\n","        return input_shape[0], self.n_clusters\n","\n","    def get_config(self):\n","        config = {'n_clusters': self.n_clusters}\n","        base_config = super(ClusteringLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class CNNModel(object):\n","    def __init__(self,\n","                 dims,\n","                 n_clusters=10,\n","                 alpha=1.0,\n","                 init='glorot_uniform'):\n","\n","        super(CNNModel, self).__init__()\n","\n","        self.dims = dims\n","        #self.input_dim = dims[0]\n","        self.n_stacks = len(self.dims) - 1\n","\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        #self.model_cnn = myCNNModel(self.dims);\n","        self.model_AE, self.model_cnn = get_model(self.dims);\n","        print(\"====Model created=====\")\n","\n","        # prepare the CNN model with cnn_layers+clustering _layer\n","        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.model_cnn.output)\n","        print(\"====== clustering layer created ========\")\n","        #self.model = Model(inputs=self.model_cnn.input, outputs=clustering_layer)\n","        self.model = Model(inputs=self.model_AE.input, outputs=[self.model_AE.output, clustering_layer])\n","        #outputs=[self.autoencoder.output, clustering_layer]\n","\n","\n","    def load_weights(self, weights):  # load weights\n","        self.model.load_weights(weights)\n","\n","    def extract_features(self, x):\n","        return self.model_cnn.predict(x)\n","\n","    def predict(self, x):  # predict cluster labels using the output of clustering layer\n","        q = self.model.predict(x, verbose=0)[1]\n","        return q.argmax(1)\n","\n","    @staticmethod\n","    def target_distribution(q):\n","        weight = q ** 2 / q.sum(0)\n","        return (weight.T / weight.sum(1)).T\n","\n","    def compile(self, optimizer='sgd', loss='kld'):\n","        self.model.compile(optimizer=optimizer, loss=['mse', 'kld'])\n","\n","    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n","            update_interval=140, save_dir='./results/temp'):\n","\n","        print('Update interval', update_interval)\n","        #save_interval = int(x.shape[0] / batch_size) * 5  # 5 epochs\n","        save_interval = 500\n","        print('Save interval', save_interval)\n","\n","        # Step 1: initialize cluster centers using k-means\n","        t1 = time()\n","        print('Initializing cluster centers with k-means.')\n","        kmeans = KMeans(n_clusters=self.n_clusters, init = 'random', n_init=30)\n","        y_pred = kmeans.fit_predict(self.model_cnn.predict(x))\n","        y_pred_last = np.copy(y_pred)\n","        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n","\n","        # Step 2: deep clustering\n","        # logging file\n","        import csv\n","        logfile = open(save_dir + '/cnn_log.csv', 'w')\n","        logwriter = csv.DictWriter(logfile, fieldnames=['iter','loss'])\n","        logwriter.writeheader()\n","\n","        loss = 0\n","        index = 0\n","        index_array = np.arange(x.shape[0])\n","        patience_cnt = 0\n","        patience = 10\n","        for ite in range(int(maxiter)):\n","            if ite % update_interval == 0:\n","                q = self.model.predict(x, verbose=0)[1]\n","                #print(\"The valus of q: \", q)\n","                p = self.target_distribution(q)  # update the auxiliary target distribution p\n","                #print(\"The valus of p: \" , p)\n","\n","                # evaluate the clustering performance\n","                y_pred = q.argmax(1)\n","                #print(\"The valus of y_pred: \", y_pred)\n","                #y_pred = q\n","                print(\"#### inside iteration ### \", ite)\n","\n","                # check stop criterion\n","                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n","                print(\"##### Prediction in side the iter and the delta_label is \", delta_label)\n","                y_pred_last = np.copy(y_pred)\n","                if ite > 0 and delta_label < tol:\n","                    patience_cnt += 1\n","                    #print('delta_label ', delta_label, '< tol ', tol)\n","                    print('Assignment changes {} < {} tolerance threshold. Patience: {}/{}.'.format(delta_label, tol, patience_cnt, patience))\n","                    # if patience_cnt >= patience:\n","                    print('Reached tolerance threshold. Stopping training.')\n","                    logfile.close()\n","                    break\n","                # else:\n","                #   patience_cnt = 0\n","\n","            # train on batch\n","            # if index == 0:\n","            #     np.random.shuffle(index_array)\n","            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n","            loss = self.model.train_on_batch(x=x[idx], y=[x[idx],p[idx]])\n","            print(\"#### the loss is \", loss)\n","            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n","\n","            # save intermediate model\n","            if ite % save_interval == 0:\n","                print('saving model to:', save_dir + '/CNN_model_' + str(ite) + '.h5')\n","                self.model.save_weights(save_dir + '/CNN_model_' + str(ite) + '.h5')\n","\n","            ite += 1\n","\n","        # save the trained model\n","        logfile.close()\n","        file_name  = \"/CNN_model_final_\" + str(round(time()))+ \".h5\"\n","        print('saving model to:', save_dir + file_name)\n","        self.model.save_weights(save_dir + file_name)\n","\n","        return y_pred"],"metadata":{"id":"swC8lZ-c38sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install netCDF4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw1W4vvp9Twp","executionInfo":{"status":"ok","timestamp":1679801280319,"user_tz":240,"elapsed":9330,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"8f21c9f5-d5c0-4e88-b94b-145786f6278c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting netCDF4\n","  Downloading netCDF4-1.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from netCDF4) (1.22.4)\n","Collecting cftime\n","  Downloading cftime-1.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: cftime, netCDF4\n","Successfully installed cftime-1.6.2 netCDF4-1.6.3\n"]}]},{"cell_type":"code","source":["import netCDF4\n","import netCDF4 as nc\n","import pandas as pd\n","import numpy as np\n","import xarray as xr\n","import datetime\n","import datetime as dt\n","from netCDF4 import date2num,num2date\n","from math import sqrt"],"metadata":{"id":"iUjV0nswxgqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W97e8KvKxhZu","outputId":"1dd6b86a-82d3-4c4d-bd13-3eeb26b012b1","executionInfo":{"status":"ok","timestamp":1679801330263,"user_tz":240,"elapsed":22093,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **2. Data preparation**\n","The dataset has some NaN values in the SST variable. To replace these NaN values we used the mean value of the full dataset. The function returns 2 NumPy arrays one with size (365, 11767) and another with size (365, 41, 41, 7). The array with size (365, 11767) is used to calculate the silhouette score and the rray with size (365, 41, 41, 7) is used to train the model."],"metadata":{"id":"fvZxFxPMtmMb"}},{"cell_type":"code","source":["from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def data_preprocessing(data_path):\n","  rdata_daily = xr.open_dataset(data_path)    # data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'\n","  rdata_daily_np_array = np.array(rdata_daily.to_array())   # the shape of the dailt data is (7, 365, 41, 41)\n","  rdata_daily_np_array_T = rdata_daily_np_array.transpose(1,0,2,3)   # transform the dailt data from (7, 365, 41, 41) to (365, 7, 41, 41)\n","  overall_mean = np.nanmean(rdata_daily_np_array_T[:, :, :, :])\n","  for i in range(rdata_daily_np_array_T.shape[0]):\n","    for j in range(rdata_daily_np_array_T.shape[1]):\n","      for k in range(rdata_daily_np_array_T.shape[2]):\n","        for l in range(rdata_daily_np_array_T.shape[3]):\n","          if np.isnan(rdata_daily_np_array_T[i, j, k, l]):\n","            #print(\"NAN data in \", i, j, k, l)\n","            rdata_daily_np_array_T[i, j, k, l] = overall_mean\n","  rdata_daily_np_array_T = rdata_daily_np_array_T.transpose(0,2,3,1)\n","  rdata_daily_np_array_T_R = rdata_daily_np_array_T.reshape((rdata_daily_np_array_T.shape[0], -1))  # transform the dailt data from (365, 7, 41, 41) to (365, 11767)\n","  min_max_scaler = preprocessing.MinMaxScaler() # calling the function\n","  rdata_daily_np_array_T_R_nor = min_max_scaler.fit_transform(rdata_daily_np_array_T_R)   # now normalize the data, otherwise the loss will be very big\n","  #rdata_daily_np_array_T_R_nor = np.float32(rdata_daily_np_array_T_R_nor)    # convert the data type to float32, otherwise the loass will be out-of-limit\n","  rdata_daily_np_array_T_R_nor_R = rdata_daily_np_array_T_R_nor.reshape((rdata_daily_np_array_T_R_nor.shape[0], rdata_daily_np_array.shape[2], rdata_daily_np_array.shape[3], rdata_daily_np_array.shape[0]))\n","  return rdata_daily_np_array_T_R_nor, rdata_daily_np_array_T_R_nor_R\n"],"metadata":{"id":"54jIAcCZt-Fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_nor_eval, data_clustering = data_preprocessing('/content/drive/data/ERA5_Dataset.nc')"],"metadata":{"id":"Mffc3qawfr0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_nor_eval.shape, data_clustering.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKgcQzFsf_tF","outputId":"e1f997a7-8857-46fc-dfd5-619af9bf1cf5","executionInfo":{"status":"ok","timestamp":1679801415495,"user_tz":240,"elapsed":8,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((365, 11767), (365, 41, 41, 7))"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# **3. Model Training**\n","This function defines related parameters to train the model. Then instantiate the model and train on the pre-processed data. The model tries to optimize the clustering loss and the reconstruction loss. After training the model returns the cluster assignment for each data point of the input dataset."],"metadata":{"id":"V_lI-wFkzZik"}},{"cell_type":"code","source":["def main():\n","    # setting the hyper parameters\n","\n","    batch_size = 8 # Number of input will be considerer for each training iteration\n","    maxiter = 2e4 # Maximum number of times the model traning will iterate\n","    update_interval = 30 # After each interval the clustering weights will be modified\n","    tol = 0.0000001 # If there is a cluster change more than this tollerance the model training will run\n","    save_dir = '/content/drive/MyDrive/my_CNN_AE_Results' # The trained model will be stored here\n","\n","    # load dataset\n","    x = data_clustering  # Input dataset of the transformed daily data\n","    y = None             # The cluster level of input data. Not available for our dataset.\n","    n_clusters = 7       # Number of clusters we want to generate.\n","\n","\n","    # prepare the model\n","    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","    cnnmodel = CNNModel(dims=x.shape[1:4], n_clusters=n_clusters, init=init)\n","\n","    cnnmodel.model.summary()\n","    t0 = time()\n","    cnnmodel.compile(optimizer=SGD(0.0000001, 0.9), loss='kld')\n","    y_pred = cnnmodel.fit(x, y=y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n","                     update_interval=update_interval, save_dir=save_dir)\n","    #print('acc:', metrics.acc(y, y_pred))\n","    print('clustering time: ', (time() - t0))\n","    return y_pred"],"metadata":{"id":"GSPnRn0t3pMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import silhouette_samples, silhouette_score\n","def silhouette_score(X, labels, *, metric=\"cosine\", sample_size=None, random_state=None, **kwds):\n"," return np.mean(silhouette_samples(X, labels, metric=\"cosine\", **kwds))"],"metadata":{"id":"BXJ9_3OY-iIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = main()\n","silhouette_avg_rdata_daily = silhouette_score(data_nor_eval, result)\n","print(\"The average silhouette_score is :\", silhouette_avg_rdata_daily)"],"metadata":{"id":"UmuaC_51y_eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The result :\", result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-edxpHWp2T0","executionInfo":{"status":"ok","timestamp":1679801928719,"user_tz":240,"elapsed":347,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"12acb28c-3178-40ee-e8f0-b650a582bf54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The average silhouette_score is : 0.3149449075506245\n","The result : [6 6 6 6 6 6 4 4 4 4 4 0 4 0 0 4 0 0 4 0 4 0 0 0 0 6 0 0 0 0 0 0 1 1 1 4 4\n"," 4 4 0 0 0 0 0 1 1 4 4 0 1 1 0 0 1 4 4 4 4 0 4 0 4 4 4 4 4 4 4 4 4 4 6 6 4\n"," 6 6 6 4 4 4 4 4 4 4 4 4 4 0 4 4 0 6 6 6 6 6 0 4 4 6 6 0 0 4 4 0 0 4 4 0 0\n"," 1 1 1 4 4 4 4 4 4 0 4 4 4 4 0 0 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 2 3 3 3 3 3 3 5 2 2 2 2 5 5 5 5 5 5\n"," 5 5 5 5 5 5 5 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n"," 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n"," 5 5 5 2 2 2 2 2 2 5 2 2 5 5 5 5 5 5 5 5 5 5 2 2 2 5 2 2 5 2 5 5 5 5 5 5 5\n"," 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 6 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import davies_bouldin_score\n","print(\"Davies-Bouldin score is \", davies_bouldin_score(data_nor_eval, result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679803234673,"user_tz":240,"elapsed":8,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"dae8770e-832c-4ba6-cb66-a38e297c5e5f","id":"CEDvxlD4AYFR"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Davies-Bouldin score is  1.6404546787197296\n"]}]},{"cell_type":"markdown","source":["# **4. Model testing with pre-trained weights:**\n","To test the model on other datasets we have to create the model and initialize the model with pre-trained weights stored in the drive. Then we have to call the main function to get the clustering results."],"metadata":{"id":"_aaTcOjF1orx"}},{"cell_type":"code","source":["def main_test():\n","    # setting the hyper parameters\n","\n","    batch_size = 8 # Number of input will be considerer for each training iteration\n","    maxiter = 2e4 # Maximum number of times the model traning will iterate\n","    update_interval = 50 # After each interval the clustering weights will be modified\n","    tol = 0.0000001 # If there is a cluster change more than this tollerance the model training will run\n","    save_dir = '/content/drive/MyDrive/my_CNN_AE_Results' # The trained model will be stored here\n","\n","    # load dataset\n","    x = data_clustering  # Input dataset of the transformed daily data\n","    y = None             # The cluster level of input data. Not available for our dataset.\n","    n_clusters = 7       # Number of clusters we want to generate.\n","\n","    # prepare the model\n","    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","    cnnmodel = CNNModel(dims=x.shape[1:5], n_clusters=n_clusters, init=init)\n","\n","    cnnmodel.model.summary()\n","    cnnmodel.load_weights('/content/drive/MyDrive/my_CNN_AE_Results/model_final.h5')\n","    t0 = time()\n","    y_pred = cnnmodel.predict(x)\n","    print('clustering time: ', (time() - t0))\n","    return y_pred"],"metadata":{"id":"qnElFnR8F2eZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_res = main_test()\n","val_res"],"metadata":{"id":"JqTEugqb2dwo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **5. Plotting the clustering results:**\n","To plot the clustering results first we have to create the model and initialize the model with pre-trained weights. Then we will create a new model by taking the Dense layer output and clustering output from the original model. The Dense layer output will be used to plot the clusters using the dimension reduction algorithm."],"metadata":{"id":"PFJHnZIC3EiT"}},{"cell_type":"code","source":["n_clusters = 7\n","init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","cnnmodel = CNNModel(dims=data_clustering.shape[1:5], n_clusters=n_clusters, init=init)\n","\n","cnnmodel.model.summary()\n","cnnmodel.load_weights('/content/drive/MyDrive/My_CNN_AE-Results/model_final.h5')"],"metadata":{"id":"ar13qkgQHOqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gradModel = Model(\n","\t\t\tinputs=[cnnmodel.model.input],\n","\t\t\toutputs=[cnnmodel.model.get_layer('fc2').output,\n","\t\t\t\tcnnmodel.model.output])"],"metadata":{"id":"4aAeGYRTHKpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(convOutputs, predictions) = gradModel(data_clustering)"],"metadata":{"id":"W7VNebp_IoQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.lines import Line2D\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","tsne = TSNE(n_components=2, learning_rate='auto', perplexity=10)\n","tsne_data = tsne.fit_transform(convOutputs)\n","\n","tsne_df = pd.DataFrame(tsne_data, columns=['TSNE1','TSNE2'])\n","tsne_df['cluster'] = pd.Categorical(result)\n","\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","    Clusters['Cluster' + str(i)] = np.array(tsne_df[tsne_df.cluster == i].drop(columns=['cluster']))\n","for i in range(len(Clusters)):\n","    Cluster_Centers[i] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","\n","cen_x = [Cluster_Centers[i][0] for i in range(7)]\n","cen_y = [Cluster_Centers[i][1] for i in range(7)]\n","\n","\n","tsne_df['cen_x'] = tsne_df.cluster.map({0:Cluster_Centers[0][0], 1:Cluster_Centers[1][0], 2:Cluster_Centers[2][0],\n","                                        3:Cluster_Centers[3][0], 4:Cluster_Centers[4][0], 5:Cluster_Centers[5][0],\n","                                        6:Cluster_Centers[6][0]})\n","tsne_df['cen_y'] = tsne_df.cluster.map({0:Cluster_Centers[0][1], 1:Cluster_Centers[1][1], 2:Cluster_Centers[2][1],\n","                                        3:Cluster_Centers[3][1], 4:Cluster_Centers[4][1], 5:Cluster_Centers[5][1],\n","                                        6:Cluster_Centers[6][1]})\n","\n","colors = ['blue', 'orange', 'green', 'red', 'purple', 'cyan', 'olive']\n","tsne_df['c'] = tsne_df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3], 4:colors[4], 5:colors[5], 6:colors[6]})\n","\n","fig, ax = plt.subplots(1, figsize=(8,8))\n","# plot data\n","plt.scatter(tsne_df.TSNE1, tsne_df.TSNE2, c=tsne_df.c, alpha = 0.6, s=10,)\n","# plot centroids\n","plt.scatter(cen_x, cen_y, marker='^', c=colors, s=70)\n","# plot lines\n","for idx, val in tsne_df.iterrows():\n","    x = [val.TSNE1, val.cen_x,]\n","    y = [val.TSNE2, val.cen_y]\n","    plt.plot(x, y, c=val.c, alpha=0.2)\n","# legend\n","legend_elements = [Line2D([0], [0], marker='o', color='w', label='Cluster {}'.format(i+1),\n","                   markerfacecolor=mcolor, markersize=5) for i, mcolor in enumerate(colors)]\n","legend_elements.extend([Line2D([0], [0], marker='^', color='w', label='Centroid - C{}'.format(i+1),\n","            markerfacecolor=mcolor, markersize=10) for i, mcolor in enumerate(colors)])\n","\n","plt.legend(handles=legend_elements, title='Clusters', bbox_to_anchor=(1.02, 1), loc='upper left', ncol=2, borderaxespad=0)\n","\n","plt.title('CNN Autoencoder Clustering Result\\n', loc='left', fontsize=22)\n","plt.xlabel('Feature-1')\n","plt.ylabel('Feature-2')"],"metadata":{"id":"aUl9TmzFGiDL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **6. Evaluation:**\n","To compute the RMSE, variance, and average inter cluster distance we have to use the xarray format of our real data and the clustering result."],"metadata":{"id":"HvdF9hL31DOJ"}},{"cell_type":"code","source":["\n","def total_rmse(data_path,formed_clusters):\n","  processed_data = data_preprocessing(data_path)\n","  trans_data = pd.DataFrame(processed_data)\n","  trans_data['Cluster'] = formed_clusters\n","\n","  # Normalized\n","  # Function that creates two dictionaries that hold all the clusters and cluster centers\n","  def nor_get_clusters_and_centers(input,formed_clusters):\n","    Clusters = {}\n","    Cluster_Centers = {}\n","    for i in set(formed_clusters):\n","      Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))\n","      Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","    return Clusters,Cluster_Centers\n","\n","  intra_rmse = []\n","  sq_diff = []\n","  Clusters,Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)\n","  for i in range(len(Clusters)):\n","    for j in range(len(Clusters['Cluster' + str(i)])):\n","      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]\n","      Sq_diff = (diff)**2\n","      sq_diff.append(Sq_diff)\n","\n","  Sq_diff_sum = np.sum(np.sum(sq_diff))\n","  rmse = np.sqrt(Sq_diff_sum/data_nor_eval.shape[0])\n","  return rmse"],"metadata":{"id":"iDyqPcGPRVJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_rmse('/content/data/ERA5_Dataset.nc', result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679803213768,"user_tz":240,"elapsed":5551,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"1a67ebda-5363-41b3-849c-7b0b57b5207d","id":"LlU0sad6AYFQ"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13.965799700377612"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### This cell measure the variances of the generated clusters.  "],"metadata":{"id":"3vqdgBdOW2V7"}},{"cell_type":"code","source":["trans_data = pd.DataFrame(data_nor_eval)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","variances = pd.DataFrame(columns=range(len(Clusters)),index=range(2))\n","for i in range(len(Clusters)):\n","    variances[i].iloc[0] = np.var(Clusters['Cluster' + str(i)])\n","    variances[i].iloc[1] = Clusters['Cluster' + str(i)].shape[0]\n","\n","var_sum = 0\n","for i in range(7):\n","    var_sum = var_sum + (variances[i].iloc[0] * variances[i].iloc[1])\n","\n","var_avg = var_sum/data_nor_eval.shape[0]\n","var_avg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679803155700,"user_tz":240,"elapsed":162,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"e3b6d098-5b67-4344-ab1c-c174b2036368","id":"Dmx29QajAYFQ"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.04583304732762859"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### The following cell measure the average inter cluster distance.  "],"metadata":{"id":"J7V-K6PvXJAk"}},{"cell_type":"code","source":["from scipy.spatial.distance import cdist,pdist\n","\n","trans_data = pd.DataFrame(data_nor_eval)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","distance_matrix = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))\n","for i in range(len(Clusters)):\n","  for j in range(len(Clusters)):\n","    if i == j:\n","      #distance_matrix[i].iloc[j] = 0\n","      distance_intra = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.max(distance_intra)\n","    elif i > j:\n","       continue\n","    else:\n","      distance = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.min(distance)\n","      distance_matrix[j].iloc[i] = np.min(distance)\n","\n","sum_min = 0\n","for i in range(n_clusters):\n","    sum_min = sum_min + np.min(distance_matrix[i])\n","\n","avg_inter = sum_min/n_clusters\n","avg_inter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679803189954,"user_tz":240,"elapsed":176,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"fe721b6e-7f95-442a-b400-05fbc0e8c856","id":"VrBJ9JOYAYFQ"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.588720440754571"]},"metadata":{},"execution_count":20}]}]}