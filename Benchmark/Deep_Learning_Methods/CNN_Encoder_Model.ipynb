{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1i-iRxg138X9iv5U73FfYlpRUhGUko3Xe","timestamp":1681140769763}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Multivariate Climate Data Clustering using CNN Encoder Model**\n","Here we are dealing with climate data that comprises spatial information, time information, and scientific values. The dataset contains the value of 7 parameters for a region of 41 longitudes and 41 latitudes for 365 days in a year.\n","\n","Our goal is to create meaningful clusters of 365 days based on the values of these 7 parameters. As the data dimension is high we planned to use deep learning-based models to generate the latent representation of each day and then generate clusters for 365 days.\n","\n","We have developed a new CNN Encoder using different convolution neural network layers and activation functions from the Keras library."],"metadata":{"id":"3SHnI1qVF27J"}},{"cell_type":"markdown","source":["# **1. Model Creation:**\n","This model considers our dataset as an image of size 41x41x7. The model takes a 365x41x41x7 NumPy array as input and applies convolution layers, and max pooling layers to learn the latent features. The output of this model is 512 latent features for each data point.  "],"metadata":{"id":"Ip5H8UZYGBg9"}},{"cell_type":"code","source":["import keras,os\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, LeakyReLU\n","#from keras.preprocessing.image import ImageDataGenerator.  tanh. relu\n","import numpy as np\n","\n","def myCNNModel(input_dims):\n","  '''\n","  The value of the parameter input_dims is the size of the input image/data we want to train on, for our data it is (41, 41, 7)\n","  '''\n","  model = Sequential()\n","  model.add(Conv2D(input_shape=input_dims,filters=64,kernel_size=(3,3),padding=\"same\", activation=\"tanh\"))\n","  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"tanh\"))\n","  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","  model.add(Dropout(0.1))\n","  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n","  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n","  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n","  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","  model.add(Dropout(0.1))\n","  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n","  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n","  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n","  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","  model.add(Flatten(name='flatten'))\n","  model.add(Dense(2048, activation=\"sigmoid\", name='fc1'))\n","  model.add(Dense(512,  activation=\"sigmoid\", name='fc2'))\n","  return model"],"metadata":{"id":"wI5bFl2aMCxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from time import time\n","import numpy as np\n","import keras.backend as K\n","from tensorflow.keras.layers import Layer, InputSpec\n","from keras.layers import Dense, Input, Dropout\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras import callbacks\n","from keras.initializers import VarianceScaling\n","from sklearn.cluster import KMeans\n","\n","\n","class ClusteringLayer(Layer):\n","\n","    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(ClusteringLayer, self).__init__(**kwargs)\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.initial_weights = weights\n","        self.input_spec = InputSpec(ndim=2)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 2\n","        input_dim = input_shape[1]\n","        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n","        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","        self.built = True\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n","                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n","        Arguments:\n","            inputs: the variable containing data, shape=(n_samples, n_features)\n","        Return:\n","            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n","        \"\"\"\n","        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n","        q **= (self.alpha + 1.0) / 2.0\n","        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n","        return q\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) == 2\n","        return input_shape[0], self.n_clusters\n","\n","    def get_config(self):\n","        config = {'n_clusters': self.n_clusters}\n","        base_config = super(ClusteringLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class CNNModel(object):\n","    def __init__(self,\n","                 dims,\n","                 n_clusters=10,\n","                 alpha=1.0,\n","                 init='glorot_uniform'):\n","\n","        super(CNNModel, self).__init__()\n","\n","        self.dims = dims\n","        #self.input_dim = dims[0]\n","        self.n_stacks = len(self.dims) - 1\n","\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.model_cnn = myCNNModel(self.dims);\n","        print(\"====Model created=====\")\n","\n","        # prepare the CNN model with cnn_layers+clustering _layer\n","        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.model_cnn.output)\n","        print(\"====== clustering layer created ========\")\n","        self.model = Model(inputs=self.model_cnn.input, outputs=clustering_layer)\n","\n","\n","    def load_weights(self, weights):\n","        self.model.load_weights(weights)\n","\n","    def extract_features(self, x):\n","        return self.model_cnn.predict(x)\n","\n","    def predict(self, x):  # predict cluster labels using the output of clustering layer\n","        q = self.model.predict(x, verbose=0)\n","        return q.argmax(1)\n","\n","    @staticmethod\n","    def target_distribution(q):\n","        weight = q ** 2 / q.sum(0)\n","        return (weight.T / weight.sum(1)).T\n","\n","    def compile(self, optimizer='sgd', loss='kld'):\n","        self.model.compile(optimizer=optimizer, loss=loss)\n","\n","    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n","            update_interval=140, save_dir='./results/temp'):\n","\n","        print('Update interval', update_interval)\n","        #save_interval = int(x.shape[0] / batch_size) * 5  # 5 epochs\n","        save_interval = 500\n","        print('Save interval', save_interval)\n","\n","        # Step 1: initialize cluster centers using k-means\n","        t1 = time()\n","        print('Initializing cluster centers with k-means.')\n","        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n","        y_pred = kmeans.fit_predict(self.model_cnn.predict(x))\n","        y_pred_last = np.copy(y_pred)\n","        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n","\n","        # Step 2: deep clustering\n","        # logging file\n","        import csv\n","        logfile = open(save_dir + '/cnn_log.csv', 'w')\n","        logwriter = csv.DictWriter(logfile, fieldnames=['iter','loss'])\n","        logwriter.writeheader()\n","\n","        loss = 0\n","        index = 0\n","        index_array = np.arange(x.shape[0])\n","        for ite in range(int(maxiter)):\n","            if ite % update_interval == 0:\n","                q = self.model.predict(x, verbose=0)\n","                p = self.target_distribution(q)  # update the auxiliary target distribution p\n","\n","                # evaluate the clustering performance\n","                y_pred = q.argmax(1)\n","                print(\"#### inside iteration ### \", ite)\n","\n","                # check stop criterion\n","                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n","                print(\"##### Prediction in side the iter and the delta_label is \", delta_label, \", cluster change= \", np.sum(y_pred != y_pred_last))\n","                y_pred_last = np.copy(y_pred)\n","                if ite > 0 and delta_label < tol:\n","                    print('delta_label ', delta_label, '< tol ', tol)\n","                    print('Reached tolerance threshold. Stopping training.')\n","                    logfile.close()\n","                    break\n","\n","            # train on batch\n","            # if index == 0:\n","            #     np.random.shuffle(index_array)\n","            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n","            loss = self.model.train_on_batch(x=x[idx], y=p[idx])\n","            print(\"#### the loss is \", loss)\n","            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n","\n","            # save intermediate model\n","            if ite % save_interval == 0:\n","                print('saving model to:', save_dir + '/CNN_model_' + str(ite) + '.h5')\n","                self.model.save_weights(save_dir + '/CNN_model_' + str(ite) + '.h5')\n","\n","            ite += 1\n","\n","        # save the trained model\n","        logfile.close()\n","        file_name  = \"/CNN_model_final_\" + str(round(time()))+ \".h5\"\n","        print('saving model to:', save_dir + file_name)\n","        self.model.save_weights(save_dir + file_name)\n","\n","        return y_pred"],"metadata":{"id":"swC8lZ-c38sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install netCDF4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw1W4vvp9Twp","executionInfo":{"status":"ok","timestamp":1679801280319,"user_tz":240,"elapsed":9330,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"8f21c9f5-d5c0-4e88-b94b-145786f6278c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting netCDF4\n","  Downloading netCDF4-1.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from netCDF4) (1.22.4)\n","Collecting cftime\n","  Downloading cftime-1.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: cftime, netCDF4\n","Successfully installed cftime-1.6.2 netCDF4-1.6.3\n"]}]},{"cell_type":"code","source":["import netCDF4 as nc\n","import pandas as pd\n","import numpy as np\n","import xarray as xr"],"metadata":{"id":"iUjV0nswxgqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W97e8KvKxhZu","outputId":"53631188-e4fb-4bc1-98c4-680215b8d28b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **2. Data preparation**\n","The dataset has some NaN values in the SST variable. To replace these NaN values we used the mean value of the full dataset. The function returns 2 NumPy arrays one with size (365, 11767) and another with size (365, 41, 41, 7). The array with size (365, 11767) is used to calculate the silhouette score and the array with size (365, 41, 41, 7) is used to train the model."],"metadata":{"id":"dGXjIAmzL05u"}},{"cell_type":"code","source":["from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def data_preprocessing(data_path):\n","  rdata_daily = xr.open_dataset(data_path)    # data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'\n","  rdata_daily_np_array = np.array(rdata_daily.to_array())   # the shape of the dailt data is (7, 365, 41, 41)\n","  rdata_daily_np_array_T = rdata_daily_np_array.transpose(1,0,2,3)   # transform the dailt data from (7, 365, 41, 41) to (365, 7, 41, 41)\n","  overall_mean = np.nanmean(rdata_daily_np_array_T[:, :, :, :])\n","  for i in range(rdata_daily_np_array_T.shape[0]):\n","    for j in range(rdata_daily_np_array_T.shape[1]):\n","      for k in range(rdata_daily_np_array_T.shape[2]):\n","        for l in range(rdata_daily_np_array_T.shape[3]):\n","          if np.isnan(rdata_daily_np_array_T[i, j, k, l]):\n","            #print(\"NAN data in \", i, j, k, l)\n","            rdata_daily_np_array_T[i, j, k, l] = overall_mean\n","  rdata_daily_np_array_T = rdata_daily_np_array_T.transpose(0,2,3,1)\n","  rdata_daily_np_array_T_R = rdata_daily_np_array_T.reshape((rdata_daily_np_array_T.shape[0], -1))  # transform the dailt data from (365, 7, 41, 41) to (365, 11767)\n","  min_max_scaler = preprocessing.MinMaxScaler() # calling the function\n","  rdata_daily_np_array_T_R_nor = min_max_scaler.fit_transform(rdata_daily_np_array_T_R)   # now normalize the data, otherwise the loss will be very big\n","  #rdata_daily_np_array_T_R_nor = np.float32(rdata_daily_np_array_T_R_nor)    # convert the data type to float32, otherwise the loass will be out-of-limit\n","  rdata_daily_np_array_T_R_nor_R = rdata_daily_np_array_T_R_nor.reshape((rdata_daily_np_array_T_R_nor.shape[0], rdata_daily_np_array.shape[2], rdata_daily_np_array.shape[3], rdata_daily_np_array.shape[0]))\n","  return rdata_daily_np_array_T_R_nor, rdata_daily_np_array_T_R_nor_R"],"metadata":{"id":"P009c8uMAUli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_nor_eval, data_clustering = data_preprocessing('/content/drive/MyDrive/ERA5_Dataset.nc')"],"metadata":{"id":"pHCZsIo4Aa05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_nor_eval.shape, data_clustering.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zk-7evmrAeyg","outputId":"f89702cf-0f90-4551-e122-7782269bbf8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((365, 11767), (365, 41, 41, 7))"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["This function calculates the silhoutte score of the formed clusters."],"metadata":{"id":"OaYMs-wcMf7G"}},{"cell_type":"code","source":["from sklearn.metrics import silhouette_samples, silhouette_score\n","def silhouette_score(X, labels, *, metric=\"cosine\", sample_size=None, random_state=None, **kwds):\n"," return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))"],"metadata":{"id":"BXJ9_3OY-iIa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **3. Model Training**\n","This function defines related parameters to train the model. Then instantiate the model and train on the pre-processed data. The model tries to optimize the clustering loss. After training the model returns the cluster assignment for each data point of the input dataset."],"metadata":{"id":"yCr8P9SjMuL1"}},{"cell_type":"code","source":["def main():\n","    # setting the hyper parameters\n","\n","    batch_size = 8       # Batch size for training the CNN model\n","    maxiter = 2e4        # Maximum number of iteration the model will train\n","    update_interval = 30 # After this number of iteration the model will update the cluster assignment\n","    tol = 0.0000001      # If there is a cluster change more than this tollerance the model training will run\n","    save_dir = '/content/drive/MyDrive/my_CNN_result'    # The trained model will be stored here\n","\n","    # load dataset\n","    x = data_clustering   # Input dataset of the transformed daily data\n","    y = None              # The cluster level of input data. Not available for our dataset.\n","    n_clusters = 7        # Number of clusters we want to generate.\n","\n","    #init = 'glorot_uniform'\n","    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","\n","    # prepare the model\n","    cnnmodel = CNNModel(dims=x.shape[1:4], n_clusters=n_clusters, init=init)\n","\n","    cnnmodel.model.summary()\n","    t0 = time()\n","    cnnmodel.compile(optimizer=SGD(0.0000001, 0.9), loss='kld')\n","    y_pred = cnnmodel.fit(x, y=y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n","                     update_interval=update_interval, save_dir=save_dir)\n","    #print('acc:', metrics.acc(y, y_pred))\n","    print('clustering time: ', (time() - t0))\n","    return y_pred"],"metadata":{"id":"GSPnRn0t3pMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = main()\n","silhouette_avg_rdata_daily = silhouette_score(data_nor_eval, result)\n","print(\"The average silhouette_score is :\", silhouette_avg_rdata_daily)"],"metadata":{"id":"4FOCg7AxD6Tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["silhouette_avg_rdata_daily = silhouette_score(data_nor_eval, result)\n","print(\"The average silhouette_score is :\", silhouette_avg_rdata_daily)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkPNr0QkUHUN","outputId":"5c69a249-6650-484e-db08-3f0cd48c6d71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The average silhouette_score is : 0.3560080152580256\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import davies_bouldin_score\n","\n","print(\"Davies-Bouldin score is \", davies_bouldin_score(data_nor_eval, result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ao-iVLqJA4uO","outputId":"e45fe0b0-7001-42ea-aea0-6278d5e14290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Davies-Bouldin score is  1.4769867493186457\n"]}]},{"cell_type":"markdown","source":["# **4. Model testing with pre-trained weights:**\n","To test the model on other datasets we have to create the model and initialize the model with pre-trained weights stored in the drive. Then we have to call the main function to get the clustering results."],"metadata":{"id":"wNU80tzWSy5n"}},{"cell_type":"code","source":["def main_test():\n","    # setting the hyper parameters\n","\n","    batch_size = 8       # Batch size for training the CNN model\n","    maxiter = 2e4        # Maximum number of iteration the model will train\n","    update_interval = 30 # After this number of iteration the model will update the cluster assignment\n","    tol = 0.0000001      # If there is a cluster change more than this tollerance the model training will run\n","    save_dir = '/content/drive/MyDrive/my_CNN_result'    # The trained model will be stored here\n","\n","    # load dataset\n","    x = data_clustering   # Input dataset of the transformed daily data\n","    y = None              # The cluster level of input data. Not available for our dataset.\n","    n_clusters = 7        # Number of clusters we want to generate.\n","\n","    #init = 'glorot_uniform'\n","    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","\n","    # prepare the model\n","    cnnmodel = CNNModel(dims=x.shape[1:4], n_clusters=n_clusters, init=init)\n","\n","    cnnmodel.model.summary()\n","    cnnmodel.load_weights('/content/drive/MyDrive/my_CNN_result/CNN_model_final.h5')\n","    t0 = time()\n","    y_pred = cnnmodel.predict(x)\n","    print('clustering time: ', (time() - t0))\n","    return y_pred"],"metadata":{"id":"5Of4U78Y3aU2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_res = main_test()\n","val_res"],"metadata":{"id":"sgw_3olt5Oes"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **5. Plotting the clustering results:**\n","To plot the clustering results first we have to create the model and initialize the model with pre-trained weights. Then we will create a new model by taking the Dense layer output and clustering output from the original model. The Dense layer output will be used to plot the clusters using the dimension reduction algorithm."],"metadata":{"id":"jt48H8Y1YGRe"}},{"cell_type":"code","source":["n_clusters = 7\n","init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n","cnnmodel = CNNModel(dims=data_clustering.shape[1:5], n_clusters=n_clusters, init=init)\n","\n","cnnmodel.model.summary()\n","cnnmodel.load_weights('/content/drive/MyDrive/my_CNN_result/CNN_model_final.h5')"],"metadata":{"id":"ar13qkgQHOqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gradModel = Model(\n","\t\t\tinputs=[cnnmodel.model.input],\n","\t\t\toutputs=[cnnmodel.model.get_layer('fc2').output,\n","\t\t\t\tcnnmodel.model.output])"],"metadata":{"id":"4aAeGYRTHKpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(convOutputs, predictions) = gradModel(data_clustering)"],"metadata":{"id":"W7VNebp_IoQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.lines import Line2D\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","tsne = TSNE(n_components=2, learning_rate='auto', perplexity=10)\n","tsne_data = tsne.fit_transform(convOutputs)\n","\n","tsne_df = pd.DataFrame(tsne_data, columns=['TSNE1','TSNE2'])\n","tsne_df['cluster'] = pd.Categorical(result)\n","\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","    Clusters['Cluster' + str(i)] = np.array(tsne_df[tsne_df.cluster == i].drop(columns=['cluster']))\n","for i in range(len(Clusters)):\n","    Cluster_Centers[i] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","\n","cen_x = [Cluster_Centers[i][0] for i in range(7)]\n","cen_y = [Cluster_Centers[i][1] for i in range(7)]\n","\n","\n","tsne_df['cen_x'] = tsne_df.cluster.map({0:Cluster_Centers[0][0], 1:Cluster_Centers[1][0], 2:Cluster_Centers[2][0],\n","                                        3:Cluster_Centers[3][0], 4:Cluster_Centers[4][0], 5:Cluster_Centers[5][0],\n","                                        6:Cluster_Centers[6][0]})\n","tsne_df['cen_y'] = tsne_df.cluster.map({0:Cluster_Centers[0][1], 1:Cluster_Centers[1][1], 2:Cluster_Centers[2][1],\n","                                        3:Cluster_Centers[3][1], 4:Cluster_Centers[4][1], 5:Cluster_Centers[5][1],\n","                                        6:Cluster_Centers[6][1]})\n","\n","colors = ['blue', 'orange', 'green', 'red', 'purple', 'cyan', 'olive']\n","tsne_df['c'] = tsne_df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3], 4:colors[4], 5:colors[5], 6:colors[6]})\n","\n","fig, ax = plt.subplots(1, figsize=(8,8))\n","# plot data\n","plt.scatter(tsne_df.TSNE1, tsne_df.TSNE2, c=tsne_df.c, alpha = 0.6, s=10,)\n","# plot centroids\n","plt.scatter(cen_x, cen_y, marker='^', c=colors, s=70)\n","# plot lines\n","for idx, val in tsne_df.iterrows():\n","    x = [val.TSNE1, val.cen_x,]\n","    y = [val.TSNE2, val.cen_y]\n","    plt.plot(x, y, c=val.c, alpha=0.2)\n","# legend\n","legend_elements = [Line2D([0], [0], marker='o', color='w', label='Cluster {}'.format(i+1),\n","                   markerfacecolor=mcolor, markersize=5) for i, mcolor in enumerate(colors)]\n","legend_elements.extend([Line2D([0], [0], marker='^', color='w', label='Centroid - C{}'.format(i+1),\n","            markerfacecolor=mcolor, markersize=10) for i, mcolor in enumerate(colors)])\n","\n","plt.legend(handles=legend_elements, title='Clusters', bbox_to_anchor=(1.02, 1), loc='upper left', ncol=2, borderaxespad=0)\n","\n","plt.title('CNN Encoder Clustering Result\\n', loc='left', fontsize=22)\n","plt.xlabel('Feature-1')\n","plt.ylabel('Feature-2')"],"metadata":{"id":"aUl9TmzFGiDL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **6. Evaluation:**\n","To compute the RMSE, variance, and average inter cluster distance we have to use the xarray format of our real data and the clustering result."],"metadata":{"id":"HvdF9hL31DOJ"}},{"cell_type":"code","source":["def total_rmse(data_path,formed_clusters):\n","  processed_data = data_preprocessing(data_path)\n","  trans_data = pd.DataFrame(processed_data)\n","  trans_data['Cluster'] = formed_clusters\n","\n","  # Normalized\n","  # Function that creates two dictionaries that hold all the clusters and cluster centers\n","  def nor_get_clusters_and_centers(input,formed_clusters):\n","    Clusters = {}\n","    Cluster_Centers = {}\n","    for i in set(formed_clusters):\n","      Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))\n","      Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","    return Clusters,Cluster_Centers\n","\n","  intra_rmse = []\n","  sq_diff = []\n","  Clusters,Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)\n","  for i in range(len(Clusters)):\n","    for j in range(len(Clusters['Cluster' + str(i)])):\n","      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]\n","      Sq_diff = (diff)**2\n","      sq_diff.append(Sq_diff)\n","\n","  Sq_diff_sum = np.sum(np.sum(sq_diff))\n","  rmse = np.sqrt(Sq_diff_sum/data_nor_eval.shape[0])\n","  return rmse"],"metadata":{"id":"iDyqPcGPRVJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_rmse('/content/data/ERA5_Dataset.nc', result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679786248873,"user_tz":240,"elapsed":8758,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"1661228d-1b1e-4d7d-d35c-174ec185768d","id":"vS1oDWtNDcEp"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13.74329829019433"]},"metadata":{},"execution_count":141}]},{"cell_type":"markdown","source":["### This cell measure the variances of the generated clusters.  "],"metadata":{"id":"3vqdgBdOW2V7"}},{"cell_type":"code","source":["trans_data = pd.DataFrame(data_nor_eval)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","variances = pd.DataFrame(columns=range(len(Clusters)),index=range(2))\n","for i in range(len(Clusters)):\n","    variances[i].iloc[0] = np.var(Clusters['Cluster' + str(i)])\n","    variances[i].iloc[1] = Clusters['Cluster' + str(i)].shape[0]\n","\n","var_sum = 0\n","for i in range(7):\n","    var_sum = var_sum + (variances[i].iloc[0] * variances[i].iloc[1])\n","\n","var_avg = var_sum/data_nor_eval.shape[0]\n","var_avg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679786179381,"user_tz":240,"elapsed":378,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"97e84f41-9968-437c-920c-5e41df666bac","id":"Uo2mRiwfDcEp"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.044965697050399205"]},"metadata":{},"execution_count":137}]},{"cell_type":"markdown","source":["### The following cell measure the average inter cluster distance.  "],"metadata":{"id":"J7V-K6PvXJAk"}},{"cell_type":"code","source":["from scipy.spatial.distance import cdist,pdist\n","\n","trans_data = pd.DataFrame(data_nor_eval)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","distance_matrix = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))\n","for i in range(len(Clusters)):\n","  for j in range(len(Clusters)):\n","    if i == j:\n","      #distance_matrix[i].iloc[j] = 0\n","      distance_intra = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.max(distance_intra)\n","    elif i > j:\n","       continue\n","    else:\n","      distance = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.min(distance)\n","      distance_matrix[j].iloc[i] = np.min(distance)\n","\n","sum_min = 0\n","for i in range(n_clusters):\n","    sum_min = sum_min + np.min(distance_matrix[i])\n","\n","avg_inter = sum_min/n_clusters\n","avg_inter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679786223028,"user_tz":240,"elapsed":401,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"a1119758-3665-4492-a5e7-8fa2d6a93182","id":"aTRVnBcoDcEp"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.313395563952699"]},"metadata":{},"execution_count":140}]}]}