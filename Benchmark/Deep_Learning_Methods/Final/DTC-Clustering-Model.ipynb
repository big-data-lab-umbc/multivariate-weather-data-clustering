{"cells":[{"cell_type":"markdown","source":["# **Multivariate Climate Data Clustering using Deep Temporal Clustering (DTC) Model**\n","Here we are dealing with climate data that comprises spatial information, time information, and scientific values. The dataset contains the value of 7 parameters for a region of 41 longitudes and 41 latitudes for 365 days in a year. \n","\n","Our goal is to create meaningful clusters of 365 days based on the values of these 7 parameters. We have used the model suggested in the paper with the title \"DEEP TEMPORAL CLUSTERING: FULLY UNSUPERVISED LEARNING OF TIME-DOMAIN FEATURES\". \n","\n","Source: https://github.com/FlorentF9/DeepTemporalClustering/blob/master/TAE.py"],"metadata":{"id":"ssVm7TlSzci0"},"id":"ssVm7TlSzci0"},{"cell_type":"markdown","source":["# **1. Model Creation:**\n","The authors used an autoencoder and a clustering layer in this model on top of the encoder output. DTC model takes a 2D array as input data. The model uses a three-level approach. The first level, implemented as a CNN, reduces the data dimensionality. The second level, implemented as a BI-LSTM, reduces the data dimensionality further and learns the temporal connections between time scales. The third level performs clustering of BI-LSTM latent representations. Mean square error (MSE) and KL divergence losses are used to train the model.  "],"metadata":{"id":"shIrMxTczhkp"},"id":"shIrMxTczhkp"},{"cell_type":"code","source":["!pip install tslearn==0.5.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PI4NHvaQaXC_","executionInfo":{"status":"ok","timestamp":1680276987901,"user_tz":240,"elapsed":13269,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"6882a836-1af4-4822-d55d-3fc81cd9b0af"},"id":"PI4NHvaQaXC_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tslearn==0.5.2\n","  Downloading tslearn-0.5.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (862 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.3/862.3 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from tslearn==0.5.2) (1.10.1)\n","Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from tslearn==0.5.2) (0.56.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from tslearn==0.5.2) (1.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tslearn==0.5.2) (1.22.4)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.9/dist-packages (from tslearn==0.5.2) (0.29.33)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from tslearn==0.5.2) (1.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->tslearn==0.5.2) (67.6.1)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->tslearn==0.5.2) (0.39.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->tslearn==0.5.2) (3.1.0)\n","Installing collected packages: tslearn\n","Successfully installed tslearn-0.5.2\n"]}]},{"cell_type":"code","execution_count":null,"id":"1bf2aba6","metadata":{"id":"1bf2aba6"},"outputs":[],"source":["\"\"\"\n","These distance measures are used to compute the distance between two timesteps. \n","\"\"\"\n","\n","import numpy as np\n","from statsmodels.tsa import stattools\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","\n","\n","def eucl(x, y):\n","    \"\"\"\n","    Euclidean distance between two multivariate time series given as arrays of shape (timesteps, dim)\n","    \"\"\"\n","    d = np.sqrt(np.sum(np.square(x - y), axis=0))\n","    return np.sum(d)\n","\n","\n","def cid(x, y):\n","    \"\"\"\n","    Complexity-Invariant Distance (CID) between two multivariate time series given as arrays of shape (timesteps, dim)\n","    Reference: Batista, Wang & Keogh (2011). A Complexity-Invariant Distance Measure for Time Series. https://doi.org/10.1137/1.9781611972818.60\n","    \"\"\"\n","    assert(len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n","    ce_x = np.sqrt(np.sum(np.square(np.diff(x, axis=0)), axis=0) + 1e-9)\n","    ce_y = np.sqrt(np.sum(np.square(np.diff(y, axis=0)), axis=0) + 1e-9)\n","    d = np.sqrt(np.sum(np.square(x - y), axis=0)) * np.divide(np.maximum(ce_x, ce_y), np.minimum(ce_x, ce_y))\n","    return np.sum(d)\n","\n","\n","def cor(x, y):\n","    \"\"\"\n","    Correlation-based distance (COR) between two multivariate time series given as arrays of shape (timesteps, dim)\n","    \"\"\"\n","    scaler = TimeSeriesScalerMeanVariance()\n","    x_norm = scaler.fit_transform(x)\n","    y_norm = scaler.fit_transform(y)\n","    pcc = np.mean(x_norm * y_norm)  # Pearson correlation coefficients\n","    d = np.sqrt(2.0 * (1.0 - pcc + 1e-9))  # correlation-based similarities\n","    return np.sum(d)\n","\n","\n","def acf(x, y):\n","    \"\"\"\n","    Autocorrelation-based distance (ACF) between two multivariate time series given as arrays of shape (timesteps, dim)\n","    Computes a linearly weighted euclidean distance between the autocorrelation coefficients of the input time series.\n","    Reference: Galeano & Pena (2000). Multivariate Analysis in Vector Time Series.\n","    \"\"\"\n","    assert (len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n","    x_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, x)\n","    y_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, y)\n","    weights = np.linspace(1.0, 0.0, x.shape[0])\n","    d = np.sqrt(np.sum(np.expand_dims(weights, axis=1) * np.square(x_acf - y_acf), axis=0))\n","    return np.sum(d)"]},{"cell_type":"code","execution_count":null,"id":"1042ed43","metadata":{"id":"1042ed43"},"outputs":[],"source":["\"\"\"\n","The clustering layer is defined here.\n","\"\"\"\n","\n","#from keras.engine.topology import Layer, InputSpec\n","from tensorflow.keras.layers import Layer, InputSpec\n","import keras.backend as K\n","\n","\n","class TSClusteringLayer(Layer):\n","    \"\"\"\n","    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n","    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n","    # Arguments\n","        n_clusters: number of clusters.\n","        weights: list of Numpy array with shape `(n_clusters, timesteps, n_features)` witch represents the initial cluster centers.\n","        alpha: parameter in Student's t-distribution. Default to 1.0.\n","        dist_metric: distance metric between sequences used in similarity kernel ('eucl', 'cir', 'cor' or 'acf').\n","    # Input shape\n","        3D tensor with shape: `(n_samples, timesteps, n_features)`.\n","    # Output shape\n","        2D tensor with shape: `(n_samples, n_clusters)`.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters, weights=None, alpha=1.0, dist_metric='eucl', **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(TSClusteringLayer, self).__init__(**kwargs)\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.dist_metric = dist_metric\n","        self.initial_weights = weights\n","        self.input_spec = InputSpec(ndim=3)\n","        self.clusters = None\n","        self.built = False\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        input_dim = input_shape[2]\n","        input_steps = input_shape[1]\n","        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_steps, input_dim))\n","        self.clusters = self.add_weight(shape=(self.n_clusters, input_steps, input_dim), initializer='glorot_uniform', name='cluster_centers')\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","        self.built = True\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\"\n","        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n","            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n","        Arguments:\n","            inputs: encoded input sequences, shape=(n_samples, timesteps, n_features)\n","        Return:\n","            q: soft labels for each sample. shape=(n_samples, n_clusters)\n","        \"\"\"\n","        if self.dist_metric == 'eucl':\n","            distance = K.sum(K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2)), axis=-1)\n","        elif self.dist_metric == 'cid':\n","            ce_x = K.sqrt(K.sum(K.square(inputs[:, 1:, :] - inputs[:, :-1, :]), axis=1))  # shape (n_samples, n_features)\n","            ce_w = K.sqrt(K.sum(K.square(self.clusters[:, 1:, :] - self.clusters[:, :-1, :]), axis=1))  # shape (n_clusters, n_features)\n","            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, n_features)\n","            ed = K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2))  # shape (n_samples, n_clusters, n_features)\n","            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n","        elif self.dist_metric == 'cor':\n","            inputs_norm = (inputs - K.expand_dims(K.mean(inputs, axis=1), axis=1)) / K.expand_dims(K.std(inputs, axis=1), axis=1)  # shape (n_samples, timesteps, n_features)\n","            clusters_norm = (self.clusters - K.expand_dims(K.mean(self.clusters, axis=1), axis=1)) / K.expand_dims(K.std(self.clusters, axis=1), axis=1)  # shape (n_clusters, timesteps, n_features)\n","            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * clusters_norm, axis=2)  # Pearson correlation coefficients\n","            distance = K.sum(K.sqrt(2.0 * (1.0 - pcc)), axis=-1)  # correlation-based similarities, shape (n_samples, n_clusters)\n","        elif self.dist_metric == 'acf':\n","            raise NotImplementedError\n","        else:\n","            raise ValueError('Available distances are eucl, cid, cor and acf!')\n","        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n","        q **= (self.alpha + 1.0) / 2.0\n","        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n","        return q\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) == 3\n","        return input_shape[0], self.n_clusters\n","\n","    def get_config(self):\n","        config = {'n_clusters': self.n_clusters, 'dist_metric': self.dist_metric}\n","        base_config = super(TSClusteringLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"]},{"cell_type":"code","execution_count":null,"id":"51382918","metadata":{"id":"51382918"},"outputs":[],"source":["\"\"\"\n","The structure of the encoder and decoder is defined here. To define the autoencoder the encoder and decoder are merged together.\n","To train this model without GPU comment out the CuDNNLSTM layers and use LSTM layers.\n","\"\"\"\n","\n","from keras.models import Model\n","from keras.layers import Input, Conv1D, LeakyReLU, MaxPool1D, CuDNNLSTM, LSTM, Bidirectional, TimeDistributed, Dense, Reshape\n","from keras.layers import UpSampling2D, Conv2DTranspose\n","\n","\n","def temporal_autoencoder(input_dim, timesteps, n_filters=50, kernel_size=10, strides=1, pool_size=10, n_units=[50, 1]):\n","    \"\"\"\n","    Temporal Autoencoder (TAE) model with Convolutional and BiLSTM layers.\n","    # Arguments\n","        input_dim: input dimension\n","        timesteps: number of timesteps (can be None for variable length sequences)\n","        n_filters: number of filters in convolutional layer\n","        kernel_size: size of kernel in convolutional layer\n","        strides: strides in convolutional layer\n","        pool_size: pooling size in max pooling layer, must divide time series length\n","        n_units: numbers of units in the two BiLSTM layers\n","        alpha: coefficient in Student's kernel\n","        dist_metric: distance metric between latent sequences\n","    # Return\n","        (ae_model, encoder_model, decoder_model): AE, encoder and decoder models\n","    \"\"\"\n","    assert(timesteps % pool_size == 0)\n","\n","    # Input\n","    x = Input(shape=(timesteps, input_dim), name='input_seq')\n","\n","    # Encoder\n","    encoded = Conv1D(n_filters, kernel_size, strides=strides, padding='same', activation='linear')(x)\n","    encoded = LeakyReLU()(encoded)\n","    encoded = MaxPool1D(pool_size)(encoded)\n","    encoded = Bidirectional(CuDNNLSTM(n_units[0], return_sequences=True), merge_mode='sum')(encoded)\n","    #encoded = Bidirectional(LSTM(n_units[0], return_sequences=True), merge_mode='sum')(encoded)\n","    encoded = LeakyReLU()(encoded)\n","    encoded = Bidirectional(CuDNNLSTM(n_units[1], return_sequences=True), merge_mode='sum')(encoded)\n","    #encoded = Bidirectional(LSTM(n_units[1], return_sequences=True), merge_mode='sum')(encoded)\n","    encoded = LeakyReLU(name='latent')(encoded)\n","\n","    # Decoder\n","    decoded = Reshape((-1, 1, n_units[1]), name='reshape')(encoded)\n","    decoded = UpSampling2D((pool_size, 1), name='upsampling')(decoded)  #decoded = UpSampling1D(pool_size, name='upsampling')(decoded)\n","    decoded = Conv2DTranspose(input_dim, (kernel_size, 1), padding='same', name='conv2dtranspose')(decoded)\n","    output = Reshape((-1, input_dim), name='output_seq')(decoded)  #output = Conv1D(1, kernel_size, strides=strides, padding='same', activation='linear', name='output_seq')(decoded)\n","\n","    # AE model\n","    autoencoder = Model(inputs=x, outputs=output, name='AE')\n","\n","    # Encoder model\n","    encoder = Model(inputs=x, outputs=encoded, name='encoder')\n","\n","    # Create input for decoder model\n","    encoded_input = Input(shape=(timesteps // pool_size, n_units[1]), name='decoder_input')\n","\n","    # Internal layers in decoder\n","    decoded = autoencoder.get_layer('reshape')(encoded_input)\n","    decoded = autoencoder.get_layer('upsampling')(decoded)\n","    decoded = autoencoder.get_layer('conv2dtranspose')(decoded)\n","    decoder_output = autoencoder.get_layer('output_seq')(decoded)\n","\n","    # Decoder model\n","    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n","\n","    return autoencoder, encoder, decoder\n"]},{"cell_type":"code","execution_count":null,"id":"dbe2e626","metadata":{"id":"dbe2e626"},"outputs":[],"source":["\"\"\"\n","The complete DTC model is created in this block using the autoencoder and clustering layer. \n","\"\"\"\n","\n","# Utilities\n","import os\n","import csv\n","import argparse\n","from time import time\n","\n","# Keras\n","from keras.models import Model\n","from keras.layers import Dense, Reshape, UpSampling2D, Conv2DTranspose, GlobalAveragePooling1D, Softmax\n","from keras.losses import kullback_leibler_divergence\n","import keras.backend as K\n","\n","# scikit-learn\n","from sklearn.cluster import AgglomerativeClustering, KMeans\n","\n","import numpy as np\n","\n","\n","class DTC:\n","    \"\"\"\n","    Deep Temporal Clustering (DTC) model\n","    # Arguments\n","        n_clusters: number of clusters\n","        input_dim: input dimensionality\n","        timesteps: length of input sequences (can be None for variable length)\n","        n_filters: number of filters in convolutional layer\n","        kernel_size: size of kernel in convolutional layer\n","        strides: strides in convolutional layer\n","        pool_size: pooling size in max pooling layer, must divide the time series length\n","        n_units: numbers of units in the two BiLSTM layers\n","        alpha: coefficient in Student's kernel\n","        dist_metric: distance metric between latent sequences\n","        cluster_init: cluster initialization method\n","    \"\"\"\n","\n","    def __init__(self, n_clusters, input_dim, timesteps,\n","                 n_filters=50, kernel_size=10, strides=1, pool_size=8, n_units=[50, 1],\n","                 alpha=1.0, dist_metric='eucl', cluster_init='kmeans', heatmap=False):\n","        assert(timesteps % pool_size == 0)\n","        self.n_clusters = n_clusters\n","        self.input_dim = input_dim\n","        self.timesteps = timesteps\n","        self.n_filters = n_filters\n","        self.kernel_size = kernel_size\n","        self.strides = strides\n","        self.pool_size = pool_size\n","        self.n_units = n_units\n","        self.latent_shape = (self.timesteps // self.pool_size, self.n_units[1])\n","        self.alpha = alpha\n","        self.dist_metric = dist_metric\n","        self.cluster_init = cluster_init\n","        self.heatmap = heatmap\n","        self.pretrained = False\n","        self.model = self.autoencoder = self.encoder = self.decoder = None\n","        if self.heatmap:\n","            self.heatmap_model = None\n","            self.heatmap_loss_weight = None\n","            self.initial_heatmap_loss_weight = None\n","            self.final_heatmap_loss_weight = None\n","            self.finetune_heatmap_at_epoch = None\n","\n","    def initialize(self):\n","        \"\"\"\n","        Create DTC model\n","        \"\"\"\n","        # Create AE models\n","        self.autoencoder, self.encoder, self.decoder = temporal_autoencoder(input_dim=self.input_dim,\n","                                                                            timesteps=self.timesteps,\n","                                                                            n_filters=self.n_filters,\n","                                                                            kernel_size=self.kernel_size,\n","                                                                            strides=self.strides,\n","                                                                            pool_size=self.pool_size,\n","                                                                            n_units=self.n_units)\n","        clustering_layer = TSClusteringLayer(self.n_clusters,\n","                                             alpha=self.alpha,\n","                                             dist_metric=self.dist_metric,\n","                                             name='TSClustering')(self.encoder.output)\n","\n","        # Heatmap-generating network\n","        if self.heatmap:\n","            n_heatmap_filters = self.n_clusters  # one heatmap (class activation map) per cluster\n","            encoded = self.encoder.output\n","            heatmap_layer = Reshape((-1, 1, self.n_units[1]))(encoded)\n","            heatmap_layer = UpSampling2D((self.pool_size, 1))(heatmap_layer)\n","            heatmap_layer = Conv2DTranspose(n_heatmap_filters, (self.kernel_size, 1), padding='same')(heatmap_layer)\n","            # The next one is the heatmap layer we will visualize\n","            heatmap_layer = Reshape((-1, n_heatmap_filters), name='Heatmap')(heatmap_layer)\n","            heatmap_output_layer = GlobalAveragePooling1D()(heatmap_layer)\n","            # A dense layer must be added only if `n_heatmap_filters` is different from `n_clusters`\n","            # heatmap_output_layer = Dense(self.n_clusters, activation='relu')(heatmap_output_layer)\n","            heatmap_output_layer = Softmax()(heatmap_output_layer)  # normalize activations with softmax\n","\n","        if self.heatmap:\n","            # Create DTC model\n","            self.model = Model(inputs=self.autoencoder.input,\n","                               outputs=[self.autoencoder.output, clustering_layer, heatmap_output_layer])\n","            # Create Heatmap model\n","            self.heatmap_model = Model(inputs=self.autoencoder.input,\n","                                       outputs=heatmap_layer)\n","        else:\n","            # Create DTC model\n","            self.model = Model(inputs=self.autoencoder.input,\n","                               outputs=[self.autoencoder.output, clustering_layer])\n","\n","    @property\n","    def cluster_centers_(self):\n","        \"\"\"\n","        Returns cluster centers\n","        \"\"\"\n","        return self.model.get_layer(name='TSClustering').get_weights()[0]\n","\n","    @staticmethod\n","    def weighted_kld(loss_weight):\n","        \"\"\"\n","        Custom KL-divergence loss with a variable weight parameter\n","        \"\"\"\n","        def loss(y_true, y_pred):\n","            return loss_weight * kullback_leibler_divergence(y_true, y_pred)\n","        return loss\n","\n","    def on_epoch_end(self, epoch):\n","        \"\"\"\n","        Update heatmap loss weight on epoch end\n","        \"\"\"\n","        if epoch > self.finetune_heatmap_at_epoch:\n","            K.set_value(self.heatmap_loss_weight, self.final_heatmap_loss_weight)\n","\n","    def compile(self, gamma, optimizer, initial_heatmap_loss_weight=None, final_heatmap_loss_weight=None):\n","        \"\"\"\n","        Compile DTC model\n","        # Arguments\n","            gamma: coefficient of TS clustering loss\n","            optimizer: optimization algorithm\n","            initial_heatmap_loss_weight (optional): initial weight of heatmap loss vs clustering loss\n","            final_heatmap_loss_weight (optional): final weight of heatmap loss vs clustering loss (heatmap finetuning)\n","        \"\"\"\n","        if self.heatmap:\n","            self.initial_heatmap_loss_weight = initial_heatmap_loss_weight\n","            self.final_heatmap_loss_weight = final_heatmap_loss_weight\n","            self.heatmap_loss_weight = K.variable(self.initial_heatmap_loss_weight)\n","            self.model.compile(loss=['mse', DTC.weighted_kld(1.0 - self.heatmap_loss_weight), DTC.weighted_kld(self.heatmap_loss_weight)],\n","                               loss_weights=[1.0, gamma, gamma],\n","                               optimizer=optimizer)\n","        else:\n","            self.model.compile(loss=['mse', 'kld'],\n","                               loss_weights=[1.0, gamma],\n","                               optimizer=optimizer)\n","\n","    def load_weights(self, weights_path):\n","        \"\"\"\n","        Load pre-trained weights of DTC model\n","        # Arguments\n","            weight_path: path to weights file (.h5)\n","        \"\"\"\n","        self.model.load_weights(weights_path)\n","        self.pretrained = True\n","\n","    def load_ae_weights(self, ae_weights_path):\n","        \"\"\"\n","        Load pre-trained weights of AE\n","        # Arguments\n","            ae_weight_path: path to weights file (.h5)\n","        \"\"\"\n","        self.autoencoder.load_weights(ae_weights_path)\n","        self.pretrained = True\n","\n","    def dist(self, x1, x2):\n","        \"\"\"\n","        Compute distance between two multivariate time series using chosen distance metric\n","        # Arguments\n","            x1: first input (np array)\n","            x2: second input (np array)\n","        # Return\n","            distance\n","        \"\"\"\n","        if self.dist_metric == 'eucl':\n","            return eucl(x1, x2)\n","        elif self.dist_metric == 'cid':\n","            return cid(x1, x2)\n","        elif self.dist_metric == 'cor':\n","            return cor(x1, x2)\n","        elif self.dist_metric == 'acf':\n","            return acf(x1, x2)\n","        else:\n","            raise ValueError('Available distances are eucl, cid, cor and acf!')\n","\n","    def init_cluster_weights(self, X):\n","        \"\"\"\n","        Initialize with complete-linkage hierarchical clustering or k-means.\n","        # Arguments\n","            X: numpy array containing training set or batch\n","        \"\"\"\n","        assert(self.cluster_init in ['hierarchical', 'kmeans'])\n","        print('Initializing cluster...')\n","\n","        features = self.encode(X)\n","\n","        if self.cluster_init == 'hierarchical':\n","            if self.dist_metric == 'eucl':  # use AgglomerativeClustering off-the-shelf\n","                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n","                                             affinity='euclidean',\n","                                             linkage='complete').fit(features.reshape(features.shape[0], -1))\n","            else:  # compute distance matrix using dist\n","                d = np.zeros((features.shape[0], features.shape[0]))\n","                for i in range(features.shape[0]):\n","                    for j in range(i):\n","                        d[i, j] = d[j, i] = self.dist(features[i], features[j])\n","                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n","                                             affinity='precomputed',\n","                                             linkage='complete').fit(d)\n","            # compute centroid\n","            cluster_centers = np.array([features[hc.labels_ == c].mean(axis=0) for c in range(self.n_clusters)])\n","        elif self.cluster_init == 'kmeans':\n","            # fit k-means on flattened features\n","            km = KMeans(n_clusters=self.n_clusters, n_init=10).fit(features.reshape(features.shape[0], -1))\n","            cluster_centers = km.cluster_centers_.reshape(self.n_clusters, features.shape[1], features.shape[2])\n","\n","        self.model.get_layer(name='TSClustering').set_weights([cluster_centers])\n","        print('Done!')\n","\n","    def encode(self, x):\n","        \"\"\"\n","        Encoding function. Extract latent features from hidden layer\n","        # Arguments\n","            x: data point\n","        # Return\n","            encoded (latent) data point\n","        \"\"\"\n","        return self.encoder.predict(x)\n","\n","    def decode(self, x):\n","        \"\"\"\n","        Decoding function. Decodes encoded sequence from latent space.\n","        # Arguments\n","            x: encoded (latent) data point\n","        # Return\n","            decoded data point\n","        \"\"\"\n","        return self.decoder.predict(x)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict cluster assignment.\n","        \"\"\"\n","        q = self.model.predict(x, verbose=0)[1]\n","        return q.argmax(axis=1)\n","\n","    @staticmethod\n","    def target_distribution(q):  # target distribution p which enhances the discrimination of soft label q\n","        weight = q ** 2 / q.sum(0)\n","        return (weight.T / weight.sum(1)).T\n","\n","    def predict_heatmap(self, x):\n","        \"\"\"\n","        Produces TS clustering heatmap from input sequence.\n","        # Arguments\n","            x: data point\n","        # Return\n","            heatmap\n","        \"\"\"\n","        return self.heatmap_model.predict(x, verbose=0)\n","\n","    def pretrain(self, X,\n","                 optimizer='adam',\n","                 epochs=10,\n","                 batch_size=64,\n","                 save_dir='/content/drive/MyDrive/result/tmp',\n","                 verbose=1):\n","        \"\"\"\n","        Pre-train the autoencoder using only MSE reconstruction loss\n","        Saves weights in h5 format.\n","        # Arguments\n","            X: training set\n","            optimizer: optimization algorithm\n","            epochs: number of pre-training epochs\n","            batch_size: training batch size\n","            save_dir: path to existing directory where weights will be saved\n","        \"\"\"\n","        print('Pretraining...')\n","        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n","\n","        # Begin pretraining\n","        t0 = time()\n","        self.autoencoder.fit(X, X, batch_size=batch_size, epochs=epochs, verbose=verbose)\n","        print('Pretraining time: ', time() - t0)\n","        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n","        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n","        self.pretrained = True\n","\n","    def fit(self, X_train, y_train=None,\n","            X_val=None, y_val=None,\n","            epochs=100,\n","            eval_epochs=10,\n","            save_epochs=10,\n","            batch_size=64,\n","            tol=0.001,\n","            patience=5,\n","            finetune_heatmap_at_epoch=8,\n","            save_dir='/content/drive/MyDrive/result/tmp'):\n","        \"\"\"\n","        Training procedure\n","        # Arguments\n","           X_train: training set\n","           y_train: (optional) training labels\n","           X_val: (optional) validation set\n","           y_val: (optional) validation labels\n","           epochs: number of training epochs\n","           eval_epochs: evaluate metrics on train/val set every eval_epochs epochs\n","           save_epochs: save model weights every save_epochs epochs\n","           batch_size: training batch size\n","           tol: tolerance for stopping criterion\n","           patience: patience for stopping criterion\n","           finetune_heatmap_at_epoch: epoch number where heatmap finetuning will start. Heatmap loss weight will\n","                                      switch from `self.initial_heatmap_loss_weight` to `self.final_heatmap_loss_weight`\n","           save_dir: path to existing directory where weights and logs are saved\n","        \"\"\"\n","        if not self.pretrained:\n","            print('Autoencoder was not pre-trained!')\n","\n","        if self.heatmap:\n","            self.finetune_heatmap_at_epoch = finetune_heatmap_at_epoch\n","\n","        # Logging file\n","        logfile = open(save_dir + '/dtc_log.csv', 'w')\n","        fieldnames = ['epoch', 'T', 'L', 'Lr', 'Lc']\n","        if X_val is not None:\n","            fieldnames += ['L_val', 'Lr_val', 'Lc_val']\n","        if y_train is not None:\n","            fieldnames += ['acc', 'pur', 'nmi', 'ari']\n","        if y_val is not None:\n","            fieldnames += ['acc_val', 'pur_val', 'nmi_val', 'ari_val']\n","        logwriter = csv.DictWriter(logfile, fieldnames)\n","        logwriter.writeheader()\n","\n","        y_pred_last = None\n","        patience_cnt = 0\n","\n","        print('Training for {} epochs.\\nEvaluating every {} and saving model every {} epochs.'.format(epochs, eval_epochs, save_epochs))\n","\n","        for epoch in range(epochs):\n","\n","            # Compute cluster assignments for training set\n","            q = self.model.predict(X_train)[1]\n","            p = DTC.target_distribution(q)\n","\n","            # Evaluate losses and metrics on training set\n","            if epoch % eval_epochs == 0:\n","\n","                # Initialize log dictionary\n","                logdict = dict(epoch=epoch)\n","\n","                y_pred = q.argmax(axis=1)\n","                if X_val is not None:\n","                    q_val = self.model.predict(X_val)[1]\n","                    p_val = DTC.target_distribution(q_val)\n","                    y_val_pred = q_val.argmax(axis=1)\n","\n","                print('epoch {}'.format(epoch))\n","                if self.heatmap:\n","                    loss = self.model.evaluate(X_train, [X_train, p, p], batch_size=batch_size, verbose=False)\n","                else:\n","                    loss = self.model.evaluate(X_train, [X_train, p], batch_size=batch_size, verbose=False)\n","                logdict['L'] = loss[0]\n","                logdict['Lr'] = loss[1]\n","                logdict['Lc'] = loss[2]\n","                print('[Train] - Lr={:f}, Lc={:f} - total loss={:f}'.format(logdict['Lr'], logdict['Lc'], logdict['L']))\n","                if X_val is not None:\n","                    val_loss = self.model.evaluate(X_val, [X_val, p_val], batch_size=batch_size, verbose=False)\n","                    logdict['L_val'] = val_loss[0]\n","                    logdict['Lr_val'] = val_loss[1]\n","                    logdict['Lc_val'] = val_loss[2]\n","                    print('[Val] - Lr={:f}, Lc={:f} - total loss={:f}'.format(logdict['Lr_val'], logdict['Lc_val'], logdict['L_val']))\n","\n","                # Evaluate the clustering performance using labels\n","                if y_train is not None:\n","                    logdict['acc'] = cluster_acc(y_train, y_pred)\n","                    logdict['pur'] = cluster_purity(y_train, y_pred)\n","                    logdict['nmi'] = metrics.normalized_mutual_info_score(y_train, y_pred)\n","                    logdict['ari'] = metrics.adjusted_rand_score(y_train, y_pred)\n","                    print('[Train] - Acc={:f}, Pur={:f}, NMI={:f}, ARI={:f}'.format(logdict['acc'], logdict['pur'],\n","                                                                                    logdict['nmi'], logdict['ari']))\n","                if y_val is not None:\n","                    logdict['acc_val'] = cluster_acc(y_val, y_val_pred)\n","                    logdict['pur_val'] = cluster_purity(y_val, y_val_pred)\n","                    logdict['nmi_val'] = metrics.normalized_mutual_info_score(y_val, y_val_pred)\n","                    logdict['ari_val'] = metrics.adjusted_rand_score(y_val, y_val_pred)\n","                    print('[Val] - Acc={:f}, Pur={:f}, NMI={:f}, ARI={:f}'.format(logdict['acc_val'], logdict['pur_val'],\n","                                                                                  logdict['nmi_val'], logdict['ari_val']))\n","\n","                logwriter.writerow(logdict)\n","\n","                # check stop criterion\n","                if y_pred_last is not None:\n","                    assignment_changes = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n","                y_pred_last = y_pred\n","                if epoch > 0 and assignment_changes < tol:\n","                    patience_cnt += 1\n","                    print('Assignment changes {} < {} tolerance threshold. Patience: {}/{}.'.format(assignment_changes, tol, patience_cnt, patience))\n","                    if patience_cnt >= patience:\n","                        print('Reached max patience. Stopping training.')\n","                        logfile.close()\n","                        break\n","                else:\n","                    patience_cnt = 0\n","\n","            # Save intermediate model and plots\n","            if epoch % save_epochs == 0:\n","                self.model.save_weights(save_dir + '/DTC_model_' + str(epoch) + '.h5')\n","                print('Saved model to:', save_dir + '/DTC_model_' + str(epoch) + '.h5')\n","\n","            # Train for one epoch\n","            if self.heatmap:\n","                self.model.fit(X_train, [X_train, p, p], epochs=1, batch_size=batch_size, verbose=False)\n","                self.on_epoch_end(epoch)\n","            else:\n","                self.model.fit(X_train, [X_train, p], epochs=1, batch_size=batch_size, verbose=False)\n","\n","        # Save the final model\n","        logfile.close()\n","        print('Saving model to:', save_dir + '/DTC_model_final.h5')\n","        self.model.save_weights(save_dir + '/DTC_model_final.h5')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0OuCHOjcwhZ","executionInfo":{"status":"ok","timestamp":1680277059561,"user_tz":240,"elapsed":14763,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"412c54ff-decb-45e7-a2c7-13dd58c1a0b3"},"id":"M0OuCHOjcwhZ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install netCDF4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWE9Zb8OyNFo","executionInfo":{"status":"ok","timestamp":1680277072949,"user_tz":240,"elapsed":8104,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"005d358c-8251-46f1-8a5f-8196cde10948"},"id":"kWE9Zb8OyNFo","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting netCDF4\n","  Downloading netCDF4-1.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cftime\n","  Downloading cftime-1.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from netCDF4) (1.22.4)\n","Installing collected packages: cftime, netCDF4\n","Successfully installed cftime-1.6.2 netCDF4-1.6.3\n"]}]},{"cell_type":"code","source":["import netCDF4 as nc\n","import pandas as pd\n","import numpy as np\n","import xarray as xr\n","import datetime\n","import datetime as dt\n","from netCDF4 import date2num,num2date\n","from math import sqrt"],"metadata":{"id":"DwtEz8jk3DY0"},"id":"DwtEz8jk3DY0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **2. Data preparation**\n","The model takes one row for each data point. But our dataset size is 265x7x41x41. So we have transformed the data of one day from 7x41x41 to one row with 11767 columns. The dataset has some NaN values in the SST variable. To replace these NaN values we used the mean value of the full dataset. The function returns one NumPy array with size (365, 11767). "],"metadata":{"id":"C-B0DxXN141Q"},"id":"C-B0DxXN141Q"},{"cell_type":"code","source":["from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def data_preprocessing(data_path):\n","  rdata_daily = xr.open_dataset(data_path)    # data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'\n","  rdata_daily_np_array = np.array(rdata_daily.to_array())   # the shape of the dailt data is (7, 365, 41, 41)\n","  rdata_daily_np_array_T = rdata_daily_np_array.transpose(1,0,2,3)   # transform the dailt data from (7, 365, 41, 41) to (365, 7, 41, 41)\n","  overall_mean = np.nanmean(rdata_daily_np_array_T[:, :, :, :])\n","  for i in range(rdata_daily_np_array_T.shape[0]):\n","    for j in range(rdata_daily_np_array_T.shape[1]):\n","      for k in range(rdata_daily_np_array_T.shape[2]):\n","        for l in range(rdata_daily_np_array_T.shape[3]):\n","          if np.isnan(rdata_daily_np_array_T[i, j, k, l]):\n","            #print(\"NAN data in \", i, j, k, l)\n","            rdata_daily_np_array_T[i, j, k, l] = overall_mean \n","  rdata_daily_np_array_T_R = rdata_daily_np_array_T.reshape((rdata_daily_np_array_T.shape[0], -1))  # transform the dailt data from (365, 7, 41, 41) to (365, 11767)\n","  min_max_scaler = preprocessing.MinMaxScaler() # calling the function\n","  rdata_daily_np_array_T_R_nor = min_max_scaler.fit_transform(rdata_daily_np_array_T_R)   # now normalize the data, otherwise the loss will be very big \n","  rdata_daily_np_array_T_R_nor = np.float32(rdata_daily_np_array_T_R_nor)    # convert the data type to float32, otherwise the loass will be out-of-limit \n","  return rdata_daily_np_array_T_R_nor"],"metadata":{"id":"JHI2SDsE183p"},"id":"JHI2SDsE183p","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'\n","nor_data = data_preprocessing(data_path)"],"metadata":{"id":"LXtGa_Ed1a-k"},"id":"LXtGa_Ed1a-k","execution_count":null,"outputs":[]},{"cell_type":"code","source":["nor_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUhhzu4x1cvV","executionInfo":{"status":"ok","timestamp":1680277221519,"user_tz":240,"elapsed":186,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"6dd63aef-7b1e-485a-b795-75c789a66b04"},"id":"CUhhzu4x1cvV","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(365, 11767)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from sklearn.metrics import silhouette_samples, silhouette_score\n","def silhouette_score(X, labels, *, metric=\"cosine\", sample_size=None, random_state=None, **kwds):  \n"," return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))"],"metadata":{"id":"mhhKOi41aktj"},"id":"mhhKOi41aktj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Model Training**\n","This function defines related parameters to train the model. Then instantiate the model and train on the pre-processed data. The model tries to optimize the clustering loss. After training the model returns the cluster results. "],"metadata":{"id":"JffotIes3Vay"},"id":"JffotIes3Vay"},{"cell_type":"code","source":["class Config(object):\n","    ae_weights = None #, help='pre-trained autoencoder weights')\n","    n_clusters = 7 #, type=int, help='number of clusters')\n","    n_filters = 50 #, type=int, help='number of filters in convolutional layer')\n","    kernel_size = 10 #,  type=int, help='size of kernel in convolutional layer')\n","    strides = 1 #, type=int, help='strides in convolutional layer')\n","    pool_size = 8 #, type=int, help='pooling size in max pooling layer')\n","    n_units = [50, 1] #, type=int, help='numbers of units in the BiLSTM layers')\n","    gamma = 1.0 #, type=float, help='coefficient of clustering loss')\n","    alpha = 1.0 #, type=float, help='coefficient in Student\\'s kernel')\n","    dist_metric ='eucl' #, type=str, choices=['eucl', 'cid', 'cor', 'acf'], help='distance metric between latent sequences')\n","    cluster_init = 'kmeans' #, type=str, choices=['kmeans', 'hierarchical'], help='cluster initialization method')\n","    heatmap = False #, type=bool, help='train heatmap-generating network')\n","    pretrain_epochs = 10 #, type=int)\n","    epochs = 100 #, type=int)\n","    eval_epochs = 1 #, type=int)\n","    save_epochs = 50 #, type=int)\n","    batch_size = 366 #, type=int)\n","    tol = 0.00001 #, type=float, help='tolerance for stopping criterion')\n","    patience = 10 #, type=int, help='patience for stopping criterion')\n","    finetune_heatmap_at_epoch = 8 #, type=int, help='epoch where heatmap finetuning starts')\n","    initial_heatmap_loss_weight = 0.1 #, type=float, help='initial weight of heatmap loss vs clustering loss')\n","    final_heatmap_loss_weight = 0.9 #, type=float, help='final weight of heatmap loss vs clustering loss (heatmap finetuning)')\n","    save_dir = '/content/drive/MyDrive/result/tmp'\n","\n","\n","\n","\n","def main():\n","\n","    args = Config()\n","    print(args)\n","\n","    # Create save directory if not exists\n","    if not os.path.exists(args.save_dir):\n","        os.makedirs(args.save_dir)\n","\n","    # Load data\n","    data_path = '/content/drive/MyDrive/ERA5_Dataset.nc'\n","    xt = data_preprocessing(data_path)\n","    xt = np.append(xt,np.zeros([len(xt),1]),1)\n","    X_train = np.reshape(xt, xt.shape + (1,))\n","    y_train = None\n","    X_val = None\n","    y_val = None\n","\n","    # Find number of clusters\n","    if args.n_clusters is None:\n","        args.n_clusters = 7 #len(np.unique(y_train))\n","\n","    # Set default values\n","    pretrain_optimizer = 'adam'\n","    print(\"Timestep: \", X_train.shape[1], \"Pool Size: \", args.pool_size)\n","\n","    # Instantiate model\n","    dtc = DTC(n_clusters=args.n_clusters,\n","              input_dim=X_train.shape[-1],\n","              timesteps=X_train.shape[1],\n","              n_filters=args.n_filters,\n","              kernel_size=args.kernel_size,\n","              strides=args.strides,\n","              pool_size=args.pool_size,\n","              n_units=args.n_units,\n","              alpha=args.alpha,\n","              dist_metric=args.dist_metric,\n","              cluster_init=args.cluster_init,\n","              heatmap=args.heatmap)\n","\n","    # Initialize model\n","    optimizer = 'adam'\n","    dtc.initialize()\n","    dtc.model.summary()\n","    dtc.compile(gamma=args.gamma, optimizer=optimizer, initial_heatmap_loss_weight=args.initial_heatmap_loss_weight,\n","                final_heatmap_loss_weight=args.final_heatmap_loss_weight)\n","\n","    # Load pre-trained AE weights or pre-train\n","    if args.ae_weights is None and args.pretrain_epochs > 0:\n","        dtc.pretrain(X=X_train, optimizer=pretrain_optimizer,\n","                     epochs=args.pretrain_epochs, batch_size=args.batch_size,\n","                     save_dir=args.save_dir)\n","    elif args.ae_weights is not None:\n","        dtc.load_ae_weights(args.ae_weights)\n","\n","    # Initialize clusters\n","    dtc.init_cluster_weights(X_train)\n","\n","    # Fit model\n","    t0 = time()\n","    dtc.fit(X_train, y_train, X_val, y_val, args.epochs, args.eval_epochs, args.save_epochs, args.batch_size,\n","            args.tol, args.patience, args.finetune_heatmap_at_epoch, args.save_dir)\n","    print('Training time: ', (time() - t0))\n","\n","    # Evaluate\n","    print('Performance (TRAIN)')\n","    results = {}\n","    q = dtc.model.predict(X_train)[1]\n","    y_pred = q.argmax(axis=1)\n","    return y_pred"],"metadata":{"id":"OlBGu1XX1DLT"},"id":"OlBGu1XX1DLT","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d9ddc626","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"d9ddc626","outputId":"7f02e282-9e3c-4f9d-e8e5-40d86b83a443","executionInfo":{"status":"ok","timestamp":1680240568783,"user_tz":240,"elapsed":237045,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.Config object at 0x7fbef5552280>\n","Timestep:  11768 Pool Size:  8\n","Model: \"model_8\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_seq (InputLayer)         [(None, 11768, 1)]   0           []                               \n","                                                                                                  \n"," conv1d_8 (Conv1D)              (None, 11768, 50)    550         ['input_seq[0][0]']              \n","                                                                                                  \n"," leaky_re_lu_16 (LeakyReLU)     (None, 11768, 50)    0           ['conv1d_8[0][0]']               \n","                                                                                                  \n"," max_pooling1d_8 (MaxPooling1D)  (None, 1471, 50)    0           ['leaky_re_lu_16[0][0]']         \n","                                                                                                  \n"," bidirectional_16 (Bidirectiona  (None, 1471, 50)    40800       ['max_pooling1d_8[0][0]']        \n"," l)                                                                                               \n","                                                                                                  \n"," leaky_re_lu_17 (LeakyReLU)     (None, 1471, 50)     0           ['bidirectional_16[0][0]']       \n","                                                                                                  \n"," bidirectional_17 (Bidirectiona  (None, 1471, 1)     424         ['leaky_re_lu_17[0][0]']         \n"," l)                                                                                               \n","                                                                                                  \n"," latent (LeakyReLU)             (None, 1471, 1)      0           ['bidirectional_17[0][0]']       \n","                                                                                                  \n"," reshape (Reshape)              (None, 1471, 1, 1)   0           ['latent[0][0]']                 \n","                                                                                                  \n"," upsampling (UpSampling2D)      (None, 11768, 1, 1)  0           ['reshape[0][0]']                \n","                                                                                                  \n"," conv2dtranspose (Conv2DTranspo  (None, 11768, 1, 1)  11         ['upsampling[0][0]']             \n"," se)                                                                                              \n","                                                                                                  \n"," output_seq (Reshape)           (None, 11768, 1)     0           ['conv2dtranspose[0][0]']        \n","                                                                                                  \n"," TSClustering (TSClusteringLaye  (None, 7)           10297       ['latent[0][0]']                 \n"," r)                                                                                               \n","                                                                                                  \n","==================================================================================================\n","Total params: 52,082\n","Trainable params: 52,082\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Pretraining...\n","Epoch 1/10\n","1/1 [==============================] - 9s 9s/step - loss: 0.2792\n","Epoch 2/10\n","1/1 [==============================] - 0s 358ms/step - loss: 0.2113\n","Epoch 3/10\n","1/1 [==============================] - 0s 359ms/step - loss: 0.1557\n","Epoch 4/10\n","1/1 [==============================] - 0s 362ms/step - loss: 0.1140\n","Epoch 5/10\n","1/1 [==============================] - 0s 362ms/step - loss: 0.0825\n","Epoch 6/10\n","1/1 [==============================] - 0s 359ms/step - loss: 0.0584\n","Epoch 7/10\n","1/1 [==============================] - 0s 350ms/step - loss: 0.0414\n","Epoch 8/10\n","1/1 [==============================] - 0s 352ms/step - loss: 0.0311\n","Epoch 9/10\n","1/1 [==============================] - 0s 352ms/step - loss: 0.0263\n","Epoch 10/10\n","1/1 [==============================] - 0s 351ms/step - loss: 0.0260\n","Pretraining time:  13.96465277671814\n","Pretrained weights are saved to /content/drive/MyDrive/result/tmp/ae_weights-epoch10.h5\n","Initializing cluster...\n","12/12 [==============================] - 2s 83ms/step\n","Done!\n","Training for 100 epochs.\n","Evaluating every 1 and saving model every 50 epochs.\n","12/12 [==============================] - 1s 69ms/step\n","epoch 0\n","[Train] - Lr=0.028686, Lc=0.164391 - total loss=0.193077\n","Saved model to: /content/drive/MyDrive/result/tmp/DTC_model_0.h5\n","12/12 [==============================] - 1s 87ms/step\n","epoch 1\n","[Train] - Lr=0.026698, Lc=0.118589 - total loss=0.145287\n","12/12 [==============================] - 1s 67ms/step\n","epoch 2\n","[Train] - Lr=0.026509, Lc=0.140095 - total loss=0.166604\n","12/12 [==============================] - 1s 60ms/step\n","epoch 3\n","[Train] - Lr=0.027354, Lc=0.165493 - total loss=0.192847\n","12/12 [==============================] - 1s 61ms/step\n","epoch 4\n","[Train] - Lr=0.029307, Lc=0.149587 - total loss=0.178895\n","12/12 [==============================] - 1s 61ms/step\n","epoch 5\n","[Train] - Lr=0.028597, Lc=0.150822 - total loss=0.179419\n","12/12 [==============================] - 1s 60ms/step\n","epoch 6\n","[Train] - Lr=0.026891, Lc=0.163804 - total loss=0.190695\n","12/12 [==============================] - 1s 79ms/step\n","epoch 7\n","[Train] - Lr=0.026027, Lc=0.162890 - total loss=0.188917\n","12/12 [==============================] - 1s 86ms/step\n","epoch 8\n","[Train] - Lr=0.026183, Lc=0.156243 - total loss=0.182425\n","12/12 [==============================] - 1s 60ms/step\n","epoch 9\n","[Train] - Lr=0.026354, Lc=0.156577 - total loss=0.182931\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 60ms/step\n","epoch 10\n","[Train] - Lr=0.026371, Lc=0.162069 - total loss=0.188440\n","12/12 [==============================] - 1s 60ms/step\n","epoch 11\n","[Train] - Lr=0.026559, Lc=0.165649 - total loss=0.192208\n","12/12 [==============================] - 1s 61ms/step\n","epoch 12\n","[Train] - Lr=0.026985, Lc=0.163398 - total loss=0.190384\n","12/12 [==============================] - 1s 61ms/step\n","epoch 13\n","[Train] - Lr=0.027247, Lc=0.161670 - total loss=0.188917\n","12/12 [==============================] - 1s 87ms/step\n","epoch 14\n","[Train] - Lr=0.027166, Lc=0.163854 - total loss=0.191020\n","12/12 [==============================] - 1s 61ms/step\n","epoch 15\n","[Train] - Lr=0.026911, Lc=0.165635 - total loss=0.192546\n","12/12 [==============================] - 1s 61ms/step\n","epoch 16\n","[Train] - Lr=0.026664, Lc=0.163983 - total loss=0.190648\n","12/12 [==============================] - 1s 61ms/step\n","epoch 17\n","[Train] - Lr=0.026400, Lc=0.161858 - total loss=0.188258\n","12/12 [==============================] - 1s 60ms/step\n","epoch 18\n","[Train] - Lr=0.026098, Lc=0.162007 - total loss=0.188105\n","12/12 [==============================] - 1s 61ms/step\n","epoch 19\n","[Train] - Lr=0.025876, Lc=0.163814 - total loss=0.189691\n","12/12 [==============================] - 2s 155ms/step\n","epoch 20\n","[Train] - Lr=0.025869, Lc=0.165368 - total loss=0.191237\n","12/12 [==============================] - 1s 60ms/step\n","epoch 21\n","[Train] - Lr=0.026008, Lc=0.165592 - total loss=0.191600\n","12/12 [==============================] - 1s 63ms/step\n","epoch 22\n","[Train] - Lr=0.026048, Lc=0.165410 - total loss=0.191458\n","12/12 [==============================] - 1s 62ms/step\n","epoch 23\n","[Train] - Lr=0.025867, Lc=0.165620 - total loss=0.191487\n","12/12 [==============================] - 1s 63ms/step\n","epoch 24\n","[Train] - Lr=0.025581, Lc=0.165269 - total loss=0.190850\n","12/12 [==============================] - 1s 60ms/step\n","epoch 25\n","[Train] - Lr=0.025390, Lc=0.164037 - total loss=0.189427\n","12/12 [==============================] - 1s 92ms/step\n","epoch 26\n","[Train] - Lr=0.025391, Lc=0.163035 - total loss=0.188426\n","12/12 [==============================] - 1s 76ms/step\n","epoch 27\n","[Train] - Lr=0.025550, Lc=0.163118 - total loss=0.188669\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 28\n","[Train] - Lr=0.025804, Lc=0.164138 - total loss=0.189943\n","12/12 [==============================] - 1s 61ms/step\n","epoch 29\n","[Train] - Lr=0.026078, Lc=0.165155 - total loss=0.191233\n","12/12 [==============================] - 1s 62ms/step\n","epoch 30\n","[Train] - Lr=0.026246, Lc=0.165475 - total loss=0.191721\n","12/12 [==============================] - 1s 62ms/step\n","epoch 31\n","[Train] - Lr=0.026188, Lc=0.165425 - total loss=0.191613\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 60ms/step\n","epoch 32\n","[Train] - Lr=0.025910, Lc=0.165209 - total loss=0.191119\n","12/12 [==============================] - 1s 84ms/step\n","epoch 33\n","[Train] - Lr=0.025541, Lc=0.164448 - total loss=0.189989\n","12/12 [==============================] - 1s 75ms/step\n","epoch 34\n","[Train] - Lr=0.025231, Lc=0.163397 - total loss=0.188628\n","12/12 [==============================] - 1s 61ms/step\n","epoch 35\n","[Train] - Lr=0.025058, Lc=0.162811 - total loss=0.187870\n","12/12 [==============================] - 1s 62ms/step\n","epoch 36\n","[Train] - Lr=0.025035, Lc=0.162988 - total loss=0.188023\n","12/12 [==============================] - 1s 63ms/step\n","epoch 37\n","[Train] - Lr=0.025144, Lc=0.163657 - total loss=0.188802\n","12/12 [==============================] - 1s 61ms/step\n","epoch 38\n","[Train] - Lr=0.025322, Lc=0.164264 - total loss=0.189586\n","12/12 [==============================] - 1s 66ms/step\n","epoch 39\n","[Train] - Lr=0.025458, Lc=0.164464 - total loss=0.189922\n","12/12 [==============================] - 1s 106ms/step\n","epoch 40\n","[Train] - Lr=0.025466, Lc=0.164284 - total loss=0.189750\n","12/12 [==============================] - 1s 110ms/step\n","epoch 41\n","[Train] - Lr=0.025352, Lc=0.163732 - total loss=0.189084\n","12/12 [==============================] - 1s 61ms/step\n","epoch 42\n","[Train] - Lr=0.025189, Lc=0.162991 - total loss=0.188180\n","12/12 [==============================] - 1s 61ms/step\n","epoch 43\n","[Train] - Lr=0.025051, Lc=0.162508 - total loss=0.187559\n","12/12 [==============================] - 1s 60ms/step\n","epoch 44\n","[Train] - Lr=0.024980, Lc=0.162525 - total loss=0.187505\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 61ms/step\n","epoch 45\n","[Train] - Lr=0.024978, Lc=0.162888 - total loss=0.187866\n","12/12 [==============================] - 1s 62ms/step\n","epoch 46\n","[Train] - Lr=0.025000, Lc=0.163226 - total loss=0.188227\n","12/12 [==============================] - 1s 87ms/step\n","epoch 47\n","[Train] - Lr=0.024973, Lc=0.163273 - total loss=0.188246\n","12/12 [==============================] - 1s 63ms/step\n","epoch 48\n","[Train] - Lr=0.024866, Lc=0.162981 - total loss=0.187847\n","12/12 [==============================] - 1s 62ms/step\n","epoch 49\n","[Train] - Lr=0.024730, Lc=0.162446 - total loss=0.187176\n","12/12 [==============================] - 1s 61ms/step\n","epoch 50\n","[Train] - Lr=0.024650, Lc=0.161950 - total loss=0.186599\n","Saved model to: /content/drive/MyDrive/result/tmp/DTC_model_50.h5\n","12/12 [==============================] - 1s 61ms/step\n","epoch 51\n","[Train] - Lr=0.024671, Lc=0.161774 - total loss=0.186446\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 61ms/step\n","epoch 52\n","[Train] - Lr=0.024776, Lc=0.161956 - total loss=0.186732\n","12/12 [==============================] - 1s 83ms/step\n","epoch 53\n","[Train] - Lr=0.024893, Lc=0.162265 - total loss=0.187157\n","12/12 [==============================] - 1s 87ms/step\n","epoch 54\n","[Train] - Lr=0.024929, Lc=0.162405 - total loss=0.187334\n","12/12 [==============================] - 1s 61ms/step\n","epoch 55\n","[Train] - Lr=0.024835, Lc=0.162246 - total loss=0.187080\n","12/12 [==============================] - 1s 61ms/step\n","epoch 56\n","[Train] - Lr=0.024643, Lc=0.161841 - total loss=0.186484\n","12/12 [==============================] - 1s 65ms/step\n","epoch 57\n","[Train] - Lr=0.024451, Lc=0.161411 - total loss=0.185862\n","12/12 [==============================] - 1s 63ms/step\n","epoch 58\n","[Train] - Lr=0.024350, Lc=0.161210 - total loss=0.185559\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 63ms/step\n","epoch 59\n","[Train] - Lr=0.024370, Lc=0.161302 - total loss=0.185673\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 2/10.\n","12/12 [==============================] - 1s 82ms/step\n","epoch 60\n","[Train] - Lr=0.024465, Lc=0.161525 - total loss=0.185990\n","12/12 [==============================] - 1s 84ms/step\n","epoch 61\n","[Train] - Lr=0.024537, Lc=0.161631 - total loss=0.186168\n","12/12 [==============================] - 1s 61ms/step\n","epoch 62\n","[Train] - Lr=0.024517, Lc=0.161493 - total loss=0.186010\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 63ms/step\n","epoch 63\n","[Train] - Lr=0.024412, Lc=0.161174 - total loss=0.185586\n","12/12 [==============================] - 1s 61ms/step\n","epoch 64\n","[Train] - Lr=0.024295, Lc=0.160883 - total loss=0.185178\n","12/12 [==============================] - 1s 61ms/step\n","epoch 65\n","[Train] - Lr=0.024231, Lc=0.160799 - total loss=0.185031\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 66\n","[Train] - Lr=0.024235, Lc=0.160903 - total loss=0.185138\n","12/12 [==============================] - 1s 61ms/step\n","epoch 67\n","[Train] - Lr=0.024256, Lc=0.161018 - total loss=0.185274\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 84ms/step\n","epoch 68\n","[Train] - Lr=0.024232, Lc=0.160979 - total loss=0.185211\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 2/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 69\n","[Train] - Lr=0.024155, Lc=0.160774 - total loss=0.184929\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 3/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 70\n","[Train] - Lr=0.024078, Lc=0.160539 - total loss=0.184617\n","12/12 [==============================] - 1s 61ms/step\n","epoch 71\n","[Train] - Lr=0.024060, Lc=0.160430 - total loss=0.184490\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 61ms/step\n","epoch 72\n","[Train] - Lr=0.024103, Lc=0.160478 - total loss=0.184582\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 2/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 73\n","[Train] - Lr=0.024145, Lc=0.160566 - total loss=0.184711\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 3/10.\n","12/12 [==============================] - 1s 84ms/step\n","epoch 74\n","[Train] - Lr=0.024114, Lc=0.160553 - total loss=0.184667\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 4/10.\n","12/12 [==============================] - 1s 72ms/step\n","epoch 75\n","[Train] - Lr=0.024003, Lc=0.160412 - total loss=0.184415\n","12/12 [==============================] - 1s 61ms/step\n","epoch 76\n","[Train] - Lr=0.023884, Lc=0.160242 - total loss=0.184126\n","12/12 [==============================] - 1s 62ms/step\n","epoch 77\n","[Train] - Lr=0.023839, Lc=0.160161 - total loss=0.184001\n","12/12 [==============================] - 1s 62ms/step\n","epoch 78\n","[Train] - Lr=0.023887, Lc=0.160194 - total loss=0.184080\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 79\n","[Train] - Lr=0.023954, Lc=0.160253 - total loss=0.184206\n","12/12 [==============================] - 1s 62ms/step\n","epoch 80\n","[Train] - Lr=0.023949, Lc=0.160231 - total loss=0.184180\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 86ms/step\n","epoch 81\n","[Train] - Lr=0.023853, Lc=0.160119 - total loss=0.183972\n","12/12 [==============================] - 1s 62ms/step\n","epoch 82\n","[Train] - Lr=0.023741, Lc=0.160009 - total loss=0.183750\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 60ms/step\n","epoch 83\n","[Train] - Lr=0.023701, Lc=0.159983 - total loss=0.183683\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 2/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 84\n","[Train] - Lr=0.023737, Lc=0.160022 - total loss=0.183760\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 3/10.\n","12/12 [==============================] - 1s 62ms/step\n","epoch 85\n","[Train] - Lr=0.023771, Lc=0.160042 - total loss=0.183813\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 4/10.\n","12/12 [==============================] - 1s 61ms/step\n","epoch 86\n","[Train] - Lr=0.023733, Lc=0.159987 - total loss=0.183720\n","12/12 [==============================] - 1s 87ms/step\n","epoch 87\n","[Train] - Lr=0.023648, Lc=0.159899 - total loss=0.183547\n","12/12 [==============================] - 1s 90ms/step\n","epoch 88\n","[Train] - Lr=0.023597, Lc=0.159856 - total loss=0.183453\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 63ms/step\n","epoch 89\n","[Train] - Lr=0.023616, Lc=0.159872 - total loss=0.183489\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 2/10.\n","12/12 [==============================] - 1s 63ms/step\n","epoch 90\n","[Train] - Lr=0.023644, Lc=0.159896 - total loss=0.183540\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 3/10.\n","12/12 [==============================] - 1s 61ms/step\n","epoch 91\n","[Train] - Lr=0.023606, Lc=0.159875 - total loss=0.183481\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 4/10.\n","12/12 [==============================] - 1s 61ms/step\n","epoch 92\n","[Train] - Lr=0.023525, Lc=0.159827 - total loss=0.183352\n","12/12 [==============================] - 1s 62ms/step\n","epoch 93\n","[Train] - Lr=0.023489, Lc=0.159800 - total loss=0.183290\n","12/12 [==============================] - 1s 89ms/step\n","epoch 94\n","[Train] - Lr=0.023526, Lc=0.159809 - total loss=0.183336\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","12/12 [==============================] - 1s 64ms/step\n","epoch 95\n","[Train] - Lr=0.023548, Lc=0.159827 - total loss=0.183375\n","12/12 [==============================] - 1s 62ms/step\n","epoch 96\n","[Train] - Lr=0.023481, Lc=0.159831 - total loss=0.183312\n","12/12 [==============================] - 1s 64ms/step\n","epoch 97\n","[Train] - Lr=0.023391, Lc=0.159831 - total loss=0.183223\n","12/12 [==============================] - 1s 61ms/step\n","epoch 98\n","[Train] - Lr=0.023395, Lc=0.159832 - total loss=0.183227\n","12/12 [==============================] - 1s 61ms/step\n","epoch 99\n","[Train] - Lr=0.023456, Lc=0.159832 - total loss=0.183288\n","Assignment changes 0.0 < 1e-05 tolerance threshold. Patience: 1/10.\n","Saving model to: /content/drive/MyDrive/result/tmp/DTC_model_final.h5\n","Training time:  210.46323919296265\n","Performance (TRAIN)\n","12/12 [==============================] - 1s 63ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["array([5, 0, 0, 0, 0, 0, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5,\n","       6, 6, 6, 6, 5, 6, 6, 5, 5, 6, 6, 6, 6, 5, 6, 6, 6, 5, 5, 6, 6, 6,\n","       6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5,\n","       6, 6, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","       5, 1, 6, 6, 6, 5, 5, 6, 6, 6, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4,\n","       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3,\n","       3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4,\n","       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3,\n","       3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3,\n","       3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n","       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1])"]},"metadata":{},"execution_count":48}],"source":["result = main()\n","result"]},{"cell_type":"code","source":["silhouette_avg_rdata_daily = silhouette_score(nor_data, result)\n","print(\"The average silhouette_score is :\", silhouette_avg_rdata_daily)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tReQP2Gr8_gh","executionInfo":{"status":"ok","timestamp":1680240932985,"user_tz":240,"elapsed":254,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"697cde1f-22c6-4154-8816-66333ca8acd4"},"id":"tReQP2Gr8_gh","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The average silhouette_score is : 0.28188276\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import davies_bouldin_score \n","\n","print(\"Davies-Bouldin score is \", davies_bouldin_score(nor_data, result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680241427678,"user_tz":240,"elapsed":427,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"5091d4f5-8935-4ce3-a38e-d6c2b14f0b1c","id":"Gggb-_dLfRf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Davies-Bouldin score is  1.685088684615513\n"]}],"id":"Gggb-_dLfRf4"},{"cell_type":"markdown","source":["# **4. Evaluation:**\n","To compute the RMSE, variance, and average inter cluster distance we have to use the array format of our dataset and the clustering result."],"metadata":{"id":"cZVCUT204Piu"},"id":"cZVCUT204Piu"},{"cell_type":"code","source":["def total_rmse(data_path,formed_clusters):\n","  processed_data = data_preprocessing(data_path)\n","  trans_data = pd.DataFrame(processed_data)\n","  trans_data['Cluster'] = formed_clusters\n","\n","  # Normalized\n","  # Function that creates two dictionaries that hold all the clusters and cluster centers\n","  def nor_get_clusters_and_centers(input,formed_clusters):\n","    Clusters = {}\n","    Cluster_Centers = {}\n","    for i in set(formed_clusters):\n","      Clusters['Cluster' + str(i)] = np.array(input[input.Cluster == i].drop(columns=['Cluster']))\n","      Cluster_Centers['Cluster_Center' + str(i)] = np.mean(Clusters['Cluster' + str(i)],axis=0)\n","    return Clusters,Cluster_Centers\n","\n","  intra_rmse = []\n","  sq_diff = []\n","  Clusters,Cluster_Centers = nor_get_clusters_and_centers(trans_data,formed_clusters)\n","  for i in range(len(Clusters)):\n","    for j in range(len(Clusters['Cluster' + str(i)])):\n","      diff = Clusters['Cluster' + str(i)][j] - Cluster_Centers['Cluster_Center' + str(i)]\n","      Sq_diff = (diff)**2\n","      sq_diff.append(Sq_diff)\n","\n","  Sq_diff_sum = np.sum(np.sum(sq_diff))\n","  rmse = np.sqrt(Sq_diff_sum/data_nor_eval.shape[0])\n","  return rmse"],"metadata":{"id":"fR9HdtHppkXY"},"id":"fR9HdtHppkXY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_rmse('/content/data/ERA5_Dataset.nc', result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680241303484,"user_tz":240,"elapsed":8039,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"c8b37ef0-bba0-4ece-c1ab-fc0d90e50c07","id":"iPi0oPD7fRf4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14.706457404693607"]},"metadata":{},"execution_count":93}],"id":"iPi0oPD7fRf4"},{"cell_type":"markdown","source":["### This cell measure the variances of the generated clusters.  "],"metadata":{"id":"3vqdgBdOW2V7"},"id":"3vqdgBdOW2V7"},{"cell_type":"code","source":["trans_data = pd.DataFrame(nor_data)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","variances = pd.DataFrame(columns=range(len(Clusters)),index=range(2))\n","for i in range(len(Clusters)):\n","    variances[i].iloc[0] = np.var(Clusters['Cluster' + str(i)])\n","    variances[i].iloc[1] = Clusters['Cluster' + str(i)].shape[0]\n","\n","var_sum = 0\n","for i in range(7):\n","    var_sum = var_sum + (variances[i].iloc[0] * variances[i].iloc[1])\n","\n","var_avg = var_sum/data_nor_eval.shape[0]\n","var_avg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680241167407,"user_tz":240,"elapsed":155,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"3f18f684-c5a4-4ad4-9f2f-0b82e8657582","id":"kJ1eGJ0EfRf3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.04575348730991926"]},"metadata":{},"execution_count":86}],"id":"kJ1eGJ0EfRf3"},{"cell_type":"markdown","source":["### The following cell measure the average inter cluster distance.  "],"metadata":{"id":"J7V-K6PvXJAk"},"id":"J7V-K6PvXJAk"},{"cell_type":"code","source":["from scipy.spatial.distance import cdist,pdist\n","\n","trans_data = pd.DataFrame(nor_data)\n","trans_data['Cluster'] = result\n","Clusters = {}\n","Cluster_Centers = {}\n","for i in set(result):\n","  Clusters['Cluster' + str(i)] = np.array(trans_data[trans_data.Cluster == i].drop(columns=['Cluster']))\n","\n","distance_matrix = pd.DataFrame(columns=range(len(Clusters)),index=range(len(Clusters)))\n","for i in range(len(Clusters)):\n","  for j in range(len(Clusters)):\n","    if i == j:\n","      #distance_matrix[i].iloc[j] = 0\n","      distance_intra = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.max(distance_intra)\n","    elif i > j:\n","       continue\n","    else:\n","      distance = cdist(Clusters['Cluster' + str(i)], Clusters['Cluster' + str(j)], 'euclidean')\n","      distance_matrix[i].iloc[j] = np.min(distance)\n","      distance_matrix[j].iloc[i] = np.min(distance)\n","\n","sum_min = 0\n","for i in range(n_clusters):\n","    sum_min = sum_min + np.min(distance_matrix[i])\n","\n","avg_inter = sum_min/n_clusters\n","avg_inter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680241231018,"user_tz":240,"elapsed":136,"user":{"displayName":"Omar Faruque","userId":"13159414346226003068"}},"outputId":"44ef1b34-7326-4eb9-a53e-c7f07eb490ed","id":"60OkSVPJfRf4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.523884436048127"]},"metadata":{},"execution_count":89}],"id":"60OkSVPJfRf4"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[{"file_id":"1c6oteKxFLc45Df9eCi1OqZx0SLkFRMU4","timestamp":1671164211680}]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}